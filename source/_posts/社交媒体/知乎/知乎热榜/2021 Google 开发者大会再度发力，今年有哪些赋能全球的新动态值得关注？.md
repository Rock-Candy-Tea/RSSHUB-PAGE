
---
title: '2021 Google 开发者大会再度发力，今年有哪些赋能全球的新动态值得关注？'
categories: 
 - 社交媒体
 - 知乎
 - 知乎热榜
headimg: 'https://pic1.zhimg.com/v2-d150a97b49e0d0be002de297f81fdbd0_1440w.jpg'
author: 知乎
comments: false
date: Tue, 16 Nov 2021 07:32:56 GMT
thumbnail: 'https://pic1.zhimg.com/v2-d150a97b49e0d0be002de297f81fdbd0_1440w.jpg'
---

<div>   
IT人刘俊明的回答<br><br><p data-pid="pmyWrsK1">无论是对于IT互联网行业的技术从业者、爱好者, 亦或计算机大类专业的老师和同学来说，一年一度的谷歌开发者大会是绝对不应该错过的。</p><p data-pid="fCQkHiwG">今年谷歌开发者大会的主题是Develop as One。结合本次大会的内容，我个人认为可以对这个主题的理解分成三个方面，其一是谷歌希望借助于开发者大会来推动国内开发者的国际化，其二是谷歌希望构建一个更好的技术社区文化，其三，就是谷歌希望打造一系列被广泛使用的工具和平台，让开发更简单，创新更高效，交流更便捷。</p><p data-pid="1kE2gZr-">在众多的互联网科技公司当中，谷歌对于开发者是相对比较友好的，这一点在历年的谷歌开发者大会上也都有所体现，而在今年，这一点更是进一步得到证实。</p><p data-pid="xPLFUfVr">一方面谷歌首次推出了<b>Google开发者在线课程</b>（我把网址也放出来给大家</p><a href="http://link.zhihu.com/?target=https%3A//developers.google.cn/learn/pathways%3Futm_source%3Dzhihu%26utm_medium%3Da%26utm_campaign%3Dgds21" data-draft-node="block" data-draft-type="link-card" class=" wrap external" target="_blank" rel="nofollow noreferrer">开发者在线课程  |  Google Developers</a><p data-pid="vg7s85tJ">），课程由谷歌产品团队打造，课程内容涵盖了10余个谷歌产品和服务，80余篇专业文章，110多个视频深度解析，以及170多个引导式编程练习，另一方面谷歌还推出了自己的开发者服务号（@谷歌开发者服务号），可以让开发者第一时间了解到最新的技术咨询，更加方便地参加开发者社区活动，这些正是同学们和开发者最为关注的内容。</p><p data-pid="-flJ_5mM">谷歌历年的开发者大会，也比较<b>关注技术和创新，</b>今年同样有很多看点。作为一名科研、教育工作者，由于我的很多课题和项目都与谷歌的产品线有密切的关系，所以我是带着更多的期待来观看今年的谷歌开发者大会，令我感到欣慰的是，我<b>关注的内容几乎在今年的开发者大会上都有了最新的解决方案，包括Android、Tensorflow、Flutter、ARCore等等</b>。</p><p data-pid="djR_ysY-"><b>一．Android12的革新</b></p><p data-pid="adCUvOMd"><b>Android</b>是本次开发者大会上我关注的核心内容之一，一方面我的移动互联网项目组有大量基于Android的创新应用，另一方面在国内工业互联网快速发展的大背景， Android未来是否有相应的解决方案，是一个非常具实用性的命题。</p><p data-pid="bTkFKOxu">本次大会上，谷歌的技术专家重点探讨了Anroid 12的多个技术亮点，其中提升用户体验是一个核心点；包括 Android能否完成大屏幕适配方面，也为大家带来了想要的答案。</p><p data-pid="CL1HTxSP">Android12在用户视觉体验方面做了多个改进，以下列举出三个比较有代表性的改进：</p><p data-pid="X1OWSZoc">1.滚动事件的视觉行为发生了变化。当发生拖动事件时，视觉元素会拉伸和反弹，当发生快速滑动事件时，它们会快速滑动和反弹，这会在一定程度上提升操作的质感。</p><p data-pid="mMsoGLnP">该行为会应用于使用 EdgeEffect 的所有应用，并且适用于RecyclerView、ListView、ScrollView、NestedScrollView、HorizontalScrollView、ViewPager、ViewPager2等类。</p><p data-pid="B6eSjJ2R">EdgeEffect 添加了两个用于实现拉伸滚动效果的 API，包括float getDistance()和float onPullDistance(float deltaDistance, float displacement)。</p><p data-pid="aNszm2Zs">当用户捕捉活动的拉伸动画时，EdgeEffect.isFinished() 会返回 false。这表示拉伸应由轻触动作操控。在大多数容器中，这在 onInterceptTouchEvent() 中检测，看一个代码案例：</p><p data-pid="SMLzv2OM">@Override</p><p data-pid="kTDax67_">public boolean onInterceptTouchEvent(MotionEvent ev) &#123;</p><p data-pid="aCzL1ri5">  ...</p><p data-pid="vs8LTHSn">  switch (action & MotionEvent.ACTION_MASK) &#123;</p><p data-pid="7oQqOhRD">    case MotionEvent.ACTION_DOWN:</p><p data-pid="W7bWftiR">      ...</p><p data-pid="MHNdJJJZ">      mIsBeingDragged = !mEdgeEffectBottom.isFinished()</p><p data-pid="ZrDSUeG1">          || !mEdgeEffectTop.isFinished();</p><p data-pid="8CwGqcMI">      ...</p><p data-pid="AJC_Iuhm">另外，务必在滚动之前释放拉伸效果，以防止将拉伸应用于滚动内容，看一下代码案例：</p><p data-pid="HrdfPSGW">public boolean onTouchEvent(MotionEvent ev) &#123;</p><p data-pid="ldD-d-x4"> final int actionMasked = ev.getActionMasked();</p><p data-pid="Q2lgDpNp">  switch (actionMasked) &#123;</p><p data-pid="N30U06Q8">    case MotionEvent.ACTION_MOVE:</p><p data-pid="bfeTdzn2">      final float x = ev.getX(activePointerIndex);</p><p data-pid="QLuVvo_8">      final float y = ev.getY(activePointerIndex);</p><p data-pid="VF8m_cEB">      float deltaY = y - mLastMotionY;</p><p data-pid="jGC6jP-d">      float pullDistance = deltaY / getHeight();</p><p data-pid="4bpEpN8U">      float displacement = x / getWidth();</p><p data-pid="A0i2kgS9">      if (deltaY < 0 && mEdgeEffectTop.getDistance() > 0) &#123;</p><p data-pid="FIVpZQ3y">        deltaY -= getHeight() * mEdgeEffectTop</p><p data-pid="OSFj2gKk">            .onPullDistance(pullDistance, displacement);</p><p data-pid="Rpw4L3I5">      &#125;</p><p data-pid="O0McDOUF">      if (deltaY > 0 && mEdgeEffectBottom.getDistance() > 0) &#123;</p><p data-pid="tbEa_5O9">        deltaY += getHeight() * mEdgeEffectBottom</p><p data-pid="BK44GXpO">            .onPullDistance(-pullDistance, 1 - displacement);</p><p data-pid="XOswdd_E">      &#125;</p><p data-pid="3T0lO8UT">...</p><p data-pid="Vbd2MMr2">2.Android 12 引入了 RoundedCorner 和 WindowInsets.getRoundedCorner(int position)，它们可以提供圆角的半径和中心点。借助这些 API，开发者的应用可以避免界面元素在带有圆角的屏幕上被截断，看一个代码案例：</p><figure data-size="normal"><img src="https://pic1.zhimg.com/v2-d150a97b49e0d0be002de297f81fdbd0_1440w.jpg" data-caption data-size="normal" data-rawwidth="922" data-rawheight="721" class="origin_image zh-lightbox-thumb" data-original="https://pic1.zhimg.com/v2-d150a97b49e0d0be002de297f81fdbd0_r.jpg" referrerpolicy="no-referrer"></figure><p data-pid="w8K4BOOo">3.Android 12更改了完全自定义通知的外观和行为。在以前的版本中，自定义通知能够使用整个通知区域并提供自己的布局和样式，由此产生的反模式可能会令用户困惑，或在不同设备上引发布局兼容性问题。</p><p data-pid="l2zgF49N">对于以 Android 12 为目标平台的应用，包含自定义内容视图的通知将不再使用完整通知区域，而是系统会应用标准模板，这个模板可以确保自定义通知在所有状态下都与其他通知相同，例如，在收起状态下的通知图标和展开功能，以及在展开状态下的通知图标、应用名称和收起功能，此行为与 Notification.DecoratedCustomViewStyle 的行为几乎完全相同。</p><p data-pid="lOOIMxzB">通过这种方式，Android 12 通过为用户提供可看到且熟悉的通知展开功能，使所有通知保持外观一致且易于浏览。</p><p data-pid="HPW5S5gL">下图显示了标准模板中的自定义通知：</p><figure data-size="normal"><img src="https://pic1.zhimg.com/v2-1d9c21657a5aa53c3a2d0508582d7b8b_1440w.jpg" data-caption data-size="normal" data-rawwidth="1672" data-rawheight="654" class="origin_image zh-lightbox-thumb" data-original="https://pic1.zhimg.com/v2-1d9c21657a5aa53c3a2d0508582d7b8b_r.jpg" referrerpolicy="no-referrer"></figure><p data-pid="iVOqcBuo">另外，从 Android 12 开始，开发者可以通过ApplicationExitInfo.getTraceInputStream() 方法以协议缓冲区的形式访问应用的原生代码崩溃 Tombstone，协议缓冲区使用此架构进行序列化。以前，只有通过 Android 调试桥 (adb) 才能访问此信息，看一个代码案例：</p><p data-pid="I5KJ5mUq">ActivityManager activityManager: ActivityManager = getSystemService(Context.ACTIVITY_SERVICE);</p><p data-pid="ZmnL3utC">MutableList<ApplicationExitInfo> exitReasons = activityManager.getHistoricalProcessExitReasons(/* packageName = */ null, /* pid = */ 0, /* maxNum = */ 5);</p><p data-pid="0q_IzUoI">for ( ApplicationExitInfo aei: exitReasons ) &#123;</p><p data-pid="lfH9ETOA">    if ( aei.getReason() == REASON_CRASH_NATIVE ) &#123;</p><p data-pid="_Ih3UrYV">        // Get the tombstone input stream.</p><p data-pid="gcmM-N6B">        InputStream tombstoneInputStream = aei.getTraceInputStream();</p><p data-pid="2Ue4oohm">        // The tombstone parser built with protoc uses the tombstone schema, then parses the trace.</p><p data-pid="-oqRdTPO">        Tombstone tombstone = Tombstone.parseFrom(trace);</p><p data-pid="FqVYOQCh">    &#125;</p><p data-pid="Q0gDIv-5">&#125;</p><p data-pid="BsSWlBjI">以上仅仅是我列举出的一些比较有代表性的改进，大家可以通过谷歌提供的开发者在线课程来进行全面的了解。</p><p data-pid="d82ICNFK">在开发方面，Android目前已经构建起了一个比较庞大和完善的技术生态，此次谷歌技术专家重点提及了两个技术亮点，其一是采用Jetpack Compose，能帮助用户界面实现全面自适应，其二是开发工具Modern Android Development（MAD）助力开发者更快速、简洁的完成开发任务。</p><p data-pid="eWa7UBpM"><b>二．Tensorflow</b></p><p data-pid="WUgRb2vA">我目前带的研究生主要集中在两个方向上：移动互联网方向，和大数据和机器学习。所以这次也重点观看了<b>Tensorflow</b>的迭代。</p><p data-pid="H_rDOBht">这里强调一下，对于主攻大数据、人工智能相关方向的同学来说，Tensorflow是要重点掌握的工具，确实非常方便，也更加高效。一是，在机器学习落地应用的过程中，Tensorflow起到了非常积极的促进作用，很多国内外的互联网大厂在采用；二是，很多老师的课题组和项目组也都在采用Tensorflow来完成一些落地应用，包括我目前的项目组。近两年在招募一些本科生进大数据课题组时，通常都会让同学们先学习Tensorflow，而每次谷歌开发者大会也都会组织课题组的同学积极观看相关的技术视频，以便于了解它的最新应用。</p><p data-pid="ZfBMSSMb">在本次大会上，谷歌技术专家重点探讨了四个内容，其一是借助 Keras 的框架，开发者可以解决各种各样的应用机器学习问题，并开发出更复杂的模型。</p><p data-pid="u5qyRLnm">其二是TensorFlow Hub 提供多种多样的预训练模型，开发者可以用于视频、图片、文本、语音和音频等场景。</p><p data-pid="vfk55G5N">其三是利用TensorFlow Lite可以构建具备机器学习功能的原生移动应用，运行即可覆盖 Android和iOS平台上的数十亿用户。</p><p data-pid="Zib324Q8">其四是TensorFlow.js 可以在任何支持 JavaScript 的环境中运行模型，无需环境设置一键启动，另外TensorFlow.js 即将支持 TensorFlow Lite模型，可以更加高效且便捷地在 web 环境中高效开发。</p><p data-pid="3Q96Xu7d">比如使用keras对服装图像进行分类，大概需要经过以下几个过程：</p><ol><li data-pid="n2Y3J2hk">导入 Fashion MNIST 数据集。该数据集包含 10 个类别的 70,000 个灰度图像。这些图像以低分辨率（28x28 像素）展示了单件衣物，如下所示：</li></ol><figure data-size="normal"><img src="https://pic3.zhimg.com/v2-02f9425ac449c187f6fc5b99e2f1c309_1440w.jpg" data-caption data-size="normal" data-rawwidth="840" data-rawheight="840" class="origin_image zh-lightbox-thumb" data-original="https://pic3.zhimg.com/v2-02f9425ac449c187f6fc5b99e2f1c309_r.jpg" referrerpolicy="no-referrer"></figure><p data-pid="YNC9GqOb">我们使用 60,000 个图像来训练网络，使用 10,000 个图像来评估网络学习对图像分类的准确率，大家可以直接从 TensorFlow 访问 Fashion MNIST，运行以下代码，可以直接从 TensorFlow 中导入和加载 Fashion MNIST 数据：</p><p data-pid="XT7leaqD">fashion_mnist = keras.datasets.fashion_mnist</p><p data-pid="TrbqYeMP">(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()</p><p data-pid="abJHj1k8">加载数据集会返回四个 NumPy 数组：train_images 和 train_labels 数组是训练集，即模型用于学习的数据。测试集、test_images 和 test_labels 数组会被用来对模型进行测试。</p><p data-pid="7cZD5S4N">2. 预处理数据。在训练网络之前，必须对数据进行预处理。如果我们检查训练集中的第一个图像，我们会看到像素值处于 0 到 255 之间：</p><figure data-size="normal"><img src="https://pic2.zhimg.com/v2-fc35021f80f0959f5a9fabc2bbbca444_1440w.jpg" data-caption data-size="normal" data-rawwidth="753" data-rawheight="379" class="origin_image zh-lightbox-thumb" data-original="https://pic2.zhimg.com/v2-fc35021f80f0959f5a9fabc2bbbca444_r.jpg" referrerpolicy="no-referrer"></figure><figure data-size="normal"><img src="https://pic2.zhimg.com/v2-b3a9c61b5e1719e3b6c9346900245a9a_720w.jpg" data-caption data-size="normal" data-rawwidth="305" data-rawheight="248" class="content_image" referrerpolicy="no-referrer"></figure><p data-pid="9nbzFsh-">将这些值缩小至 0 到 1 之间，然后将其馈送到神经网络模型。为此，请将这些值除以 255。请务必以相同的方式对训练集和测试集进行预处理：</p><p data-pid="W3Owku-2">train_images = train_images / 255.0</p><p data-pid="JwljQctE">test_images = test_images / 255.0</p><p data-pid="zHzt11UP">为了验证数据的格式是否正确，以及我们是否已准备好构建和训练网络，让我们显示训练集中的前 25 个图像，并在每个图像下方显示类名称。</p><figure data-size="normal"><img src="https://pic4.zhimg.com/v2-c3a8c51e679f8d78de06fe329d09d2e9_1440w.jpg" data-caption data-size="normal" data-rawwidth="1141" data-rawheight="526" class="origin_image zh-lightbox-thumb" data-original="https://pic4.zhimg.com/v2-c3a8c51e679f8d78de06fe329d09d2e9_r.jpg" referrerpolicy="no-referrer"></figure><figure data-size="normal"><img src="https://pic2.zhimg.com/v2-c4e18742a2736f004e0fe222084cfb7a_1440w.jpg" data-caption data-size="normal" data-rawwidth="573" data-rawheight="572" class="origin_image zh-lightbox-thumb" data-original="https://pic2.zhimg.com/v2-c4e18742a2736f004e0fe222084cfb7a_r.jpg" referrerpolicy="no-referrer"></figure><p data-pid="p328Yjil">3. 构建模型。构建神经网络需要先配置模型的层，然后再编译模型。</p><p data-pid="ZkduWeXp">（1）设置层。神经网络的基本组成部分是层，层会从向其馈送的数据中提取表示形式，希望这些表示形式有助于解决手头上的问题。大多数深度学习都包括将简单的层链接在一起，大多数层（如 tf.keras.layers.Dense）都具有在训练期间才会学习的参数。</p><figure data-size="normal"><img src="https://pic2.zhimg.com/v2-311cf6f5bbae6280c439d3ed44845286_1440w.jpg" data-caption data-size="normal" data-rawwidth="1242" data-rawheight="396" class="origin_image zh-lightbox-thumb" data-original="https://pic2.zhimg.com/v2-311cf6f5bbae6280c439d3ed44845286_r.jpg" referrerpolicy="no-referrer"></figure><p data-pid="iijfuV0a">（2）编译模型。在准备对模型进行训练之前，还需要再对其进行一些设置，以下内容是在模型的编译步骤中添加的：</p><p data-pid="tK8FxI3N">损失函数 - 用于测量模型在训练期间的准确率。我们会希望最小化此函数，以便将模型“引导”到正确的方向上。</p><p data-pid="1_-BrhLp">优化器 - 决定模型如何根据其看到的数据和自身的损失函数进行更新。</p><p data-pid="IkvwOsM1">指标 - 用于监控训练和测试步骤。</p><p data-pid="pbsWPc10">以下使用了准确率，即被正确分类的图像的比率。</p><p data-pid="Z78NuTOW">model.compile(optimizer='adam',loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),metrics=['accuracy'])</p><p data-pid="MVUrOOf4">4.训练模型。训练神经网络模型需要执行以下步骤：（1）将训练数据馈送给模型。在本例中，训练数据位于 train_images 和 train_labels 数组中。（2）模型学习将图像和标签关联起来。（3）要求模型对测试集（在本例中为 test_images 数组）进行预测。（4）验证预测是否与 test_labels 数组中的标签相匹配。</p><p data-pid="j4L3bQq8">我们直接看看模型对于全部 10 个类的预测：</p><figure data-size="normal"><img src="https://pic2.zhimg.com/v2-2e45adde808083e38ad579500970798e_1440w.jpg" data-caption data-size="normal" data-rawwidth="1158" data-rawheight="648" class="origin_image zh-lightbox-thumb" data-original="https://pic2.zhimg.com/v2-2e45adde808083e38ad579500970798e_r.jpg" referrerpolicy="no-referrer"></figure><figure data-size="normal"><img src="https://pic2.zhimg.com/v2-944ca4b020a50b7cd7c2f2fc4f3368bf_1440w.jpg" data-caption data-size="normal" data-rawwidth="1020" data-rawheight="413" class="origin_image zh-lightbox-thumb" data-original="https://pic2.zhimg.com/v2-944ca4b020a50b7cd7c2f2fc4f3368bf_r.jpg" referrerpolicy="no-referrer"></figure><p data-pid="2s8t8C50">在模型经过训练后，我们可以使用它对一些图像进行预测，也就是验证预测结果。</p><p data-pid="DTKqfY3D">我们来看看第 0 个图像、预测结果和预测数组，正确的预测标签为蓝色，错误的预测标签为红色，数字表示预测标签的百分比（总计为 100）。</p><figure data-size="normal"><img src="https://pic2.zhimg.com/v2-b5ad74d64567e91cddfadbd88a97fe87_1440w.jpg" data-caption data-size="normal" data-rawwidth="1111" data-rawheight="380" class="origin_image zh-lightbox-thumb" data-original="https://pic2.zhimg.com/v2-b5ad74d64567e91cddfadbd88a97fe87_r.jpg" referrerpolicy="no-referrer"></figure><figure data-size="normal"><img src="https://pic3.zhimg.com/v2-5721188c979c02371dca10c0c989cce4_720w.jpg" data-caption data-size="normal" data-rawwidth="352" data-rawheight="194" class="content_image" referrerpolicy="no-referrer"></figure><p data-pid="kre4o0Z6">让我们用模型的预测绘制几张图像。请注意，即使置信度很高，模型也可能出错。</p><figure data-size="normal"><img src="https://pic3.zhimg.com/v2-2d99f77b7b5293231b133836b31112fc_1440w.jpg" data-caption data-size="normal" data-rawwidth="1044" data-rawheight="444" class="origin_image zh-lightbox-thumb" data-original="https://pic3.zhimg.com/v2-2d99f77b7b5293231b133836b31112fc_r.jpg" referrerpolicy="no-referrer"></figure><p><br></p><figure data-size="normal"><img src="https://pic2.zhimg.com/v2-5095916629a5f3c65ad218e9a4c3633d_1440w.jpg" data-caption data-size="normal" data-rawwidth="852" data-rawheight="712" class="origin_image zh-lightbox-thumb" data-original="https://pic2.zhimg.com/v2-5095916629a5f3c65ad218e9a4c3633d_r.jpg" referrerpolicy="no-referrer"></figure><p data-pid="yp-5Zr1R">5. 使用训练好的模型。最后，使用训练好的模型对单个图像进行预测。</p><p data-pid="keF6_AiK">对于很多开发者来说，运营是一个较大的挑战，而开源解决方案TFX简化了运营挑战，高效管理运营工作，TFX内提供集成公平性指标和 Model Card Toolkit及隐私工具包，实现负责任的AI 做法。</p><p data-pid="nAHi95yK">目前我带领的一个移动互联团队正在使用Tensorflow来完成模型训练，以便于为用户提供更好的使用体验。甚至不少同学正在尝试基于Tensorflow来完成自己的创新成果。这次大会上，谷歌技术专家分享的行业应用案例，知衣科技的案例让人印象非常深刻，同样在使用Tensorflow训练模型，大家在场景上会有所不同，这些成功案例也给到了我们相应地启发。</p><p data-pid="sxVGn6_U"><b>三．ARCore</b></p><p data-pid="YX2xSvHZ">本次谷歌开发者大会还有一些比较有潜力的技术，<b>ARCore</b>就是一个典型的代表。近两年移动互联项目组里的同学也有尝试采用ARCore来完成增强现实的应用，并基于ARCore做了一些设想，如完成一些工业场景领域的智能应用等等。这次大会上，也带给我们很多新的创新想法。</p><p data-pid="V8qNsyOW">本次大会技术专家分享了抖音基于Depth API开发的特效，虽然这是一个比较娱乐化的应用，但在工业场景下，应该也可以做一些创新尝试。我的团队在2016年曾经做过一个AR应用，当时并没有采用ARCore。相信如果采用ARCore来重新构建，应该会有更好的使用体验，这也是我观看本次大会的一个收获，至少多了一个技术选项。</p><p data-pid="Wnq80DY2"><b>四．Web</b></p><p data-pid="SJeSmQbA">近些年谷歌在<b>Web领域的技术创新</b>一直走在众多互联网公司的前列，本次开发者大会还重点谈到了Chrome在保护用户隐私方面做出的努力：比如The Privacy Sandbox项目，就致力于不断强化web平台的隐私保障，同时降低对用户信息的跨网站跟踪能力，这在当前的大数据时代背景下，给了开发者更多保护用户隐私的选项，也可以让自己的产品更加人性化。</p><p data-pid="YqW7iESS">虽然当前云计算已经开始逐渐普及，但是由于Web兼容性问题，很多应用依然没能加入到Web阵营。此次开发者大会上，谷歌的技术专家也强调了关于改进API兼容性的问题，这会促使更多的应用进入Web，这对于很多技术薄弱的开发团队来说，是一个利好消息。</p><p data-pid="P-c--vQz">在大会上，我们可以看出谷歌技术团队目前正在将注意力侧重在——“小而精”的技术创新团队上，这是未来工业互联网时代进行产业创新的重要力量。然而对于目前很多国内的中小技术团队来说，能否快速且稳定地构建出自己的互联网应用，是非常关键的。</p><p data-pid="Ajw44Nk1"><b>五．Firebase和Flutter</b></p><p data-pid="H8QkN5bS">在进入到产业互联网时代，我有一个观点，那就是创新会从互联网向产业领域转移，或者说这是一种创新回归。但是能否完成创新回归，一个重要的基础是技术壁垒能否被打破。</p><p data-pid="PulLb88N">幸运的是，谷歌给出了自己的解决方案。<b>Firebase和Flutter这</b>两款产品在产业互联网时代，就会给很多技术力量相对薄弱的创新团队带来更多的可能。</p><p data-pid="3HBEIpF8">Firebase和Flutter会全面加速应用开发，同时还能够保证用户的使用体验，这对于广大的中小技术团队来说，绝对是一个重要的选项。</p><p data-pid="FR-Selix">大数据和人工智能领域竞争挺厉害的，但是Firebase和Flutter确实给我们这些中小创新团队带来了突围的机会，一些验证性质的应用也可以快速上线。</p><p data-pid="sYxDY9qG"><b>六．总结</b></p><p data-pid="LY6TvJw2">在每年参加谷歌开发者大会的时候，我们都非常关注的是这些技术和工具上的革新，也会要求组里的同学们，包括本科生和研究生，积极到官网（</p><a href="http://link.zhihu.com/?target=https%3A//developersummit.googlecnapps.cn/%3Futm_source%3Dzhihu%26utm_medium%3Dzhihu%26utm_campaign%3Dgds2021" data-draft-node="block" data-draft-type="link-card" data-image="https://pic3.zhimg.com/v2-29abbb24bab9bfd9f3eda0a0425e4e66_bh.jpg" data-image-width="800" data-image-height="530" class=" wrap external" target="_blank" rel="nofollow noreferrer">2021 Google 开发者大会，加入我们 Develop as One!</a><p data-pid="7Y5ei8nx">）观看开发者大会的相关视频，以便于能够尽快使用这些新的技术，从而提升开发和创新的效率。</p><p data-pid="BycRCusu">今年的大会有相当的亮点，也带来了很多启发。当然，谷歌开发者大会之所以能够得到广泛关注，一个重要的原因本身是大会上的“干货”比较多，参加完大会后，能够在开阔视野的同时，也得到实实在在的提升。</p>  
</div>
            