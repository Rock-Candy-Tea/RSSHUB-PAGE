
---
title: '深度学习如何入门？'
categories: 
 - 社交媒体
 - 知乎
 - 知乎热榜
headimg: 'https://pic3.zhimg.com/v2-eabfde057dc289980a47b30cb4f13d21_1440w.jpg'
author: 知乎
comments: false
date: Thu, 06 Jan 2022 14:04:27 GMT
thumbnail: 'https://pic3.zhimg.com/v2-eabfde057dc289980a47b30cb4f13d21_1440w.jpg'
---

<div>   
Deeper Go的回答<br><br><p data-pid="SzEjMI5H">昨天被日报转载了，褒贬不一，仍过千赞；最好的礼物。但是，除了日报，其它的转载请提前私信我，并注明出处。感谢各位捧场！</p><p data-pid="lf78MGO8">更新１：----------新年快乐    鸡年大吉----------</p><p data-pid="V1tq6J0b">更新２：转眼又到了猪年末，宣传一下我们最新的工作：如果你在做音频分类或者多模态方向的工作，请移步：</p><a data-draft-node="block" data-draft-type="link-card" href="https://zhuanlan.zhihu.com/p/100558627" data-size="small" data-entity-type="article" class="internal">关于Audioset的音频分类研究</a><p><br></p><p data-pid="XzQNCagb">因为近期要做一个关于深度学习入门的技术分享，不想堆砌公式，让大家听得一头雾水不知不觉摸裤兜掏手机刷知乎。所以花了大量时间查资料看论文，有的博客或者论文写得非常赞，比如三巨头LeCun，Bengio和Hinton 2015年在Nature上发表综述论文的“<a href="http://link.zhihu.com/?target=http%3A//www.nature.com/nature/journal/v521/n7553/pdf/nature14539.pdf" class=" wrap external" target="_blank" rel="nofollow noreferrer">Deep Learning</a>”，言简意赅地引用了上百篇论文，但适合阅读，不适合presentation式的分享；再如Michael Nielsen写的电子书《神经网络与深度学习》（<a href="http://link.zhihu.com/?target=http%3A//www.tensorfly.cn/home/" class=" wrap external" target="_blank" rel="nofollow noreferrer">中文版</a>，<a href="http://link.zhihu.com/?target=http%3A//neuralnetworksanddeeplearning.com/index.html" class=" wrap external" target="_blank" rel="nofollow noreferrer">英文版</a>）通俗易懂，用大量的例子解释了深度学习中的相关概念和基本原理，但适合于抽两三天的功夫来细品慢嚼，方能体会到作者的良苦用心；还有Colah写的<a href="http://link.zhihu.com/?target=http%3A//colah.github.io/" class=" wrap external" target="_blank" rel="nofollow noreferrer">博客</a>，每一篇详细阐明了一个主题，如果已经入门，这些博客将带你进阶，非常有趣。</p><p><br></p><p data-pid="YByPYRMZ">还翻了很多知乎问答，非常赞。但发现很多”千赞侯”走的是汇总论文视频教程以及罗列代码路线，本来想两小时入门却一脚踏进了汪洋大海；私以为，这种适合于有一定实践积累后按需查阅。还有很多”百赞户”会拿鸡蛋啊猫啊狗啊的例子来解释深度学习的相关概念，生动形象，但我又觉得有避重就轻之嫌。我想，既然要入门深度学习，得有微积分的基础，会求导数偏导数，知道链式法则，最好还学过线性代数；否则，真的，不建议入门深度学习。</p><p><br></p><p data-pid="WXoYH-D3">最后，实在没找到我想要的表达方式。我想以图的方式概要而又系统性的呈现深度学习所涉及到的基本模型和相关概念。论文“<a href="http://link.zhihu.com/?target=https%3A//arxiv.org/abs/1506.00019" class=" wrap external" target="_blank" rel="nofollow noreferrer">A Critical Review of Recurrent Neural Networks for Sequence Learning</a>”中的示意图画得简单而又形象，足以说明问题，但这篇文章仅就RNN而展开论述，并未涉及CNN，RBM等其它经典模型；<a href="http://link.zhihu.com/?target=https%3A//deeplearning4j.org/cn/neuralnet-overview.html%23forward" class=" wrap external" target="_blank" rel="nofollow noreferrer">Deeplearning4j</a>上的教程貌似缺少关于编码器相关内容的介绍，而<a href="http://link.zhihu.com/?target=http%3A//ufldl.stanford.edu/wiki/index.php/UFLDL%25E6%2595%2599%25E7%25A8%258B" class=" wrap external" target="_blank" rel="nofollow noreferrer">UFLDL教程</a>只是详细介绍了编码器的方方面面。但是如果照抄以上三篇的图例，又涉及到图例中的模块和符号不统一的问题。所以，索性自己画了部分模型图；至于直接引用的图，文中已经给了链接或出处。如有不妥之处，望指正。以下，以飨来访。</p><p><br></p><p data-pid="fDpQTxXs"><b>1. </b> 从经典的二分类开始说起，为此构建二分类的神经网络单元，并以Sigmoid函数和平方差损失（比较常用的还有交叉熵损失函数）函数来举例说明<a href="http://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA5ODUxOTA5Mg%3D%3D%26mid%3D2652550294%26idx%3D1%26sn%3D820ddc89e1d1af35f14ccf645b963a76%26chksm%3D8b7e45cdbc09ccdb985b3bbc22fbc0dcd013d53e9e9a6073d09d1b676b338af7bd8b7dd2a92d%26mpshare%3D1%26scene%3D1%26srcid%3D0930LxixeTcq5wCcRStBTylE%26pass_ticket%3Dw2yCF%252F3Z2KTqyWW%252FUwkvnidRV3HF9ym5iEfJ%252BZ1dMObpcYUW3hQymA4BpY9W3gn4%23rd" class=" wrap external" target="_blank" rel="nofollow noreferrer">梯度下降法</a>以及基于链式法则的反向传播（BP），所有涉及到的公式都在这里：</p><figure data-size="normal"><img src="https://pic3.zhimg.com/v2-eabfde057dc289980a47b30cb4f13d21_1440w.jpg" data-rawwidth="921" data-rawheight="446" data-size="normal" data-caption class="origin_image zh-lightbox-thumb" data-original="https://pic3.zhimg.com/v2-eabfde057dc289980a47b30cb4f13d21_r.jpg" referrerpolicy="no-referrer"></figure><p><br></p><p data-pid="RzOqY_mM"><b>2. </b>神经元中的非线性变换激活函数（<a href="http://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzI1NTE4NTUwOQ%3D%3D%26mid%3D2650325236%26idx%3D1%26sn%3D7bd8510d59ddc14e5d4036f2acaeaf8d%26mpshare%3D1%26scene%3D1%26srcid%3D1214qIBJrRhevScKXQQuqas4%26pass_ticket%3Dw2yCF%252F3Z2KTqyWW%252FUwkvnidRV3HF9ym5iEfJ%252BZ1dMObpcYUW3hQymA4BpY9W3gn4%23rd" class=" wrap external" target="_blank" rel="nofollow noreferrer">深度学习中的激活函数导引</a>）及其作用（参考<a href="https://www.zhihu.com/people/yan-qin-rui" class="internal">颜沁睿</a>的<a href="https://www.zhihu.com/question/22334626/answer/103835591" class="internal">回答</a>），激活函数是神经网络强大的基础，好的激活函数（根据任务来选择）还可以加速训练：</p><figure data-size="normal"><img src="https://pic4.zhimg.com/v2-9227fb8304a64498209ea07772cdd7e0_1440w.jpg" data-rawwidth="1879" data-rawheight="869" data-size="normal" data-caption class="origin_image zh-lightbox-thumb" data-original="https://pic4.zhimg.com/v2-9227fb8304a64498209ea07772cdd7e0_r.jpg" referrerpolicy="no-referrer"></figure><p data-pid="TzDlvlS9">不同的激活函数搭配不同的参数初始化策略，比如Tahn和<a href="http://link.zhihu.com/?target=http%3A//jmlr.csail.mit.edu/proceedings/papers/v9/glorot10a/glorot10a.pdf%3Forigin%3Dpublication_detail" class=" wrap external" target="_blank" rel="nofollow noreferrer">Xavier初始化</a>方法:</p><figure data-size="normal"><img src="https://pic4.zhimg.com/v2-326600169cb13d57c7ac1f05f46cdfd3_720w.jpg" data-rawwidth="327" data-rawheight="64" data-size="normal" data-caption class="content_image" referrerpolicy="no-referrer"></figure><p data-pid="YPmiRn01">以及ReLU和MSRAFiller初始化(<a href="http://link.zhihu.com/?target=http%3A//xueshu.baidu.com/s%3Fwd%3Dpaperuri%253A%2528d7da5edfb3250f7fbbdeaab4e0d82ee9%2529%26filter%3Dsc_long_sign%26tn%3DSE_xueshusource_2kduw22v%26sc_vurl%3Dhttp%253A%252F%252Farxiv.org%252Fabs%252F1502.01852%26ie%3Dutf-8%26sc_us%3D9914304850284386500" class=" wrap external" target="_blank" rel="nofollow noreferrer">S</a>r<a href="http://link.zhihu.com/?target=http%3A//xueshu.baidu.com/s%3Fwd%3Dpaperuri%253A%2528d7da5edfb3250f7fbbdeaab4e0d82ee9%2529%26filter%3Dsc_long_sign%26tn%3DSE_xueshusource_2kduw22v%26sc_vurl%3Dhttp%253A%252F%252Farxiv.org%252Fabs%252F1502.01852%26ie%3Dutf-8%26sc_us%3D9914304850284386500" class=" wrap external" target="_blank" rel="nofollow noreferrer">urpassing Human</a>)方法。</p><p data-pid="If5sZ2KY"><b>3.</b> 前馈性神经网络和自动编码器的区别在于输出层，从而引出无监督学习的概念；而降噪编码器和自动编码器的区别又在输入层，即对输入进行部分遮挡或加入噪声；稀疏编码器（引出<a href="http://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA5ODUxOTA5Mg%3D%3D%26mid%3D2652549828%26idx%3D1%26sn%3D5b77192a91d593342e00a984f1132c50%26mpshare%3D1%26scene%3D1%26srcid%3D0802dqmt7jHAo8DZuBVYHO7T%26pass_ticket%3Dw2yCF%252F3Z2KTqyWW%252FUwkvnidRV3HF9ym5iEfJ%252BZ1dMObpcYUW3hQymA4BpY9W3gn4%23rd" class=" wrap external" target="_blank" rel="nofollow noreferrer">正则项</a>的概念）和自动编码器的区别在隐藏层，即隐藏层的节点数大于输入层节点数；而编码器都属于无监督学习的范畴。浅层网络的不断栈式叠加构成相应的深度网络。</p><figure data-size="normal"><img src="https://pic1.zhimg.com/v2-d8190365ba77b450d64de91f4538e2a9_1440w.jpg" data-rawwidth="929" data-rawheight="446" data-size="normal" data-caption class="origin_image zh-lightbox-thumb" data-original="https://pic1.zhimg.com/v2-d8190365ba77b450d64de91f4538e2a9_r.jpg" referrerpolicy="no-referrer"></figure><p data-pid="p3UNOL6I">值得一提的是，三层前馈型神经网络（只包含一个隐藏层）的<a href="http://link.zhihu.com/?target=http%3A//jalammar.github.io/illustrated-word2vec/" class=" wrap external" target="_blank" rel="nofollow noreferrer">word2vec</a>是迈向NLP的大门，包括CBOW和skip-gram两种模型，另外在输出层还分别做了基于Huffman树的Hierarchical Softmax以及negative sampling（就是选择性地更新连接负样本的权重参数）的加速。</p><p><br></p><p data-pid="Rzw9v7CO"><b>4. </b>受限波兹曼机RBM属于无监督学习中的生成学习，输入层和隐藏层的传播是双向的，分正向过程和反向过程，学习的是数据分布，因此又引出马尔可夫过程和Gibbs采样的概念，以及KL散度的度量概念：</p><figure data-size="normal"><img src="https://pic2.zhimg.com/v2-cd53ee50bf533b453408318d3f13dc73_1440w.jpg" data-rawwidth="940" data-rawheight="434" data-size="normal" data-caption class="origin_image zh-lightbox-thumb" data-original="https://pic2.zhimg.com/v2-cd53ee50bf533b453408318d3f13dc73_r.jpg" referrerpolicy="no-referrer"></figure><p data-pid="6CyAvgRC">与生成学习对应的是判别学习也就是大多数的分类器，生成对抗网络GAN融合两者；对抗是指生成模型与判别模型的零和博弈，近两年最激动人心的应用是从文本生成图像（<a href="http://link.zhihu.com/?target=http%3A//www.evolvingai.org/ppgn" class=" wrap external" target="_blank" rel="nofollow noreferrer">Evolving AI Lab - University of Wyoming</a>）（参考代码：<a href="http://link.zhihu.com/?target=https%3A//github.com/paarthneekhara/text-to-image" class=" wrap external" target="_blank" rel="nofollow noreferrer">text-to-image</a>）：</p><figure data-size="normal"><img src="https://pic1.zhimg.com/v2-53b180b2f2b7e9f37f712c8ae12e8f43_1440w.jpg" data-rawwidth="1279" data-rawheight="599" data-size="normal" data-caption class="origin_image zh-lightbox-thumb" data-original="https://pic1.zhimg.com/v2-53b180b2f2b7e9f37f712c8ae12e8f43_r.jpg" referrerpolicy="no-referrer"></figure><p data-pid="VTuMr9sA">再推荐一个讲DCGAN的超赞教程<a href="http://link.zhihu.com/?target=http%3A//bamos.github.io/2016/08/09/deep-completion/" class=" wrap external" target="_blank" rel="nofollow noreferrer">Image Completion with Deep Learning in TensorFlow</a> （参考代码：<a href="http://link.zhihu.com/?target=https%3A//github.com/bamos/dcgan-completion.tensorflow" class=" wrap external" target="_blank" rel="nofollow noreferrer">dcgan-completion.tensorflow</a>）：</p><figure data-size="normal"><img src="https://pic1.zhimg.com/v2-c26e080dd30e8a07d31173578c52ae07_1440w.jpg" data-rawwidth="815" data-rawheight="336" data-size="normal" data-caption class="origin_image zh-lightbox-thumb" data-original="https://pic1.zhimg.com/v2-c26e080dd30e8a07d31173578c52ae07_r.jpg" referrerpolicy="no-referrer"></figure><p data-pid="42X3lJvM"><b>5. </b>深度网络的实现基于逐层贪心训练算法，而随着模型的深度逐渐增加，会产生梯度消失或梯度爆炸的问题，梯度爆炸一般采用阈值截断的方法解决，而梯度消失不易解决；网络越深，这些问题越严重，这也是深度学习的核心问题，出现一系列技术及衍生模型。</p><figure data-size="normal"><img src="https://pic4.zhimg.com/v2-247ecfca25fcad04aa4122eb1e892765_1440w.jpg" data-rawwidth="869" data-rawheight="441" data-size="normal" data-caption class="origin_image zh-lightbox-thumb" data-original="https://pic4.zhimg.com/v2-247ecfca25fcad04aa4122eb1e892765_r.jpg" referrerpolicy="no-referrer"></figure><p data-pid="XW2sdbKM">深度制胜，网络越深越好，因此有了<a href="http://link.zhihu.com/?target=http%3A//xueshu.baidu.com/s%3Fwd%3Dpaperuri%253A%25283821a90f58762386e257eb4e6fa11f79%2529%26filter%3Dsc_long_sign%26tn%3DSE_xueshusource_2kduw22v%26sc_vurl%3Dhttp%253A%252F%252Farxiv.org%252Fabs%252F1512.03385%26ie%3Dutf-8%26sc_us%3D13213678896270879240" class=" wrap external" target="_blank" rel="nofollow noreferrer">深度残差网络</a>将深度扩展到152层，并在ImageNe多项竞赛任务中独孤求败。</p><p data-pid="rxdehGXK">随着网络的加深和对训练加速收敛的要求，提出了各种Normalization技术，比如Batch Normalization、Layer Normalization、Instance Normalization和Group Normalization，既然是归一化，肯定涉及到在定义集合内的均值和方差计算，关于集合的定义以及各种Normalization技术为什么可以加速训练模型收敛，这批文章有详细的解释：<a href="http://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3MzI4MjgzMw%3D%3D%26mid%3D2650747731%26idx%3D3%26sn%3D61e24eaac7f274bd6b311b3007ebbbaf%26chksm%3D871af72db06d7e3ba7dc9d608abe5602adcd5950dea35e6d51fa81c3f88b5e35b2d5aa1f0cb2%26mpshare%3D1%26scene%3D1%26srcid%3D0829tNy62XGJ3t6KWpN7bUb1%26sharer_sharetime%3D1580699380879%26sharer_shareid%3D512a61bb0cb56749a7cbb827c71dce88%26exportkey%3DASBize64H6cUhPexizCg9I8%253D%26pass_ticket%3D6uXzkDUz3DeVmRB5w6pr16wV6QsFnCFYLEo%252F1I5JHVLatlioiS6Oez8Hgc7DPhLy%23rd" class=" wrap external" target="_blank" rel="nofollow noreferrer">专栏 | 深度学习中的Normalization模型</a></p><p data-pid="EHEojCz0"><b>6.</b> 卷积神经网络在层与层之间采取局部链接的方式，即卷积层和采样层，在计算机视觉的相关任务上有突出表现，关于卷积神经网络的更多介绍请参考我的另一篇文章（<a href="https://zhuanlan.zhihu.com/p/21699462" class="internal">戳戳戳</a>）：</p><figure data-size="normal"><img src="https://pic4.zhimg.com/v2-aa4855dc4cf11dc0bff5c131a278c4e9_1440w.jpg" data-rawwidth="1845" data-rawheight="877" data-size="normal" data-caption class="origin_image zh-lightbox-thumb" data-original="https://pic4.zhimg.com/v2-aa4855dc4cf11dc0bff5c131a278c4e9_r.jpg" referrerpolicy="no-referrer"></figure><p><br></p><p data-pid="GF1ZvuJL">而在NIPS 2016上来自康奈尔大学计算机系的副教授 Killan Weinberger 探讨了深度极深的卷积网络，在数据集CIFAR-10 上训练一个 <a href="http://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1603.09382v3.pdf" class=" wrap external" target="_blank" rel="nofollow noreferrer">1202 层深的网络</a>。</p><p data-pid="BXOsE6IT">CNN的历史进程，一图总结：</p><figure data-size="normal"><img src="https://pic3.zhimg.com/v2-dd092add7fc31fb84fab66499fe29142_1440w.jpg" data-rawwidth="836" data-rawheight="382" data-size="normal" data-caption class="origin_image zh-lightbox-thumb" data-original="https://pic3.zhimg.com/v2-dd092add7fc31fb84fab66499fe29142_r.jpg" referrerpolicy="no-referrer"></figure><p data-pid="A-SV83Js">近年来，CNN的研究更加深入，应用范围也更广泛。包括卷积核大小的研究，进一步降低模型参数，2012年在Network In Network中提出了1*1卷积核，而Google2014年提出的Inception模型则发扬了它的作用；还有卷积相邻点步长的研究，2014提出了dilated CNN模型，可以增加感受野，利用更长的上下文信息，适合于图像语义分割（<a href="http://link.zhihu.com/?target=http%3A//xueshu.baidu.com/s%3Fwd%3Dpaperuri%253A%2528faeb25fe8d7c883f1a326d5fbbcbf029%2529%26filter%3Dsc_long_sign%26tn%3DSE_xueshusource_2kduw22v%26sc_vurl%3Dhttp%253A%252F%252Farxiv.org%252Fabs%252F1412.7062%26ie%3Dutf-8%26sc_us%3D10672394650882731083" class=" wrap external" target="_blank" rel="nofollow noreferrer">Semantic Image Segmentation with Deep ...</a>）任务，并将CNN拓展到了语音领域，2016年Google提出用于语音合成的Wavenet（wavenet-generative-model-raw-audio）模型，尽管在paper中只是略加提及，但Wavenet也可以用于语音识别（英文语音识别，参考代码：<a href="http://link.zhihu.com/?target=https%3A//github.com/buriburisuri/speech-to-text-wavenet" class=" wrap external" target="_blank" rel="nofollow noreferrer">speech-to-text-wavenet</a>；<b>中文语音识别</b>：<a href="https://zhuanlan.zhihu.com/p/27064536?group_id=851279135044153344" class="internal">用Wavenet做中文语音识别 - 知乎专栏</a>）：</p><p><br></p><figure data-size="normal"><img src="https://pic2.zhimg.com/v2-a8a879db7f3b690dbf776160d95e7bcd_1440w.jpg" data-rawwidth="1724" data-rawheight="930" data-size="normal" data-caption class="origin_image zh-lightbox-thumb" data-original="https://pic2.zhimg.com/v2-a8a879db7f3b690dbf776160d95e7bcd_r.jpg" referrerpolicy="no-referrer"></figure><p data-pid="cs1R76dV">而原始的Wavenet模型存在大量重复计算，当层数叠加的时候，效率降低十分明显：</p><figure data-size="normal"><img src="https://pic2.zhimg.com/v2-38668ad1865652ae7a9817fa45d979ee_720w.jpg" data-rawwidth="403" data-rawheight="321" data-size="normal" data-caption class="content_image" referrerpolicy="no-referrer"></figure><p data-pid="6rT4iuuf">因此2016年又提出了fast Wavenet模型（参考代码：<a href="http://link.zhihu.com/?target=https%3A//github.com/tomlepaine/fast-wavenet" class=" wrap external" target="_blank" rel="nofollow noreferrer">fast-wavenet</a>）。</p><p><br></p><p data-pid="DW7wsJQQ"><b>7.</b> 循环神经网络在隐藏层之间建立了链接，以利用时间维度上的历史信息和未来信息，与此同时在时间轴上也会产生梯度消失和梯度爆炸现象，而LSTM和GRU则在一定程度上解决了这个问题，两者与经典RNN的区别在隐藏层的神经元内部结构，在语音识别，NLP（比如RNNLM）和机器翻译上有突出表现（<a href="http://link.zhihu.com/?target=http%3A//karpathy.github.io/2015/05/21/rnn-effectiveness/" class=" wrap external" target="_blank" rel="nofollow noreferrer">推荐阅读</a>）：</p><p data-pid="l-mEVcvH">除了RNNLM采用最简单最经典的RNN模型，其他任务隐层神经元往往采用LSTM或者GRU形式，关于LSTM的进化历史，一图胜千言，更多内容可以参阅<a href="http://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1503.04069.pdf" class=" wrap external" target="_blank" rel="nofollow noreferrer">LSTM: A Search Space Odyssey</a>：</p><figure data-size="normal"><img src="https://pic4.zhimg.com/v2-76ed6e05615409d1069793b21bd18a08_1440w.jpg" data-rawwidth="912" data-rawheight="427" data-size="normal" data-caption class="origin_image zh-lightbox-thumb" data-original="https://pic4.zhimg.com/v2-76ed6e05615409d1069793b21bd18a08_r.jpg" referrerpolicy="no-referrer"></figure><p data-pid="aqCLJ6eM">RNN模型在一定程度上也算是分类器，在图像描述（<a href="http://link.zhihu.com/?target=http%3A//cs.stanford.edu/people/karpathy/deepimagesent/" class=" wrap external" target="_blank" rel="nofollow noreferrer">Deep Visual-Semantic Alignments for Generating Image Descriptions</a>）的任务中已经取得了不起的成果（第四节GAN用文本生成图像是逆过程，注意区别）：</p><figure data-size="normal"><img src="https://pic3.zhimg.com/v2-efb1bc6baa74b79ab5ff2757e75f3337_1440w.jpg" data-rawwidth="1271" data-rawheight="585" data-size="normal" data-caption class="origin_image zh-lightbox-thumb" data-original="https://pic3.zhimg.com/v2-efb1bc6baa74b79ab5ff2757e75f3337_r.jpg" referrerpolicy="no-referrer"></figure><p><br></p><p data-pid="hwQCvEZK">另外，关于RNN的最新研究是基于attention机制来建立模型（<a href="http://link.zhihu.com/?target=http%3A//distill.pub/2016/augmented-rnns/" class=" wrap external" target="_blank" rel="nofollow noreferrer">推荐阅读文章</a>），即能够在时间轴上选择有效信息加以利用，比如百度App中的"<a href="http://link.zhihu.com/?target=https%3A//arxiv.org/abs/1610.09889" class=" wrap external" target="_blank" rel="nofollow noreferrer">为你写诗</a>"的功能核心模型就是attention-based RNN encoder-decoder：</p><figure data-size="normal"><img src="https://pic2.zhimg.com/v2-3463b8dbd7ed0e92c21d5f73043513fe_1440w.jpg" data-rawwidth="1086" data-rawheight="542" data-size="normal" data-caption class="origin_image zh-lightbox-thumb" data-original="https://pic2.zhimg.com/v2-3463b8dbd7ed0e92c21d5f73043513fe_r.jpg" referrerpolicy="no-referrer"></figure><figure data-size="normal"><img src="https://pic4.zhimg.com/v2-205b838a0cb13544f14f25e28d9bb8b6_1440w.jpg" data-rawwidth="896" data-rawheight="386" data-size="normal" data-caption class="origin_image zh-lightbox-thumb" data-original="https://pic4.zhimg.com/v2-205b838a0cb13544f14f25e28d9bb8b6_r.jpg" referrerpolicy="no-referrer"></figure><p data-pid="codxnu2b">再推荐一篇关于音乐合成的博客：<a href="http://link.zhihu.com/?target=http%3A//www.hexahedria.com/2015/08/03/composing-music-with-recurrent-neural-networks/" class=" wrap external" target="_blank" rel="nofollow noreferrer">Composing Music With Recurrent Neural Networks</a>。</p><p data-pid="GkXjaHln">基于Attention的模型Transformer是论文<a href="http://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1706.03762.pdf" class=" wrap external" target="_blank" rel="nofollow noreferrer">Attention Is All You Need</a>　提出的，推荐一篇超赞且详细的解读博客：<a href="http://link.zhihu.com/?target=http%3A//jalammar.github.io/illustrated-transformer/" class=" wrap external" target="_blank" rel="nofollow noreferrer">The Illustrated Transformer</a></p><p data-pid="y2fKjhPQ"><b>8.</b> 总结了深度学习中的基本模型并再次解释部分相关的技术概念：</p><figure data-size="normal"><img src="https://pic4.zhimg.com/v2-487f13a5de2ef2105a90be878e5f3ed5_1440w.jpg" data-rawwidth="1249" data-rawheight="607" data-size="normal" data-caption class="origin_image zh-lightbox-thumb" data-original="https://pic4.zhimg.com/v2-487f13a5de2ef2105a90be878e5f3ed5_r.jpg" referrerpolicy="no-referrer"></figure><p><br></p><p data-pid="eYhmQuhH">关于模型的演化及其在不同任务（比如目标检测、图像分割等）中的应用可以再换个维度阅读这篇文章：　<a href="http://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzI3MTA0MTk1MA%3D%3D%26mid%3D2652012167%26idx%3D3%26sn%3D3da00a2bbef4c3b9a548e0cea4b8220d%26chksm%3Df1210a76c6568360eeb30e391f170c1839c4121b42c38f2e1b1dfe9370b49fb080425f555cdc%26mpshare%3D1%26scene%3D24%26srcid%3D01246cMEQCIiaOpgGSXqj0Jb%26exportkey%3DAey7aHrcwQVX%252F5BtZvBNm9o%253D%26pass_ticket%3Doa7MT2LeGbazufaFa467iLzBkpOCZuIBb2ft9v58GZ1CqMW9C15SSn0B7a%252BZ5vx3%23rd" class=" wrap external" target="_blank" rel="nofollow noreferrer">【计算机视觉必读干货】图像分类、定位、检测，语义分割和实例分割方法梳理</a>，百读不厌，愈久弥新。</p><p data-pid="4PuMtDoo">最后，现在深度学习在工业中的应用往往是整合多个模型到产品中去，比如在语音识别的端到端系统中，利用无监督模型或者CNN作为前期处理提取特征，然后用RNN模型进行逻辑推理和判断，从而达到可媲美人类交流的水平，如百度的<a href="http://link.zhihu.com/?target=http%3A//xueshu.baidu.com/s%3Fwd%3Dpaperuri%253A%25281ba47fa102a2d61cb4a8a5d85049707c%2529%26filter%3Dsc_long_sign%26tn%3DSE_xueshusource_2kduw22v%26sc_vurl%3Dhttp%253A%252F%252Farxiv.org%252Fabs%252F1512.02595%26ie%3Dutf-8%26sc_us%3D8168572815923394227" class=" wrap external" target="_blank" rel="nofollow noreferrer">DeepSpeech2</a>:</p><p data-pid="mHrPi-eY">画图是个细活慢活，周末加班很辛苦，觉得好就动动手指给个赞吧。</p><p data-pid="cnRZfv0T">不喜请喷，转载请注明出处，谢谢。</p>  
</div>
            