
---
title: '人工智能领域有哪些精妙的数学原理？'
categories: 
 - 社交媒体
 - 知乎
 - 知乎热榜
headimg: 'https://www.zhihu.com/equation?tex=X'
author: 知乎
comments: false
date: Thu, 06 Jan 2022 05:06:49 GMT
thumbnail: 'https://www.zhihu.com/equation?tex=X'
---

<div>   
武辰的回答<br><br><p data-pid="3TKhH2mO">首先说一个很粗糙的小结论。</p><p data-pid="ASZbYb2h">我去年做了一个预测年龄的任务。任务的输入是一张人脸图片，输出是年龄。我构造了一个基于深度学习的回归模型。模型训练完成，在测试集的的MAE是4（岁）。</p><p data-pid="3xmbmv8H">我向产品经理汇报：“模型的平均预测误差是4岁”，产品经理似乎不清楚“平均预测误差”的含义，她问我“这意味着所有人的年龄误差都在4岁以内吗？”</p><p data-pid="B2ROdZUf">我向产品经理解释了“平均预测误差”的含义。产品经理表示仍然更希望知道：模型能够保证多少比例的数据落在x岁误差以内。</p><p data-pid="fA_du58V">某种意义来说，“平均预测误差”是积分的结果，“多少比例的数据落在x岁误差以内”也是积分的结果。这两者是否有一些联系？</p><p data-pid="ChE-7q54">假设 <img src="https://www.zhihu.com/equation?tex=X" alt="X" eeimg="1" referrerpolicy="no-referrer"> 服从正态分布， <img src="https://www.zhihu.com/equation?tex=%5Cmu" alt="\mu" eeimg="1" referrerpolicy="no-referrer"> 是均值， <img src="https://www.zhihu.com/equation?tex=%5Csigma" alt="\sigma" eeimg="1" referrerpolicy="no-referrer"> 是方差。</p><p data-pid="stsTi-0O">设 <img src="https://www.zhihu.com/equation?tex=E%28%7CX-%5Cmu%7C%29%3Dw" alt="E(|X-\mu|)=w" eeimg="1" referrerpolicy="no-referrer"> </p><p data-pid="wTQHGwzJ">则 <img src="https://www.zhihu.com/equation?tex=E%28%5Cfrac%7B%7CX-%5Cmu%7C%7D%7B%5Csigma%7D%29%3D%5Cfrac%7Bw%7D%7B%5Csigma%7D" alt="E(\frac&#123;|X-\mu|&#125;&#123;\sigma&#125;)=\frac&#123;w&#125;&#123;\sigma&#125;" eeimg="1" referrerpolicy="no-referrer"> </p><p data-pid="ubU345di"><img src="https://www.zhihu.com/equation?tex=E%28%7CX-%5Cmu%7C%29" alt="E(|X-\mu|)" eeimg="1" referrerpolicy="no-referrer">可以粗略地看作MAE</p><p data-pid="UD-y5vZO">根据常用结论，对于标准正态变量Y，可以计算出 <img src="https://www.zhihu.com/equation?tex=E%28%7CY%7C%29%3D%5Csqrt%7B%5Cfrac%7B2%7D%7B%5Cpi%7D%7D%5Capprox0.79" alt="E(|Y|)=\sqrt&#123;\frac&#123;2&#125;&#123;\pi&#125;&#125;\approx0.79" eeimg="1" referrerpolicy="no-referrer"> </p><p data-pid="RUcbASqt">得知 <img src="https://www.zhihu.com/equation?tex=%5Cfrac%7Bw%7D%7B%5Csigma%7D%3D0.79" alt="\frac&#123;w&#125;&#123;\sigma&#125;=0.79" eeimg="1" referrerpolicy="no-referrer"> </p><p data-pid="lc3fSzKM">那么<img src="https://www.zhihu.com/equation?tex=P%28%7CX-%5Cmu%7C%5Cleq+w%29" alt="P(|X-\mu|\leq w)" eeimg="1" referrerpolicy="no-referrer"> </p><p data-pid="HWxW7OMW"><img src="https://www.zhihu.com/equation?tex=%3DP%28%5Cfrac%7B%7CX-%5Cmu%7C%7D%7B%5Csigma%7D%5Cleq+%5Cfrac%7Bw%7D%7B%5Csigma%7D%29" alt="=P(\frac&#123;|X-\mu|&#125;&#123;\sigma&#125;\leq \frac&#123;w&#125;&#123;\sigma&#125;)" eeimg="1" referrerpolicy="no-referrer"> </p><p data-pid="bLp4jDIe"><img src="https://www.zhihu.com/equation?tex=%3DP%28-%5Cfrac%7Bw%7D%7B%5Csigma%7D+%5Cleq+%5Cfrac%7BX-%5Cmu%7D%7B%5Csigma%7D%5Cleq+%5Cfrac%7Bw%7D%7B%5Csigma%7D%29" alt="=P(-\frac&#123;w&#125;&#123;\sigma&#125; \leq \frac&#123;X-\mu&#125;&#123;\sigma&#125;\leq \frac&#123;w&#125;&#123;\sigma&#125;)" eeimg="1" referrerpolicy="no-referrer"> </p><p data-pid="003sSQzL"><img src="https://www.zhihu.com/equation?tex=%3DP%280.79+%5Cleq+%5Cfrac%7BX-%5Cmu%7D%7B%5Csigma%7D%5Cleq+0.79%29" alt="=P(0.79 \leq \frac&#123;X-\mu&#125;&#123;\sigma&#125;\leq 0.79)" eeimg="1" referrerpolicy="no-referrer"> </p><p data-pid="h5IOtE6L">查标准正态分布表可得这个概率值约等于0.56</p><p data-pid="b58yYYzt">这个结论的直观含义是，<b>对于正态分布的随机变量，有56%的概率落在MAE范围之内</b>。</p><p data-pid="RzUmywI1">套在我最初提到的问题：年龄回归模型的MAE是4岁，那么大致可以猜测有56%的数据的预测误差在4岁以内。</p><p data-pid="TZfsq18m">测试集的结果显示，有58%的数据误差落在4岁以内，相差不算大。</p><p data-pid="f_Igb3SL">这是一个非常粗糙的结论。且不说误差是否服从正态分布，它甚至不是随机变量。而且把 <img src="https://www.zhihu.com/equation?tex=E%28%7CX-%5Cmu%7C%29" alt="E(|X-\mu|)" eeimg="1" referrerpolicy="no-referrer"> 看作MAE也是不严谨的。另一方面，广泛意义上的误差，大多是一些因素加和的结果，根据中心极限定理，可以认为误差近似服从正态分布。</p><hr><p data-pid="n3KW9GZu">再补充一个算得上精妙的思想：交叉熵和图像卷积，本质蕴含着排序不等式的思想。</p><p data-pid="HP4Nq0q3">首先从排序不等式开始说起，排序不等式指的是：</p><p data-pid="6YVUUpgz">当 <img src="https://www.zhihu.com/equation?tex=a_%7B1%7D%5Cgeq+a_%7B2%7D%5Cgeq%E2%80%A6%E2%80%A6%5Cgeq+a_%7Bn%7D" alt="a_&#123;1&#125;\geq a_&#123;2&#125;\geq……\geq a_&#123;n&#125;" eeimg="1" referrerpolicy="no-referrer"> ,</p><p data-pid="xOVKQwac"><img src="https://www.zhihu.com/equation?tex=b_%7B1%7D%5Cgeq+b_%7B2%7D%5Cgeq%E2%80%A6%E2%80%A6%5Cgeq+b_%7Bn%7D" alt="b_&#123;1&#125;\geq b_&#123;2&#125;\geq……\geq b_&#123;n&#125;" eeimg="1" referrerpolicy="no-referrer"> ,</p><p data-pid="gaYc_x0m">有<img src="https://www.zhihu.com/equation?tex=%5Csum_%7Bi%3D1%7D%5E%7Bn%7D%7Ba_ib_i%7D" alt="\sum_&#123;i=1&#125;^&#123;n&#125;&#123;a_ib_i&#125;" eeimg="1" referrerpolicy="no-referrer"><img src="https://www.zhihu.com/equation?tex=%5Cgeq%5Csum_%7Bi%3D1%7D%5E%7Bn%7D%7Ba_ib_%7Bj%28i%29%7D%7D%5Cgeq%5Csum_%7Bi%3D1%7D%5E%7Bn%7D%7Ba_ib_%7Bn%2B1-i%7D%7D" alt="\geq\sum_&#123;i=1&#125;^&#123;n&#125;&#123;a_ib_&#123;j(i)&#125;&#125;\geq\sum_&#123;i=1&#125;^&#123;n&#125;&#123;a_ib_&#123;n+1-i&#125;&#125;" eeimg="1" referrerpolicy="no-referrer">，</p><p data-pid="skxqOmID">其中<img src="https://www.zhihu.com/equation?tex=j%281%29%2Cj%282%29%2C...j%28n%29" alt="j(1),j(2),...j(n)" eeimg="1" referrerpolicy="no-referrer">是<img src="https://www.zhihu.com/equation?tex=1%2C2%2C...n" alt="1,2,...n" eeimg="1" referrerpolicy="no-referrer">的一个置换。</p><p data-pid="rccd1Oka">简而言之，就是<b>顺序和≥乱序和≥倒序和</b>。</p><p data-pid="dojYExZA">深度学习中的交叉熵损失函数。 <img src="https://www.zhihu.com/equation?tex=p%28x_i%29" alt="p(x_i)" eeimg="1" referrerpolicy="no-referrer"> 是标签label， <img src="https://www.zhihu.com/equation?tex=q%28x_i%29" alt="q(x_i)" eeimg="1" referrerpolicy="no-referrer"> 是模型的预测结果。</p><p data-pid="uZ9FW4Ez">交叉熵<img src="https://www.zhihu.com/equation?tex=H%28p%2Cq%29%3D-%5Csum_%7Bi%3D1%7D%5En+p%28x_i%29log%28q%28x_i%29%29" alt="H(p,q)=-\sum_&#123;i=1&#125;^n p(x_i)log(q(x_i))" eeimg="1" referrerpolicy="no-referrer"></p><p data-pid="d_HaRRp6">损失函数越小，意味着预测与真实标签越接近。此时 <img src="https://www.zhihu.com/equation?tex=%5Csum_%7Bi%3D1%7D%5En+p%28x_i%29log%28q%28x_i%29%29" alt="\sum_&#123;i=1&#125;^n p(x_i)log(q(x_i))" eeimg="1" referrerpolicy="no-referrer"> 应该越大。根据排序不等式，对于任何一个 <img src="https://www.zhihu.com/equation?tex=i" alt="i" eeimg="1" referrerpolicy="no-referrer"> ，当 <img src="https://www.zhihu.com/equation?tex=p%28x_i%29" alt="p(x_i)" eeimg="1" referrerpolicy="no-referrer"> 比较大时，<img src="https://www.zhihu.com/equation?tex=q%28x_i%29" alt="q(x_i)" eeimg="1" referrerpolicy="no-referrer"> 也应该比较大，反之同理。这是符合直觉的，当某一个类的真实label<img src="https://www.zhihu.com/equation?tex=p%28x_i%29" alt="p(x_i)" eeimg="1" referrerpolicy="no-referrer">很大时，预测分数<img src="https://www.zhihu.com/equation?tex=q%28x_i%29" alt="q(x_i)" eeimg="1" referrerpolicy="no-referrer"> 也应该比较高，这才能保证损失函数较小。</p><p data-pid="3Mp3NOot">最后说卷积神经网络中的卷积，卷积层的输入是图像，图像的数值一般代表灰度值。比如灰度图的数值取值在[0,255]区间中，数值越高，代表颜色越白。</p><p data-pid="67BmaVNu">卷积本质上是<b>提取特征</b>的工具。比如下面的卷积核用来识别一个像素是否具有“左边周围的灰度值与右边周围的灰度值有较大差异”的<b>模式</b>或<b>特征。</b>假如图片的某个区域中，右边的灰度值高于左边的灰度值，那么根据排序不等式的原理，经过卷积的运算，得到的乘积结果会比较大。</p><figure data-size="normal"><img src="https://pic3.zhimg.com/v2-e8cbc98a53907e4d2732cdf20480fb6d_720w.jpg" data-caption data-size="normal" data-rawwidth="110" data-rawheight="83" class="content_image" referrerpolicy="no-referrer"></figure><p data-pid="3sIPgpxH">同理，下面的卷积核用来识别一个像素是否具有“上面周围的灰度值与下面周围的灰度值有较大差异”的<b>模式</b>或<b>特征。</b></p><figure data-size="normal"><img src="https://pic4.zhimg.com/v2-771783abf4622a56a892e76c568b65cb_720w.jpg" data-caption data-size="normal" data-rawwidth="125" data-rawheight="78" class="content_image" referrerpolicy="no-referrer"></figure><p data-pid="8cRbpzOq">根据排序不等式的原理，卷积后的数值越高，说明此<b>模式</b>越明显，即卷积成功提取到了<b>特征，</b>或者说找到了<b>规律。</b></p><hr><p data-pid="yiZhEPCJ">借楼再探讨一个问题：batchnormalization和relu是否不适用于回归任务？我在年龄预测的任务中发现，去掉BN、将tanh替换relu能够极大地改善模型的效果。</p>  
</div>
            