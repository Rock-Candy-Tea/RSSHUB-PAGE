
---
title: '如何评价谷歌用30亿数据训练的20亿参数ViT-G_14模型在 ImageNet 达到新的 SOTA？'
categories: 
 - 社交媒体
 - 知乎
 - 知乎热榜
headimg: 'https://pic2.zhimg.com/v2-1a0cb0668fe7a65b8e2edd1e22706141_1440w.jpg'
author: 知乎
comments: false
date: Sun, 13 Jun 2021 00:14:39 GMT
thumbnail: 'https://pic2.zhimg.com/v2-1a0cb0668fe7a65b8e2edd1e22706141_1440w.jpg'
---

<div>   
小小将的回答<br><br><p><b>质朴的ViT is  enough！</b></p><p>看了一下作者，和ViT模型是同名的，这个工作发布了包含20亿参数的vision transformer模型ViT-G/14，在ImageNet的Top-1可以达到90.45%，超过之前谷歌提出的 Meta Pseduo Labels模型：</p><figure data-size="normal"><img src="https://pic2.zhimg.com/v2-1a0cb0668fe7a65b8e2edd1e22706141_1440w.jpg" data-rawwidth="1080" data-rawheight="373" data-size="normal" data-caption data-default-watermark-src="https://pic2.zhimg.com/v2-8c5a0b5991128e71e463eed62d7e1e14_720w.jpg" class="origin_image zh-lightbox-thumb" data-original="https://pic2.zhimg.com/v2-1a0cb0668fe7a65b8e2edd1e22706141_r.jpg" referrerpolicy="no-referrer"></figure><p>但其实谷歌这篇论文的重点是研究<b>vision transformer模型的scaling laws</b>，在NLP领域已经有研究（Scaling laws for neural language models）给出了语言模型效果和 compute, data size, model size之间的指数定律，更有GPT-3这样成功的模型。虽然已经有论文研究（如EfficientNet）CNN模型的scaling strategy：模型增大提升效果。但是在CV领域还是缺少比较全面的研究，而且最近vision transformer的成功应用更是需要这样的工作。</p><p>在这篇论文中，<b>实验模型参数从5M到2B，训练数据量从30M到3B，训练时长从1 TPUv3 core-day到10 000 core-days</b>。这使得谷歌比较全面地研究ViT模型效果和model size，data size和compute之间的<b>scaling laws。</b></p><p>论文中<b>采用的是ViT模型</b>，并增加了G/14这样超大的模型（接近2B)，不同大小的模型如下所示，主要区别在patch size以及transformer layer的参数配置。</p><figure data-size="normal"><img src="https://pic2.zhimg.com/v2-c05e6957d695af660c8502d34e2be515_1440w.jpg" data-rawwidth="572" data-rawheight="494" data-size="normal" data-caption data-default-watermark-src="https://pic4.zhimg.com/v2-169ac25007e9c2dc66b0909a730539db_720w.jpg" class="origin_image zh-lightbox-thumb" data-original="https://pic2.zhimg.com/v2-c05e6957d695af660c8502d34e2be515_r.jpg" referrerpolicy="no-referrer"></figure><p>在数据方面，谷歌又抛出重磅炸弹：<b>JFT-3B</b>，这个是JFT-300M的超大版本，<b>包含接近30亿的图像，标注为包含30k类别的层级类别</b>，由于采用半自动标注，所以标注是有噪音的。具体到训练模型，直接采用sigmoid cross-entropy损失训练多分类模型，忽略类别间层级关系。所有的测试数据均从JFT-3B中移除。</p><p>据此，实验分别改变architecture size，number of training images和training duration来测试模型的representation quality，具体可以用(i) few-shot transfer via training a linear classifier on frozen weights, (ii) transfer via fine-tuning the whole model on all data, both to multiple benchmark tasks来作为评价指标，下图是在ImageNet数据集上的结果：</p><figure data-size="normal"><img src="https://pic2.zhimg.com/v2-49287f3f839268769f0f82ee6482d3bd_1440w.jpg" data-rawwidth="1012" data-rawheight="642" data-size="normal" data-caption data-default-watermark-src="https://pic4.zhimg.com/v2-f9268c9c2dba6287ebce0c1cfa1df681_720w.jpg" class="origin_image zh-lightbox-thumb" data-original="https://pic2.zhimg.com/v2-49287f3f839268769f0f82ee6482d3bd_r.jpg" referrerpolicy="no-referrer"></figure><p>最终的结论主要有三点：</p><ul><li><b>scaling up compute, model and data together improves representation quality</b>：同时增大模型，训练数据和计算力是可以同步提升效果的，近似满足指数定律（log-log曲线是线性的）；</li><li><b>representation quality can be bottlenecked by model size</b>：模型大小会限制上限，小的模型即使用再大的数据集也难以继续提升；</li><li><b>large models benefit from additional data, even beyond 1B images</b>：对于大模型来说，训练数据会制约性能上限，对于最大的模型，训练数据量从1B提升至3B，效果还有提升。</li></ul><p>虽然从实验看来，ViT模型也满足指数scaling定律，但是其实是<b>double-saturating power law</b>：对于最小的模型，其效果会比指数定律预测值要好，这是因为模型效果也有下限；而最大的模型即使给再多的训练数据和算力也不会达到0 error，模型也有上限。所以出现了双饱和。</p><p>除了上述结论，论文还发现了额外的结论：<b>bigger models are more sample efficient</b>，即大的模型需要更少的unseen数据就可以达到和小模型类似的效果，如下图所示：</p><figure data-size="normal"><img src="https://pic4.zhimg.com/v2-a2b6be203f39c957ed1ea2b2680b3b07_1440w.jpg" data-rawwidth="1056" data-rawheight="551" data-size="normal" data-caption data-default-watermark-src="https://pic2.zhimg.com/v2-3b0b6fafcee7d73967f589652d6b0d9f_720w.jpg" class="origin_image zh-lightbox-thumb" data-original="https://pic4.zhimg.com/v2-a2b6be203f39c957ed1ea2b2680b3b07_r.jpg" referrerpolicy="no-referrer"></figure><p><br></p><p>最大的模型ViT-G/14，接近2B参数，在ImageNet上finetune后top-1 acc可达90.45%，而且在few-shot learning也表现优异，每个类只用10个图像训练就能在ImageNet上达到84.86%。</p><p>另外，在训练ViT模型，论文中还设计了一些训练策略来提升内存利用和模型效果，这些策略也使得ViT-G/14可以采用数据并行训练策略，这意味着ViT-G/14模型可以放在一张TPUv3 core。具体策略包括：</p><ul><li><b>Decoupled weight decay for the “head”：</b>模型的head<b>（</b>分类的linear层）和模型的主体body部分采用不同的weight decay，论文中发现head采用较大的weight decay有助于迁移到下游任务（可能和SVM类似，拉大类间间距）；</li><li><b>Saving memory by removing the [class] token</b>：采用multihead attention pooling (MAP) 来替换class token，class token会使得TPU需要padding而增加50%内存使用；</li><li><b>Memory-efficient optimizers</b>：对于Adam优化器，采用half-precision momentum，并采用Adafactor优化器（进行了修改）来进一步减少内存使用；</li><li><b>Learning-rate schedule</b>：学习速率learning-rate schedules引入cooldown阶段（学习速率逐渐降为0），这样可以一次训练就可以得到不同训练时长的模型。</li></ul><p><br></p><figure data-size="normal"><img src="https://pic3.zhimg.com/v2-86b4aaf520eb01593164480f019cd742_1440w.jpg" data-rawwidth="518" data-rawheight="431" data-size="normal" data-caption data-default-watermark-src="https://pic2.zhimg.com/v2-f0f0df6fb55c82f17296f899158af8ce_720w.jpg" class="origin_image zh-lightbox-thumb" data-original="https://pic3.zhimg.com/v2-86b4aaf520eb01593164480f019cd742_r.jpg" referrerpolicy="no-referrer"></figure><p><br></p><p>谷歌做这个实验的代价自然不必说，也有很多人在质疑这个研究的意义，但是论文也在最后给出了这个工作的意义，首先就是这个scaling laws做出来后对后续研究是有启发意义的：</p><blockquote>First, such studies of scaling laws need only be performed once; future developers of ViT models may use our results to design models that can be trained with fewer compute resources. </blockquote><p><br></p><p>此外，这个预训练模型可以迁移到其它任务：</p><p><br></p><blockquote>Second, the models trained are designed primarily for transfer learning. Transfer of pre-trained weights is much less expensive than training from scratch on a downstream task, and typically reaches higher accuracy. </blockquote><p><b>作为一个AI大厂，Google做这么大的work，我个人还是持肯定态度。</b></p><p><b>不过无论是OpenAI的CLIP，还是谷歌的ViT-G/14，其实都是在大规模有监督数据上训练的，而NLP是大规模无监督学习。我觉得这会是一个漫长过程。。</b></p>  
</div>
            