
---
title: '什么是最有效（或价值）的科研？'
categories: 
 - 社交媒体
 - 知乎
 - 知乎热榜
headimg: 'https://pic3.zhimg.com/v2-253168c178ed586060ef39cd2ef33848_1440w.jpg'
author: 知乎
comments: false
date: Fri, 31 Dec 2021 04:50:59 GMT
thumbnail: 'https://pic3.zhimg.com/v2-253168c178ed586060ef39cd2ef33848_1440w.jpg'
---

<div>   
科研汪老徐的回答<br><br><p data-pid="83q709ZV">我在读博的第二年，绞尽脑汁想了个idea，花了几个月写代码跑实验，然后认真的写了篇文章，审稿人来了一句This is an incremental work …… 拒了！</p><p data-pid="hQA7Mpkp">当时英语也不咋地，特意去查了查incremental是啥，牛津词典说这是”增量“的意思，还是很懵逼！请教了一圈终于搞懂了，特别想回怼一句 “你才灌水，你们全家都灌水！”</p><p data-pid="o0PxNTkg">再过了两年，回头看看当时写的那篇文章确实是水文，一篇认认真真做出来的水文！这其实很常见，如果在科研圈待久了，你就会发现发表在顶会牛刊上的文章多数也是水文！</p><figure data-size="normal"><img src="https://pic3.zhimg.com/v2-253168c178ed586060ef39cd2ef33848_1440w.jpg" data-rawwidth="1080" data-rawheight="557" data-size="normal" data-caption class="origin_image zh-lightbox-thumb" data-original="https://pic3.zhimg.com/v2-253168c178ed586060ef39cd2ef33848_r.jpg" referrerpolicy="no-referrer"></figure><p data-pid="-J3exCv-">特斯拉和SpaceX的创始人Elon Musk在2013年接受采访时直言：大多数学术论文都没啥用！这个评论在美版知乎Quora上引起热议，不少科研大佬也都认同马斯克的观点，争论的无非是这些用处不大的论文是否是必须存在的 …</p><p data-pid="QjxcSQvx">那么到底应该怎么评判科研工作的价值？先上个公式：</p><blockquote data-pid="YWPMDv-Y"><img src="https://www.zhihu.com/equation?tex=%E7%A7%91%E7%A0%94%E4%BB%B7%E5%80%BC%3D%E7%A7%91%E7%A0%94%E9%97%AE%E9%A2%98%E4%BB%B7%E5%80%BC%5Ctimes+%E7%A7%91%E7%A0%94%E5%A2%9E%E7%9B%8A" alt="科研价值=科研问题价值\times 科研增益" eeimg="1" referrerpolicy="no-referrer"> </blockquote><p><br></p><hr><h2><b>科研增益</b></h2><p data-pid="NptuvPMh">信息增益是信息论里的一个基础概念，咱们借用它来谈谈一个科研工作带来的价值增益。</p><p data-pid="VAeqVzpO"><b>科研的本质是探索未知！也就是我们要减小对未知的不确定性</b>。</p><p data-pid="FY-327Lo">针对科研问题<img src="https://www.zhihu.com/equation?tex=X" alt="X" eeimg="1" referrerpolicy="no-referrer">，它的不确定性我们用信息熵<img src="https://www.zhihu.com/equation?tex=H%28X%29" alt="H(X)" eeimg="1" referrerpolicy="no-referrer"> 来表示，现在有个工作 <img src="https://www.zhihu.com/equation?tex=Y_1" alt="Y_1" eeimg="1" referrerpolicy="no-referrer"> 提供了针对它的信息，那么这个问题的不确定性就变成了条件熵<img src="https://www.zhihu.com/equation?tex=H%28X%7CY_1%29" alt="H(X|Y_1)" eeimg="1" referrerpolicy="no-referrer">，信息论告诉我们条件熵不大于无条件熵：<img src="https://www.zhihu.com/equation?tex=H%28X%7CY_1%29+%5Cleq+H%28X%29" alt="H(X|Y_1) \leq H(X)" eeimg="1" referrerpolicy="no-referrer">。<img src="https://www.zhihu.com/equation?tex=Y_1" alt="Y_1" eeimg="1" referrerpolicy="no-referrer">带来的信息增益就是<img src="https://www.zhihu.com/equation?tex=H%28X%29-H%28X%7CY_1%29+" alt="H(X)-H(X|Y_1) " eeimg="1" referrerpolicy="no-referrer">。</p><p data-pid="L5M7sf0j">假设 <img src="https://www.zhihu.com/equation?tex=Y_1" alt="Y_1" eeimg="1" referrerpolicy="no-referrer"> 之后又有个新的研究工作 <img src="https://www.zhihu.com/equation?tex=Y_2" alt="Y_2" eeimg="1" referrerpolicy="no-referrer">，那么<img src="https://www.zhihu.com/equation?tex=X" alt="X" eeimg="1" referrerpolicy="no-referrer">的不确定性就变成了<img src="https://www.zhihu.com/equation?tex=H%28X%7CY_1%2CY_2%29" alt="H(X|Y_1,Y_2)" eeimg="1" referrerpolicy="no-referrer">，<img src="https://www.zhihu.com/equation?tex=Y_2" alt="Y_2" eeimg="1" referrerpolicy="no-referrer">这个工作就带来了新的信息增益：<img src="https://www.zhihu.com/equation?tex=H%28X%7CY_1%29-H%28X%7CY_1%2CY_2%29" alt="H(X|Y_1)-H(X|Y_1,Y_2)" eeimg="1" referrerpolicy="no-referrer">。其后的工作 <img src="https://www.zhihu.com/equation?tex=Y_3%2CY_4%2C...%2CY_N%2C..." alt="Y_3,Y_4,...,Y_N,..." eeimg="1" referrerpolicy="no-referrer">带来的信息增益以此类推。</p><p data-pid="7Xsxmc-h">假设一个工作 <img src="https://www.zhihu.com/equation?tex=Y_N" alt="Y_N" eeimg="1" referrerpolicy="no-referrer">提供的是与之前所有工作正交的信息，那么它带来的信息增益就是其自身对X不确定性的降低，即，<img src="https://www.zhihu.com/equation?tex=H%28X%29-H%28X%7CY_N%29" alt="H(X)-H(X|Y_N)" eeimg="1" referrerpolicy="no-referrer">。</p><p data-pid="tca1uI9M">由此我们可以得到以下推论：</p><ul><li data-pid="VuFMdUb8">重复性工作的科研增益为0，因此没有任何科研价值，科研无第二！</li><li data-pid="XBVbZONB">所谓的水文，就是与已有工作重复性大，因此没带来多少科研增益！</li><li data-pid="3zGpK8Am">为了得到比较高的增益，科研工作要尽可能提供正交信息。也就是说，<b>我们要尽可能尝试从新的角度去解决老问题</b>！</li><li data-pid="L0OHXvoQ">对任何问题，我们总会尽力先发现那些带来增益最大的信息，所以一般来说，越往后的工作带来的科研增益越小！从这个角度而言，<b>我们应该尽可能选择新的科研课题，及早占坑</b>！</li></ul><p data-pid="HaefEs4j">各位看到这儿就知道为啥许多科研大牛都热衷于四处挖坑了吧，然而都是坑，有的能挖出矿甚至宝石出来，有的挖来挖去就是个坑儿 … </p><p data-pid="ewJaQlz0">科研汪们总是要进坑的，但<b>咱们进坑前要掌握好姿势</b>，要做到是主动思考后才跳进去而不是被人忽悠掉进去！</p><p data-pid="dsmU-rJg">那么什么是好坑呢，这就要说到科研问题本身的价值了。</p><hr><h2><b>科研问题的价值</b></h2><p data-pid="6BuKi9EM">判断科研问题价值的最大难点在于时间是我们的敌人，大多数科研汪们难免“事后诸葛亮，事前猪一样” …</p><p data-pid="9tyZvqcl">科研经常是通过试错来发展的，一个错误的前提下可能产生大量的伪学术问题，催生出几百上千篇文章，最后却是一地鸡毛。所以，在决定进哪个科研坑之前一定要思考问题本身的大前提，判断成功的可能性。如果有多个相互竞争的方向在解决同一个深层次问题，要比较之后做出自己的理性判断。虽然很多时候会判断错误，但至少不是盲目选择！人工智能领域有着横跨数十年波澜壮阔的符号主义和连结主义之争，可以想想自己如果在不同的年代会做出怎样的判断 ……</p><p data-pid="1R7jDpWQ">经过岁月的洗礼，优胜劣汰，有价值的工作才能存活下来。科研问题的终极价值会体现在能否写进教科书里。在专业教科书里提到的问题的基本上是该领域的重要问题，能够作为一个章节独立出现的都是基础性问题，能够写进大学教材里的那就是稀有物种了，相对论也不过是写进大学物理的选修教材而已 …</p><p data-pid="_Y3_bK3w">高价值的科研问题从提出到解决再到写进教科书可能要几年甚至几十年的时间。在此之前，一般会有段时间成为热点问题，在顶会牛刊上有很多讨论。举个栗子，深度学习鼻祖Hinton老爷子和他的学生Alex Krizhevsky在2012年ECCV会议上给计算机视觉领域带来了一场地震，他们提出的AlexNet模型在针对百万量级的ImageNet数据集的识别竞赛ILSVRC上将传统视觉识别方法打的满地找牙，从此，深度学习一跃成为人工智能领域的主流，各路大神都参与进来刷榜，短短三四年就把这个竞赛结果刷到超过人眼识别率，也不需要再举行了。</p><figure data-size="normal"><img src="https://pic3.zhimg.com/v2-df47a8b2309d55562cb062d13ac41ba3_1440w.jpg" data-rawwidth="1080" data-rawheight="580" data-size="normal" data-caption class="origin_image zh-lightbox-thumb" data-original="https://pic3.zhimg.com/v2-df47a8b2309d55562cb062d13ac41ba3_r.jpg" referrerpolicy="no-referrer"></figure><p data-pid="YUkKv-Hq">然而，一旦问题成为热点，低垂的果实很快就被摘完了，科研增益会比较低。</p><p data-pid="SQNnjmYC">所以，活在当下，我们真正关心的是，<b>对于一个目前还没多少人关心甚至没人明确提出的问题，它的潜在价值大不大</b>？是否有可能成为热点问题甚至将来写进教科书中。</p><p data-pid="-rgULZGD">还是先上个公式： <img src="https://www.zhihu.com/equation?tex=%5Csmall+%E4%BB%B7%E5%80%BC%3D%E9%9C%80%E6%B1%82%5Ctimes+%E7%A8%80%E7%BC%BA%E6%80%A7" alt="\small 价值=需求\times 稀缺性" eeimg="1" referrerpolicy="no-referrer"></p><p data-pid="lcTpRIFL">没有需求，就没有价值！商品如是，个人如是，公司如是，科研问题也如是！也就是说，如果说<b>一个科研问题有价值，那么一定是因为也有其他人想要知道它的答案</b>!</p><figure data-size="normal"><img src="https://pic4.zhimg.com/v2-089bb0170686ccadb45873cd02441fc2_1440w.jpg" data-rawwidth="1080" data-rawheight="544" data-size="normal" data-caption class="origin_image zh-lightbox-thumb" data-original="https://pic4.zhimg.com/v2-089bb0170686ccadb45873cd02441fc2_r.jpg" referrerpolicy="no-referrer"></figure><p data-pid="OhDFubJ9">那么，对于一个目前很少人关心的科研问题X，探索需求我们要回答的是So what? Who cares about X in the future? </p><p data-pid="xH4p9hdN">没有什么科研问题是独立存在的，我们要考虑的是问题X跟其他问题的关系。<b>如果问题X与某热门问题相关，并且热门问题在发展过程中会日益凸显X的重要性的话，那么对解决问题X的潜在需求就比较大</b>。</p><p data-pid="kUqu6iZN">时间来到2015年，在大家把神经网络越做越深、模型越做越大、越来越准确的过程中，你这个时候如果还想去刷ImageNet的性能，大概率会成为炮灰，那有没有其他相关需求呢？模型越来越复杂，对算力要求越来越高，一个日益重要的相关需求就是尽量减缓这个趋势，能够在对模型准确性影响不大的前提下降低模型复杂度。深度模型压缩就是这个时候提出来的，也迅速成为热点问题！同理，随着基于深度学习的人工智能算法在各领域攻城略地，应用到人们生活当中，另一个日益重要的相关需求就是要保证AI系统的安全性，也催生了很多有价值的科研问题。</p><p data-pid="A7onGAVn">成为热点的科研问题一定有学术价值，然而，好的科研问题不止要有学术价值，最终还是要对社会有意义！尤其是工科类科研，没有什么比发表了几十上百篇文章，却没有哪个用在真实产品中更让人恼火的事情了！事实上，即使是基础研究，也应该考虑应用场景，基础研究之所以叫基础研究，就是因为它的潜在应用范围非常广。从长远来看，所有的研究都是应用研究！</p><p data-pid="7NrtjFdE">需求的另一面是供给，越稀缺的东西越有价值。对于科研问题而言，能解决它的人越少，价值越大！也就是说，有价值的科研问题应该有挑战性，解决它能给人眼前一亮的感觉。因此对于一个科研问题，<b>要时刻问问自己“这个问题是不是换其他人也能很快解决？”</b>，如果答案是肯定的，那么这个问题的价值就不太大！要做的事情是继续深入思考，争取解决更深层次的本质问题。</p><hr><p data-pid="fdC6YMSn"> 小结一下，有价值的科研工作要同时满足以下几个因素：</p><ul><li data-pid="fD2-i2ER"><b>R</b>elevant problem：问题对他人有意义；</li><li data-pid="9tpeAmas"><b>O</b>riginal idea/methodology：观点或者方法要新颖；</li><li data-pid="qlkMWdZn"><b>S</b>ignificant results：结果有突破；</li></ul><p data-pid="HLtJx2rR"> 此外， 如果能有下面这个加分项就更好了，</p><ul><li data-pid="iyE3DAvW"><b>E</b>legant solution: 解决方案简洁优美；</li></ul><p data-pid="-UBwKU91">那么，非常有价值的科研工作就是在一个非常有意义的问题上提出很新颖的观点，创造出优美的解决方案，从而得到突破性结果的工作！</p><p data-pid="gl7m41-4">这样的工作就像<b>ROSE</b>，也需要足够多水文作为满天星来点缀。换句话说，我们固然要争取做高价值工作，灌水也是必不可少的科研活动。</p><p data-pid="-9I11BCs">更多内容请关注专栏：「科研汪的自我修养」！接下来，我们先明确什么工作才能称之为高水平的科研工作，再来谈谈科研灌水的正确姿势。</p><a data-draft-node="block" data-draft-type="link-card" href="https://zhuanlan.zhihu.com/p/449799631" data-size="normal" class="internal">你准备好做个合格的科研汪了吗？</a><p data-pid="caNSbKrw"><img src="https://www.zhihu.com/equation?tex=%5Cscriptsize+%E6%9C%AC%E6%96%87%E9%83%A8%E5%88%86%E5%9B%BE%E7%89%87%E5%8F%96%E8%87%AA%E7%BD%91%E7%BB%9C%EF%BC%8C%E7%89%88%E6%9D%83%E5%BD%92%E4%BD%9C%E8%80%85%E6%89%80%E6%9C%89%E3%80%82" alt="\scriptsize 本文部分图片取自网络，版权归作者所有。" eeimg="1" referrerpolicy="no-referrer"> </p>  
</div>
            