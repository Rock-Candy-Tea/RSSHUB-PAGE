
---
title: '游戏写实角色语音生成口型与表情技术的演进与未来'
categories: 
 - 游戏
 - GameRes 游资网
 - 热点推荐
headimg: 'https://di.gameres.com/attachment/forum/202207/25/092231a82eipzgz9h9fhe6.png'
author: GameRes 游资网
comments: false
date: Mon, 25 Jul 2022 00:00:00 GMT
thumbnail: 'https://di.gameres.com/attachment/forum/202207/25/092231a82eipzgz9h9fhe6.png'
---

<div>   
<i><font color="#808080">以下文章来源于腾讯游戏学堂 ，作者SeanQ</font></i><br>
<br>
近年来越来越多的超写实角色出现在游戏中，他们通常采用照扫(3D photogrammetry scanning)为基础对真人模特进行建模、加工，在满足游戏艺术效果的同时尽可能还原人物特别是其面部的细节特征，使得玩家能够获得更真实的角色扮演体验。<br>
<br>
但是，静态人脸照扫建模之后，怎么让这个角色面部动起来，并能自然地说话和表达情绪，是一个关键的难题，也随之衍生出了两种不同的主流解决方案：<br>
<br>
一种方案是，将3D照扫扩展为4D照扫，加入时序维度，从而能够采集到演员最精细的面部变化。然而这样的数据量十分巨大、后期处理时间及人力开销过重，并不适合大规模资产制作；<br>
<br>
另一种方案是，采用面部跟踪的技术，通过RGB、RGBD、双目、红外摄像头等设备捕捉人脸信息，提取面部特征并映射为绑定资产的参数，从而驱动角色面部运动。然而这种方案也有一些局限，例如MMO类型的游戏中通常有大量的角色对白，即使采用面部跟踪也会成本巨大。<br>
<br>
正因为两种方案的局限性，探索从语音对白作为输入直接生成角色口型表情序列，也就成为了一个重要的研究方向。<br>
<br>
为了方便阅读和比对，我这里就将语音生成口型与表情的技术演化划分为四个阶段。<br>
<br>
<div align="center"><font size="2"><font color="#808080">
<img aid="1047387" zoomfile="https://di.gameres.com/attachment/forum/202207/25/092231a82eipzgz9h9fhe6.png" data-original="https://di.gameres.com/attachment/forum/202207/25/092231a82eipzgz9h9fhe6.png" width="600" id="aimg_1047387" inpost="1" src="https://di.gameres.com/attachment/forum/202207/25/092231a82eipzgz9h9fhe6.png" referrerpolicy="no-referrer">
</font></font></div><div align="center"><font size="2"><font color="#808080">[ 照扫及表情跟踪 ]</font></font></div><br>
<strong><font color="#de5650">一、早期基于音素的方案</font></strong><br>
<br>
音素(Phoneme)，是人类语言中能够区别意义的最小声音单位。视素(Viseme)，则是视觉方式呈现出的音素，描绘出发音时的嘴部姿态。<br>
<br>
音素与视素之间，并不是一一对应的关系。例如人类对音素b、p、m的发音方式相似度很高，因而可以对应相同的视素。要想解决语音自动生成口型动画的问题，一个自然的想法是，从语音分析出对应的音素序列，并映射为视素序列。游戏角色则通过预制作好一些标准视素，根据前面得到的序列动态的激活并混合视素，生成角色口型动画。<br>
<br>
<div align="center"><font size="2"><font color="#808080">
<img aid="1047388" zoomfile="https://di.gameres.com/attachment/forum/202207/25/092231pdwlkpp323sls7p6.png" data-original="https://di.gameres.com/attachment/forum/202207/25/092231pdwlkpp323sls7p6.png" width="535" id="aimg_1047388" inpost="1" src="https://di.gameres.com/attachment/forum/202207/25/092231pdwlkpp323sls7p6.png" referrerpolicy="no-referrer">
</font></font></div><div align="center"><font size="2"><font color="#808080">[ 音频、音素对齐 ]</font></font></div><div align="center"><font size="2"><font color="#808080"><br>
</font></font></div><div align="center"><font size="2"><font color="#808080">
<img aid="1047389" zoomfile="https://di.gameres.com/attachment/forum/202207/25/092232gmkcdozmm7bl449c.png" data-original="https://di.gameres.com/attachment/forum/202207/25/092232gmkcdozmm7bl449c.png" width="600" id="aimg_1047389" inpost="1" src="https://di.gameres.com/attachment/forum/202207/25/092232gmkcdozmm7bl449c.png" referrerpolicy="no-referrer">
</font></font></div><div align="center"><font size="2"><font color="#808080">[ 视素的例子 ]</font></font></div><br>
要想从语音生成对应音素序列，不仅需要知道语音对应的音素，还需要在时间序列上和原始音频对齐。不过，这也正是ASR(自动语音识别Automatic Speech Recognition)领域的研究方向之一。因此，我们可以采用ASR领域的一些工具来实现这个目标，这通常需要结合特定语言的G2P(Grapheme-to-phoneme)以及Acoustic model共同完成这项任务。<br>
<br>
另外需要考虑的是，游戏应当制作哪些标准视素，以及如何制作这些视素？<br>
<br>
挑选视素并没有固定的标准，有些游戏仅仅制作5——10个较为重要、视觉效果明显的视素，如a、e、o、zh、w等，也有些工作会拆分的更细致制作超过20个以上的视素。这可以根据游戏需要的效果质量以及美术制作预算决定。<br>
<br>
Blendshape或者Pose的形式都可以作为这些视素的载体，这些美术资产可以手工制作，也可以使用类似Deformation Transfer或者Auto Rigging的算法进行迁移生成，最终根据视素序列对之进行K帧(Keyframing)生成动画。<br>
<br>
<strong><font color="#de5650">二、更自然的音素方案</font></strong><br>
<br>
<div align="center"><font size="2"><font color="#808080">
<img aid="1047390" zoomfile="https://di.gameres.com/attachment/forum/202207/25/092232ou7hbef7o4b7snkk.png" data-original="https://di.gameres.com/attachment/forum/202207/25/092232ou7hbef7o4b7snkk.png" width="600" id="aimg_1047390" inpost="1" src="https://di.gameres.com/attachment/forum/202207/25/092232ou7hbef7o4b7snkk.png" referrerpolicy="no-referrer">
</font></font></div><div align="center"><font size="2"><font color="#808080">[ OH的发音动作 ]</font></font></div><br>
<div align="center"><font size="2"><font color="#808080">
<img aid="1047391" zoomfile="https://di.gameres.com/attachment/forum/202207/25/092233kwz310uztjef7rwm.png" data-original="https://di.gameres.com/attachment/forum/202207/25/092233kwz310uztjef7rwm.png" width="600" id="aimg_1047391" inpost="1" src="https://di.gameres.com/attachment/forum/202207/25/092233kwz310uztjef7rwm.png" referrerpolicy="no-referrer">
</font></font></div><div align="center"><font size="2"><font color="#808080">[ JALI的LIP/JAW模型 ]</font></font></div><br>
前面我们也提到了，音素-视素并不是一一对应的关系。比如上图展示了“Oh”的五种不同发音口型，这就是一个音素对应五种视素的例子。<br>
<br>
而JALI(1)的工作，首先提出了解决上述问题的方案。他们通过观察发现，人们发音时的动作可以拆分为两个维度，一是下巴骨骼的运动，二是嘴部肌肉的运动。而不同的说话"风格"，可以通过调整这两个维度从而捕捉到更有表达力的口型。<br>
<br>
除此之外，JALI还总结了一些协同发音(Co-articulation)的规则，这样就可以通过上下文来调整视素的表达，从而使生成的视素序列更为自然。<br>
<br>
<div align="center"><font size="2"><font color="#808080">
<img aid="1047392" zoomfile="https://di.gameres.com/attachment/forum/202207/25/092233a4b2e5ik262ejjuu.png" data-original="https://di.gameres.com/attachment/forum/202207/25/092233a4b2e5ik262ejjuu.png" width="600" id="aimg_1047392" inpost="1" src="https://di.gameres.com/attachment/forum/202207/25/092233a4b2e5ik262ejjuu.png" referrerpolicy="no-referrer">
</font></font></div><div align="center"><font size="2"><font color="#808080">[ Deep Speech Animation ]</font></font></div><br>
<div align="center"><font size="2"><font color="#808080">
<img aid="1047393" zoomfile="https://di.gameres.com/attachment/forum/202207/25/092234n5fglf5z5wgvrbvr.png" data-original="https://di.gameres.com/attachment/forum/202207/25/092234n5fglf5z5wgvrbvr.png" width="600" id="aimg_1047393" inpost="1" src="https://di.gameres.com/attachment/forum/202207/25/092234n5fglf5z5wgvrbvr.png" referrerpolicy="no-referrer">
</font></font></div><div align="center"><font size="2"><font color="#808080">[ VisemeNet ]</font></font></div><br>
值得一提的是，JALI技术已经在大型RPG游戏《赛博朋克2077》中有了实际运用。游戏十余种本地化配音中的每一个字，都通过JALI技术实现了从语音到面部动画与口型的同步。<br>
<br>
<div align="center"><font size="2"><font color="#808080">
<img aid="1047394" zoomfile="https://di.gameres.com/attachment/forum/202207/25/092234gjzggvg444jeh7mr.png" data-original="https://di.gameres.com/attachment/forum/202207/25/092234gjzggvg444jeh7mr.png" width="600" id="aimg_1047394" inpost="1" src="https://di.gameres.com/attachment/forum/202207/25/092234gjzggvg444jeh7mr.png" referrerpolicy="no-referrer">
</font></font></div><div align="center"><font size="2"><font color="#808080">[ 赛博朋克2077 ]</font></font></div><br>
然而JALI的工作虽然提升了口型的表达效果，却也有一些局限：<br>
<br>
一方面，解决协同发音的现象主要依赖人为定义的若干规则，此种做法既不完备也不系统；<br>
<br>
另一方面，说话的"风格"需要较多的美术手工调整，这不利于大规模对白情形下需要的高度自动化工作流。<br>
<br>
一些基于音素的后续方案，侧重于解决上述问题。鉴于音素视素的非线性映射关系，很自然的想到可以用神经网络来表达，所以Disney(2)就设计了一种数据驱动的方案，此项工作使用Sliding Window CNN的方式接收一小段音素序列作为输入，输出AAM人脸模型参数曲线。尽管此种人脸模型略为过时，但将之替换为Blendshapes参数也并不困难，这样协同发音的隐式规则从数据中自然习得，就不再需要语言专家人为指定。<br>
<br>
VisemeNet(3)的改进则更多一些，主要由音素组、人脸关键点、视素三个阶段的LSTM组成。前两个阶段的输入是语音提取的MFCC特征，利用网络得到音素、人脸关键点序列，并将网络的特征作为输入给到下一视素阶段，分别预测视素激活、协同发音参数、风格参数序列。<br>
<br>
这一工作将之前我们对音素口型方面的经验完整的继承了下来，并将其中的几乎所有模块改造成了数据驱动的网络表示，取得较好效果的同时提升了自动化程度，减少了手工介入的必要性。<br>
<br>
该部分涉及的相关研究：<br>
<br>
<i><font size="2"><font color="#808080">(1) JALI: An Animator-Centric Viseme Model for Expressive Lip Synchronization</font></font></i><br>
<i><font size="2"><font color="#808080">JALI：一种以动画师为核心的视素模型驱动的表达力丰富的口型同步方法</font></font></i><br>
<i><font size="2"><font color="#808080">链接：https://www.dgp.toronto.edu/——elf/JALISIG16.pdf</font></font></i><br>
<i><font size="2"><font color="#808080">(2) A Deep Learning Approach for Generalized Speech Animation</font></font></i><br>
<i><font size="2"><font color="#808080">一种数据驱动的，通用的语音动画的深度学习方法</font></font></i><br>
<i><font size="2"><font color="#808080">链接：https://home.ttic.edu/——taehwan/taylor_etal_siggraph2017.pdf</font></font></i><br>
<i><font size="2"><font color="#808080">(3) VisemeNet: Audio-Driven Animator-Centric Speech Animation</font></font></i><br>
<i><font size="2"><font color="#808080">视素网络：音频驱动的动画师为中心的口型动画</font></font></i><br>
<i><font size="2"><font color="#808080">链接：https://arxiv.org/abs/1805.09488</font></font></i><br>
<br>
<strong><font color="#de5650">三、端到端的学习方案</font></strong><br>
<br>
<div align="center"><font size="2"><font color="#808080">
<img aid="1047395" zoomfile="https://di.gameres.com/attachment/forum/202207/25/092235hwupgvutiv24avs4.png" data-original="https://di.gameres.com/attachment/forum/202207/25/092235hwupgvutiv24avs4.png" width="600" id="aimg_1047395" inpost="1" src="https://di.gameres.com/attachment/forum/202207/25/092235hwupgvutiv24avs4.png" referrerpolicy="no-referrer">
</font></font></div><div align="center"><font size="2"><font color="#808080">[ 英伟达端到端方案 ]</font></font></div><br>
尽管VisemeNet很好地将传统对于音素口型的经验集成到了网络之中，但是其三阶段的网络架构有些过于复杂，训练数据的生成也需要美术的手工标注，这都加大了此方案应用落地的难度。<br>
<br>
深度神经网络表达出了强大的非线性拟合能力，因而值得我们探索是否可能建立语音到口型的端到端模型。英伟达在(4)中实现了这样的方案，这项工作采用原始音频数据作为输入，使用LPC(Linear Predictive Coding)提取的Autocorrelation系数作为特征，通过神经网络直接输出预制模板的面部顶点。<br>
<br>
这项工作得以成功输出高质量的结果、并能将之应用于游戏角色之上，有如下几个关键点：<br>
<br>
第一是高质量的训练数据，英伟达使用了Di4D Pro设备采集了高质量的演员说话时的人脸4D数据；<br>
<br>
第二是较为稠密的模板面部，这为表达出面部的丰富细节提供了基础，文章中网络在最后的全连接层使用PCA参数作为初始化，通过训练对之调整直接输出全脸稠密顶点；<br>
<br>
第三是迁移算法，这项工作使用Deformation Transfer算法将模板面部的结果迁移到其他角色，这使得高质量的口型动画重定向到游戏中的其他角色成为可能。<br>
<br>
当然英伟达的这项工作也有些不足之处，例如他的口型输出和训练数据中演员的表演特点是高度相关的，如果想切换到另一种风格，则需要重新采集数据和训练，而这又是十分费力耗时的。VOCA(5)的工作则利用了FLAME模型以及其采集的高清4D数据作为基础，加入了One-hot identity encoding作为不同个体说话风格的编码，从而可以在推理时调整此项得到不同效果的口型动画序列，当然实际测试而言VOCA并没有取得明显更优质的效果，这可能是由于数据质量等原因，不过其描述的方法论或许有一定的借鉴意义。<br>
<br>
该部分涉及的相关研究：<br>
<br>
<font size="2"><font color="#808080">（4）Audio-Driven Facial Animation by Joint End-to-End Learning of Pose and Emotion</font></font><br>
<font size="2"><font color="#808080">一种音频驱动的，端到端姿势与情绪协同学习的面部动画解决方案</font></font><br>
<font size="2"><font color="#808080">链接：https://research.nvidia.com/publication/2017-07_audio-driven-facial-animation-joint-end-end-learning-pose-and-emotion</font></font><br>
<font size="2"><font color="#808080">（5）Capture, Learning, and Synthesis of 3D Speaking Styles</font></font><br>
<font size="2"><font color="#808080">3D 口型风格的捕捉、学习与合成</font></font><br>
<font size="2"><font color="#808080">链接：https://openaccess.thecvf.com/content_CVPR_2019/papers/Cudeiro_Capture_Learning_and_Synthesis_of_3D_Speaking_Styles_CVPR_2019_paper.pdf</font></font><br>
<br>
<strong><font color="#de5650">四、口型与表情的结合</font></strong><br>
<br>
<div align="center"><font size="2"><font color="#808080">
<img aid="1047396" zoomfile="https://di.gameres.com/attachment/forum/202207/25/092235w4npcy45fn884fff.png" data-original="https://di.gameres.com/attachment/forum/202207/25/092235w4npcy45fn884fff.png" width="600" id="aimg_1047396" inpost="1" src="https://di.gameres.com/attachment/forum/202207/25/092235w4npcy45fn884fff.png" referrerpolicy="no-referrer">
</font></font></div><div align="center"><font size="2"><font color="#808080">[ 不同情绪下的口型 ]</font></font></div><br>
如果角色仅仅在说话的时候有同步的口型动画，却面无表情眼都不眨一下，那么也会显得十分呆板和不自然。除此之外人们在喜怒哀乐不同情绪下表达说出同样的对白，面部的动作也是有所区别的。<br>
<br>
之前提到的英伟达的工作通过加入情绪状态(emotional state)的隐变量(latent code)，在学习的过程中自动将各帧蕴含的情绪编码在其中间，从而对情绪进行建模。推理的时候便可以找到参考帧(reference frame)的隐变量来修改为相应的情绪，从而产生不同的表情。然而此种方法产生的隐变量没有清晰的语义，实际应用的效果也差强人意。<br>
<br>
为了得到清晰的情绪语义，我们可以对语音进行情绪分类，从而得到其对应的高兴、悲伤、惊讶、愤怒等情绪类别。美术可以预制这些情绪对应的表情资产，算法在生成口型同时混合这些面部表情从而得到更为自然、真实的语音口型效果。当然我们也可以使用其他方案来更好的编码表情。<br>
<br>
<div align="center"><font size="2"><font color="#808080">
<img aid="1047397" zoomfile="https://di.gameres.com/attachment/forum/202207/25/092235w1vhkdtddgzu8v0i.png" data-original="https://di.gameres.com/attachment/forum/202207/25/092235w1vhkdtddgzu8v0i.png" width="600" id="aimg_1047397" inpost="1" src="https://di.gameres.com/attachment/forum/202207/25/092235w1vhkdtddgzu8v0i.png" referrerpolicy="no-referrer">
</font></font></div><div align="center"><font size="2"><font color="#808080">[ 情绪编辑 ]</font></font></div><br>
例如(6)将人脸从难过到高兴分为不同的情绪层次，为其赋予不同的插值权重，除了可以表达语音情绪之外，还可以根据情绪的强烈程度来微调表情。(7)当中提出的集成情绪的重建方案，也可以提供更多优质的数据。<br>
该部分涉及的相关研究：<br>
<br>
<i><font color="#808080">（6）Audio-Driven Emotional Video Portraits</font></i><br>
<i><font color="#808080">音频驱动的情绪视频肖像生成</font></i><br>
<i><font color="#808080">链接：https://jixinya.github.io/projects/evp/resources/evp.pdf</font></i><br>
<i><font color="#808080">（7）Emotion Driven Monocular Face Capture and Animation</font></i><br>
<i><font color="#808080">情绪驱动的单目人脸捕捉及动画研究</font></i><br>
<i><font color="#808080">链接：https://ps.is.mpg.de/uploads_file/attachment/attachment/686/EMOCA__CVPR22.pdf</font></i><br>
<br>
<strong><font color="#de5650">总结</font></strong><br>
<br>
总得来说，语音生成口型与表情的技术，在游戏角色动画、高清数字人等领域有着非常广泛的应用，学术界及工业界也在这一领域进行了长期大量的探索。<br>
<br>
我相信，随着我们对语音、口型、情绪、表情的了解愈发深入，也会得到更加自然，甚至足以乱真的结果，同时拥有更多维度、更强的控制能力，并不断扩展他们的应用领域，丰富角色在虚拟世界的表现力。<br>
<br>
这里是文中提到所有研究的来源链接，有兴趣深入了解的朋友可以看看：<br>
<br>
<font size="2"><font color="#808080">(1) JALI: An Animator-Centric Viseme Model for Expressive Lip Synchronization</font></font><br>
<font size="2"><font color="#808080">JALI：一种以动画师为核心的视素模型驱动的表达力丰富的口型同步方法</font></font><br>
<font size="2"><font color="#808080">链接：https://www.dgp.toronto.edu/——elf/JALISIG16.pdf</font></font><br>
<font size="2"><font color="#808080">(2) A Deep Learning Approach for Generalized Speech Animation</font></font><br>
<font size="2"><font color="#808080">一种数据驱动的，通用的语音动画的深度学习方法</font></font><br>
<font size="2"><font color="#808080">链接：https://home.ttic.edu/——taehwan/taylor_etal_siggraph2017.pdf</font></font><br>
<font size="2"><font color="#808080">(3) VisemeNet: Audio-Driven Animator-Centric Speech Animation</font></font><br>
<font size="2"><font color="#808080">视素网络：音频驱动的动画师为中心的口型动画</font></font><br>
<font size="2"><font color="#808080">链接：https://arxiv.org/abs/1805.09488</font></font><br>
<font size="2"><font color="#808080">（4）Audio-Driven Facial Animation by Joint End-to-End Learning of Pose and Emotion</font></font><br>
<font size="2"><font color="#808080">一种音频驱动的，端到端姿势与情绪协同学习的面部动画解决方案</font></font><br>
<font size="2"><font color="#808080">链接：https://research.nvidia.com/publication/2017-07_audio-driven-facial-animation-joint-end-end-learning-pose-and-emotion</font></font><br>
<font size="2"><font color="#808080">（5）Capture, Learning, and Synthesis of 3D Speaking Styles</font></font><br>
<font size="2"><font color="#808080">3D 口型风格的捕捉、学习与合成</font></font><br>
<font size="2"><font color="#808080">链接：https://openaccess.thecvf.com/content_CVPR_2019/papers/Cudeiro_Capture_Learning_and_Synthesis_of_3D_Speaking_Styles_CVPR_2019_paper.pdf</font></font><br>
<font size="2"><font color="#808080">（6）Audio-Driven Emotional Video Portraits</font></font><br>
<font size="2"><font color="#808080">音频驱动的情绪视频肖像生成</font></font><br>
<font size="2"><font color="#808080">链接：https://jixinya.github.io/projects/evp/resources/evp.pdf</font></font><br>
<font size="2"><font color="#808080">（7）Emotion Driven Monocular Face Capture and Animation</font></font><br>
<font size="2"><font color="#808080">情绪驱动的单目人脸捕捉及动画研究</font></font><br>
<font size="2"><font color="#808080">链接：https://ps.is.mpg.de/uploads_fi</font></font><br>
<br>
<font size="2"><font color="#808080"></font></font><br>
<font size="2"><font color="#808080">来源：腾讯游戏学堂</font></font><br>
<br>
  
</div>
            