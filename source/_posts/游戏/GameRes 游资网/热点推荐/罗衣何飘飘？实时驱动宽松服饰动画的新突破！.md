
---
title: '罗衣何飘飘？实时驱动宽松服饰动画的新突破！'
categories: 
 - 游戏
 - GameRes 游资网
 - 热点推荐
headimg: 'https://di.gameres.com/attachment/forum/202208/11/104150rogr1lxkks1lzlkx.png'
author: GameRes 游资网
comments: false
date: Thu, 11 Aug 2022 00:00:00 GMT
thumbnail: 'https://di.gameres.com/attachment/forum/202208/11/104150rogr1lxkks1lzlkx.png'
---

<div>   
魏晋有曹植《美女篇》“罗衣何飘飘，轻裾随风还”；唐有白居易《长恨歌》“风吹仙袂飘摇举，犹似霓裳羽衣舞”、李白《古风》“霓裳曳广带，飘拂升天行”... 当形容佳人翩翩风貌，古诗中不乏描写衣物随风飘动的佳句，丝罗衣襟风里飘舞，轻薄裙纱随风流转，佳人回首顾盼，留下迷人的光彩。<br>
<br>
服装同样也是数字角色外形的重要组成部分，宽松服装的动态形变，例如舞蹈角色裙子的飞舞、旋转和掉落等，能帮助角色表达情感、展现个性。<br>
<br>
我们提出一种基于深度学习的方法，利用虚拟骨骼驱动的动作网络，实时预测宽松服装的复杂形变。该方法可以在保证真实性的同时，在常见配置的电脑上实时运行。除了大幅度动作鲁棒（鲁棒：Robust的音译，指系统在扰动或不确定的情况下仍能保持它们的特征行为）之外，还支持实时调整模拟的物理参数，例如布料的弯曲强度，从而方便动画师或是使用者对服装动画的效果进行调整。有助于提升数字人和游戏角色的实时动画品质和艺术表现力。<br>
<br>
这是NExT第三年登上SIGGRAPH的舞台——继2019年Matt AI亮相SIGGRAPH Asia Real-Time Live，2021年EyelashNet登上SIGGRAPH Asia Technical Papers后。NExT Studios与腾讯游戏学堂牵头成立的「浙江大学-腾讯游戏智能图形创新技术联合实验室」金小刚教授团队关于“布料仿真”的合研论文“Predicting Loose-Fitting Garment Deformations Using Bone-Driven Motion Networks” ?被SIGGRAPH 2022 Technical Paper收录，并于北京时间8月9日7点在大会上进行了分享。<br>
<br>
<div align="center"><font size="2">
<img aid="1049644" zoomfile="https://di.gameres.com/attachment/forum/202208/11/104150rogr1lxkks1lzlkx.png" data-original="https://di.gameres.com/attachment/forum/202208/11/104150rogr1lxkks1lzlkx.png" width="600" id="aimg_1049644" inpost="1" src="https://di.gameres.com/attachment/forum/202208/11/104150rogr1lxkks1lzlkx.png" referrerpolicy="no-referrer">
</font></div><div align="center"><font size="2">SIGGRAPH 线上会议分享</font></div><br>
<div align="center">
<img aid="1049653" zoomfile="https://di.gameres.com/attachment/forum/202208/11/104853udt44i2tthqi4un4.png" data-original="https://di.gameres.com/attachment/forum/202208/11/104853udt44i2tthqi4un4.png" width="600" id="aimg_1049653" inpost="1" src="https://di.gameres.com/attachment/forum/202208/11/104853udt44i2tthqi4un4.png" referrerpolicy="no-referrer">
</div><br>
服装动画是计算机图形学中的重要课题，紧身的衣物通常和身体表层肌肉贴合密切，可基于身体骨骼的运动来近似驱动，但宽松服装通常与身体有一定距离，且在外力和身体各种动作等共同作用下会具有复杂形变和碰撞，因而一直没有很好的实时呈现方法。<br>
<br>
为了又好又快地得到服装动画，人们探索了一系列的方法。这些方法可以分为两类，一类是基于物理模拟的方法，另一类是基于数据驱动的方法。<br>
<br>
物理模拟是通过在计算机中为布料赋予物理参数，并用真实的物理定律模拟得到结果。物理模拟的方法可以生成真实高质量的服装动画，它们这类方法的缺点在于计算时间过长、计算负担过大，难以实时运算。人们探索了一系列加速物理模拟的方法，例如自适应的网格，使用GPU并行加速等，始终无法满足高质量实时应用场景和实时交互的需求。<br>
<br>
基于数据驱动的方法通过深度学习，从物理模拟生成或是从真实世界中扫描数据中学习服装运动的趋势。这类方法的特点在于性能高，可实时。但现有的深度学习方法大多是针对紧身衣服的，应用范围有限。<br>
<br>
<div align="center">
<img aid="1049654" zoomfile="https://di.gameres.com/attachment/forum/202208/11/111043fxa4011v5uk45z4x.png" data-original="https://di.gameres.com/attachment/forum/202208/11/111043fxa4011v5uk45z4x.png" width="600" id="aimg_1049654" inpost="1" src="https://di.gameres.com/attachment/forum/202208/11/111043fxa4011v5uk45z4x.png" referrerpolicy="no-referrer">
</div><br>
出于以上目的，我们需要探索一个基于深度学习的快速的宽松服装预测方法。于是提出了“五大性能目标”作为开发方向的指导：<br>
<br>
集成性：能够与动画引擎环境紧密结合；<br>
<br>
快速性：能够在常见的计算机上达到实时；<br>
<br>
准确性：提供与真实物理模拟相近的结果；<br>
<br>
灵活性：能够对不同类型的服装提供支持；<br>
<br>
鲁棒性：能够鲁棒地处理大幅度的姿势。<br>
<br>
<div align="center">
<img aid="1049655" zoomfile="https://di.gameres.com/attachment/forum/202208/11/111043chhvztuususew225.png" data-original="https://di.gameres.com/attachment/forum/202208/11/111043chhvztuususew225.png" width="600" id="aimg_1049655" inpost="1" src="https://di.gameres.com/attachment/forum/202208/11/111043chhvztuususew225.png" referrerpolicy="no-referrer">
</div><br>
<div align="center">
<img aid="1049656" zoomfile="https://di.gameres.com/attachment/forum/202208/11/111044de6yb8m6mili46gl.png" data-original="https://di.gameres.com/attachment/forum/202208/11/111044de6yb8m6mili46gl.png" width="600" id="aimg_1049656" inpost="1" src="https://di.gameres.com/attachment/forum/202208/11/111044de6yb8m6mili46gl.png" referrerpolicy="no-referrer">
</div><br>
给定身体动作（各个关节的旋转欧拉角和身体的平移）以及模拟参数，可以预测服装顶点位置的序列。假定在预测过程中服装的网格具有恒定的拓扑（拓扑：空间内的连续变化下维持不变的性质），先生成虚拟骨骼，然后在动作网络中使用，最后处理模拟参数。下图为方法系统框架的示意图：<br>
<br>
<div align="center">
<img aid="1049635" zoomfile="https://di.gameres.com/attachment/forum/202208/11/104147ndq9oxwlzlae59xa.png" data-original="https://di.gameres.com/attachment/forum/202208/11/104147ndq9oxwlzlae59xa.png" width="600" id="aimg_1049635" inpost="1" src="https://di.gameres.com/attachment/forum/202208/11/104147ndq9oxwlzlae59xa.png" referrerpolicy="no-referrer">
</div><br>
<div align="center">
<img aid="1049657" zoomfile="https://di.gameres.com/attachment/forum/202208/11/111044i2s9zeg1yy1gn1li.png" data-original="https://di.gameres.com/attachment/forum/202208/11/111044i2s9zeg1yy1gn1li.png" width="600" id="aimg_1049657" inpost="1" src="https://di.gameres.com/attachment/forum/202208/11/111044i2s9zeg1yy1gn1li.png" referrerpolicy="no-referrer">
</div><br>
虚拟骨骼：虚拟骨骼对应一系列使用线性混合蒙皮 (LBS) 方法驱动服装形变的骨骼。它是对物理模拟生成的服装动画进行蒙皮分解得到的结果。它和动画师创建的骨架中的骨骼有一些区别。首先，每一个骨骼都是独立的，没有父骨骼或是子骨骼，因此每一个虚拟骨骼都具有自己的旋转和平移。其次，虚拟骨骼并没有任何语义信息，它仅仅控制一系列在运动时具有相同趋势的顶点。<br>
<br>
接下来需要数据准备和从模拟结果的网格序列中提取虚拟骨骼。我们使用Houdini的Vellum Solver生成训练所需的数据，采样了10组模拟参数，每一组参数都模拟45000帧。相比于从真实人体中捕捉的数据，这次使用的卡通类舞蹈动作数据是由人工进行调整过的，具有更强的韵律性和节奏感，更能够展现宽松服装的美感。<br>
<br>
获得了模拟的服装网格之后，使用拉普拉斯平滑（一种平滑算法，能够在保留几何网格细节的同时，较好地保留网格顶点之间的拓扑关系）提取出网格的低频部分，之后将剩余的部分作为网格的高频部分。在获得低频部分的网格序列之后，使用蒙皮分解方法从中提取出虚拟骨骼。使用的蒙皮分解方法是2014年Le等人2012年在SIGGRAPH上提出的SSDR?，是目前该领域的最优方法。使用SSDR能够将动画序列分解成一个LBS模型，以及每一帧的时候其中的骨骼运动情况。<br>
<br>
<div align="center">
<img aid="1049658" zoomfile="https://di.gameres.com/attachment/forum/202208/11/111044re0s7sto82t8bq7t.png" data-original="https://di.gameres.com/attachment/forum/202208/11/111044re0s7sto82t8bq7t.png" width="600" id="aimg_1049658" inpost="1" src="https://di.gameres.com/attachment/forum/202208/11/111044re0s7sto82t8bq7t.png" referrerpolicy="no-referrer">
</div><br>
动作网络：动作网络是一个由虚拟骨骼驱动的神经网络，输入身体动作，输出对应模拟参数的三维模型顶点位置。该网络由低频和高频两个部分组成，分别用来预测不同幅度的形变。低频部分将身体的动作转化为虚拟骨骼的旋转和平移，使用蒙皮方法生成对应的低频网格。高频部分使用低频网格和虚拟骨骼信息，生成高频形变。<br>
<br>
低频部分，使用了一种循环神经网络(GRU)，将输入的动作转化为虚拟骨骼的旋转和平移。循环神经网络可以保存每一帧的状态(hidden state)，并在获取了每一帧新的动作输入之后更新该状态。从这个角度来说，它可以保证学习到服装的动态形变，这一部分形变对于宽松衣服的展示是非常重要的。输入是身体的动作，包含了每一根骨骼的旋转和身体整体的平移。GRU网络的输出获得了虚拟骨骼的动作之后，可以用上一步获得的LBS模型得到低频部分的形变。该部分网络的训练使用的损失函数是LBS生成的低频部分和物理模拟结果的差值以及表面平滑程度（拉普拉斯项）的差。<br>
<br>
<div align="center">
<img aid="1049643" zoomfile="https://di.gameres.com/attachment/forum/202208/11/104149lkpmndiqp1otmpuk.jpg" data-original="https://di.gameres.com/attachment/forum/202208/11/104149lkpmndiqp1otmpuk.jpg" width="600" id="aimg_1049643" inpost="1" src="https://di.gameres.com/attachment/forum/202208/11/104149lkpmndiqp1otmpuk.jpg" referrerpolicy="no-referrer">
</div><br>
高频部分，使用了类似的循环神经网络来处理虚拟骨骼的旋转和平移，同时使用一个图神经网络来处理上一部分生成的低频形变的网格，将以上两个网络的结果拼接，并通过一个多层感知机（MLP）来获得最终的结果。对于虚拟骨骼，使用GRU处理，得到对应的全局信息；使用图神经网络处理输入的低频部分网格，得到对应的局部信息；使用一个MLP来处理拼接的信息，得到最终的高频部分。训练的时候使用的损失函数就是预测的顶点和模拟结果顶点的位置差，以及与身体的碰撞项。<br>
<br>
<div align="center">
<img aid="1049636" zoomfile="https://di.gameres.com/attachment/forum/202208/11/104147w2pmkmmu2fnhpa0p.png" data-original="https://di.gameres.com/attachment/forum/202208/11/104147w2pmkmmu2fnhpa0p.png" width="600" id="aimg_1049636" inpost="1" src="https://di.gameres.com/attachment/forum/202208/11/104147w2pmkmmu2fnhpa0p.png" referrerpolicy="no-referrer">
</div><br>
<div align="center">
<img aid="1049659" zoomfile="https://di.gameres.com/attachment/forum/202208/11/111044tqeccrigh2iqgiiq.png" data-original="https://di.gameres.com/attachment/forum/202208/11/111044tqeccrigh2iqgiiq.png" width="600" id="aimg_1049659" inpost="1" src="https://di.gameres.com/attachment/forum/202208/11/111044tqeccrigh2iqgiiq.png" referrerpolicy="no-referrer">
</div><br>
处理参数：使用将对应不同模拟参数的动作网络的结果进行加权求和的方式来处理不同的模拟参数，对于加权权重，使用一个径向基函数(RBF)核来求得。其中，g(θ)是一个MLP网络，用来将输入的参数投射到隐空间中，最终结果是不同动作网络的加权平均。<br>
<br>
<div align="center">
<img aid="1049637" zoomfile="https://di.gameres.com/attachment/forum/202208/11/104148w54htir3xserhrtu.png" data-original="https://di.gameres.com/attachment/forum/202208/11/104148w54htir3xserhrtu.png" width="448" id="aimg_1049637" inpost="1" src="https://di.gameres.com/attachment/forum/202208/11/104148w54htir3xserhrtu.png" referrerpolicy="no-referrer">
</div><br>
<div align="center">
<img aid="1049660" zoomfile="https://di.gameres.com/attachment/forum/202208/11/111045pbq1ifm4o1v1idnb.png" data-original="https://di.gameres.com/attachment/forum/202208/11/111045pbq1ifm4o1v1idnb.png" width="600" id="aimg_1049660" inpost="1" src="https://di.gameres.com/attachment/forum/202208/11/111045pbq1ifm4o1v1idnb.png" referrerpolicy="no-referrer">
</div><br>
我们的方法能够实时生成具有复杂形变的宽松服装。下图所示为我们的结果和物理模拟结果的对比，上行是物理模拟结果，中间是我们的结果，下行是我们的结果和物理模拟结果的逐顶点差值。可以看出我们的方法较为接近物理模拟结果，且可以表现出较为复杂的服装形变效果。<br>
<br>
<div align="center">
<img aid="1049638" zoomfile="https://di.gameres.com/attachment/forum/202208/11/104148q71vbb3ld7e77wrs.png" data-original="https://di.gameres.com/attachment/forum/202208/11/104148q71vbb3ld7e77wrs.png" width="600" id="aimg_1049638" inpost="1" src="https://di.gameres.com/attachment/forum/202208/11/104148q71vbb3ld7e77wrs.png" referrerpolicy="no-referrer">
</div><br>
我们的方法在同一动作下可以生成不同模拟参数，如下图从左到右弯曲强度下降，从上到下模拟时间尺度上升，参数不同会对应不同结果，裙子的形态发生不同改变。<br>
<br>
<div align="center">
<img aid="1049639" zoomfile="https://di.gameres.com/attachment/forum/202208/11/104148u3pfjtrss5tj0ps3.png" data-original="https://di.gameres.com/attachment/forum/202208/11/104148u3pfjtrss5tj0ps3.png" width="600" id="aimg_1049639" inpost="1" src="https://di.gameres.com/attachment/forum/202208/11/104148u3pfjtrss5tj0ps3.png" referrerpolicy="no-referrer">
</div><br>
我们的方法与其它方法生成结果的对比如下图，可以看到在极端的姿势下，我们的方法能够生成最近似于物理模拟的结果。<br>
<br>
<div align="center">
<img aid="1049640" zoomfile="https://di.gameres.com/attachment/forum/202208/11/104148a9hew05914pf0neb.png" data-original="https://di.gameres.com/attachment/forum/202208/11/104148a9hew05914pf0neb.png" width="600" id="aimg_1049640" inpost="1" src="https://di.gameres.com/attachment/forum/202208/11/104148a9hew05914pf0neb.png" referrerpolicy="no-referrer">
</div><br>
我们的方法与其它方法在动作网络高频部分的对比如下图，右下方是我们的结果和物理模拟结果的逐顶点差值度量，显示我们的结果与模拟结果是最为接近，且误差最小。<br>
<br>
<div align="center">
<img aid="1049641" zoomfile="https://di.gameres.com/attachment/forum/202208/11/104149mkvu33fu4jl6jhmk.png" data-original="https://di.gameres.com/attachment/forum/202208/11/104149mkvu33fu4jl6jhmk.png" width="600" id="aimg_1049641" inpost="1" src="https://di.gameres.com/attachment/forum/202208/11/104149mkvu33fu4jl6jhmk.png" referrerpolicy="no-referrer">
</div><br>
我们在目前使用的数据集与各种方法进行了对比，我们的方法在各项指标上都显著优于其它方法。对比得到的各项量化指标如下，我们的方法都具有较优的表现：<br>
<br>
<div align="center">
<img aid="1049642" zoomfile="https://di.gameres.com/attachment/forum/202208/11/104149eo5cqn1ecd1nfknu.png" data-original="https://di.gameres.com/attachment/forum/202208/11/104149eo5cqn1ecd1nfknu.png" width="600" id="aimg_1049642" inpost="1" src="https://di.gameres.com/attachment/forum/202208/11/104149eo5cqn1ecd1nfknu.png" referrerpolicy="no-referrer">
</div><br>
<font size="2"></font><br>
<font size="2">来源：腾讯游戏学堂</font><br>
<font size="2">原文：https://mp.weixin.qq.com/s/2PyvIIjeLX03qiK2PSxXAA</font><br>
<br>
<br>
  
</div>
            