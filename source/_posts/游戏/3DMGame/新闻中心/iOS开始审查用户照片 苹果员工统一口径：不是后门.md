
---
title: 'iOS开始审查用户照片 苹果员工统一口径：不是后门'
categories: 
 - 游戏
 - 3DMGame
 - 新闻中心
headimg: 'https://img.3dmgame.com/uploads/images/news/20210815/1629011974_423771.jpg'
author: 3DMGame
comments: false
date: Sun, 15 Aug 2021 07:19:00 GMT
thumbnail: 'https://img.3dmgame.com/uploads/images/news/20210815/1629011974_423771.jpg'
---

<div>   
<p style="text-indent:2em;">
<span>苹果一直坚称旗下手机和iOS系统在用户隐私方面坚不可摧，还跟美国政府打过官司。但苹果如今修改一项政策，将会通过iOS审查用户照片，其员工的口径也会进行相应统一。</span>
</p>
<p align="center">
<img src="https://img.3dmgame.com/uploads/images/news/20210815/1629011974_423771.jpg" alt="iOS开始审查用户照片 苹果员工统一口径：不是后门" referrerpolicy="no-referrer"> 
</p>
<p style="text-indent:2em;">
本月初，苹果宣布了一项新政策，将推出名为“neuralMatch”的新技术，它会在图片上传到iCloud之前进行扫描。如果它找到了匹配的CSAM（儿童性虐待照片），将会通知审核人员检查。若确认存在儿童色情内容，那么苹果方面将关闭账户，并通知美国国家失踪和被剥削儿童中心。
</p>
<p style="text-indent:2em;">
苹果这一举动受到了儿童保护组织的认可，但有更多的人认为苹果审查用户照片是个不好的苗头，觉得被植入了后门，担心该系统遭到滥用。
</p>
<p style="text-indent:2em;">
苹果此前解释过该系统的运作方式，称这一系统并不能真正“看到”用户的照片，而是采用“数字指纹”的技术，通过辨识图片中的关键信息，与现有的儿童性侵图像数据库内容对比。
</p>
<p style="text-indent:2em;">
虽然内部员工都有强烈的反对倾向，但苹果还是会执行这一政策，本周还给员工下发了最新的备忘录，要求员工在CSAM后门问题上统一口径，强调这一政策只针对iCloud中的CSAM内容，不会容忍被滥用。
</p>          
</div>
            