
---
title: '计算规模驱动下的网络方案演进'
categories: 
 - 编程
 - Dockone
 - 周报
headimg: 'https://cors.zfour.workers.dev/?http://dockone.io/uploads/article/20211213/19bf853822fa9d46d8b8ea49dcf56898.jpg'
author: Dockone
comments: false
date: 2021-12-14 07:08:25
thumbnail: 'https://cors.zfour.workers.dev/?http://dockone.io/uploads/article/20211213/19bf853822fa9d46d8b8ea49dcf56898.jpg'
---

<div>   
<br><h3>引言</h3>过去二十多年网络发生了很多的变化，比如：<br>
<ol><li>数据中心物理拓扑方面，由接入-汇聚-核心三级网络架构演进到 Spine-Leaf 两级架构；</li><li>软件方面，出现了各种各样的网络虚拟化，例如软件实现的虚拟网桥、虚拟机交换机、虚拟路由器、软件 overlay 网络等等；</li><li>另外还出现了很多新的概念和模型，例如软件定义网络（SDN）的转发与控制分离思想，实现上可以像 Google 一样基于 OpenFlow，可以是后来用的比较多的 BGP EVPN；另外还有 Spine-Leaf 中的分布式网关概念；还有大二层网络与纯三层路由网络等等概念。</li></ol><br>
<br><div class="aw-upload-img-list active">
<a href="http://dockone.io/uploads/article/20211213/19bf853822fa9d46d8b8ea49dcf56898.jpg" target="_blank" data-fancybox-group="thumb" rel="lightbox"><img src="https://cors.zfour.workers.dev/?http://dockone.io/uploads/article/20211213/19bf853822fa9d46d8b8ea49dcf56898.jpg" class="img-polaroid" title="1.jpg" alt="1.jpg" referrerpolicy="no-referrer"></a>
</div>
<br>
<em>Various network solutions over the past years</em><br>
<br>具体到一些我们所熟悉的计算平台：<br>
<ol><li>早起的 bare metal 集群可能会采用 DHCP 加扁平二层（flat L2）网络</li><li>虚拟机时代 —— 比较有代表性的 OpenStack 平台，采用的是 Neutron+OVS 大二层网络</li><li>Docker 内置的典型网络模型是宿主机内的 bridge 加出宿主机的 SNAT，并定义了 CNM 网络模型</li><li>Kubernetes 定义了 CNI 网络规范，符合这个规范的实现有我们现在所熟悉的 Flannel、Calico、Cilium 等等</li></ol><br>
<br>那么，我们的一个问题是，<strong>为什么会有这么多网络方案呢？</strong>或者说，网络架构和解决方案为什么在不断演进呢？<strong>这其中当然有很多方面的原因</strong>，我们今天尝试从<strong>不断增长的计算规模</strong>来分析一下这个问题。<br>
<div class="aw-upload-img-list active">
<a href="http://dockone.io/uploads/article/20211213/989026530e62f0f6ff14bc70931fc593.jpg" target="_blank" data-fancybox-group="thumb" rel="lightbox"><img src="https://cors.zfour.workers.dev/?http://dockone.io/uploads/article/20211213/989026530e62f0f6ff14bc70931fc593.jpg" class="img-polaroid" title="2.jpg" alt="2.jpg" referrerpolicy="no-referrer"></a>
</div>
<br>
<em>The ever-increasing compute endpoints' scale</em><br>
<br>这里的计算资源基本单位可以是一台 BM、VM，也可以是容器。相应的对应三个时代：<br>
<ol><li>2000-2008 裸金属时代：传统计算模型</li><li>2008-2016 虚拟机时代：云计算兴起</li><li>2016- 容器时代：云原生开篇</li></ol><br>
<br><h3>裸金属（BM）时代</h3><h4>平台特点</h4><div class="aw-upload-img-list active">
<a href="http://dockone.io/uploads/article/20211213/5eb7b4335e379b1636bbbf3d102661bf.jpg" target="_blank" data-fancybox-group="thumb" rel="lightbox"><img src="https://cors.zfour.workers.dev/?http://dockone.io/uploads/article/20211213/5eb7b4335e379b1636bbbf3d102661bf.jpg" class="img-polaroid" title="3.jpg" alt="3.jpg" referrerpolicy="no-referrer"></a>
</div>
<br>
<em>Logical BM cluster topology and traffic patterns</em><br>
<br>如图所示，BM 时期计算平台的特点有：<br>
<ol><li>计算资源没有做虚拟化</li><li>应用（applications）直接运行在物理服务器上</li><li>典型的<strong>应用访问模式</strong>（application accessing pattern）：<strong>客户端-服务器</strong>（client-server）模式</li><li><strong>南北向流量</strong>（进出集群的流量）<strong>远大于东西向流量</strong>（集群内应用互访的的流量）</li></ol><br>
<br><h4>网络需求</h4>这种计算模型给网络提出的需求也很简单：<br>
<br>首先，能够比较高效地处理南北向流量。<br>
<br>第二，集群内<strong>应用之间的互访</strong>（东西向流量），也就是节点间的网络连通性，这与本文讨论的主题（计算实例规模）密切相关。<br>
<br>第三，在连通性的基础上，要有必要的访问控制和安全措施。例如可以基于硬件防火墙做网段级别的控制、基于 iptables 做一些简单的主机防火墙功能等等。这些内容不在本文讨论范围。<br>
<h4>网络解决方案</h4>针对以上需求，业界设计了经典的<strong>接入、汇聚、核心三级网络架构</strong>，如下图所示：<br>
<div class="aw-upload-img-list active">
<a href="http://dockone.io/uploads/article/20211213/e3fa0d16ce1c5d2ffeaf0dabb08ff8fe.jpg" target="_blank" data-fancybox-group="thumb" rel="lightbox"><img src="https://cors.zfour.workers.dev/?http://dockone.io/uploads/article/20211213/e3fa0d16ce1c5d2ffeaf0dabb08ff8fe.jpg" class="img-polaroid" title="4.jpg" alt="4.jpg" referrerpolicy="no-referrer"></a>
</div>
<br>
<em>Typical network solution for BM platforms</em><br>
<ol><li>南北向流量在核心交换机处理</li><li>所有内部子网的网关一般也配置在核心，因此集群内跨网段需要经过核心交换机</li><li>整张网络是一个<strong>二层网络</strong>，也叫<strong>桥接网络</strong>或<strong>交换网络</strong></li></ol><br>
<br>在这种模型中，由于节点/应用之间通信都可能要经过核心，因此<strong>核心交换机需要记录所有节点的 IP 和 MAC 地址信息</strong>。<br>
<h4>网络瓶颈分析（计算规模相关）</h4>在这种网络方案中，<strong>与计算节点规模相关的瓶颈</strong>最可能出现在核心交换机，因为要实现任何两台之间节点之间可达，核心交换机需要占用硬件表项（FIB TCAM）来记录集群内所有主机的 IP/MAC 信息。<br>
<div class="aw-upload-img-list active">
<a href="http://dockone.io/uploads/article/20211213/db19252badad8521d3da68a6330edaaa.jpg" target="_blank" data-fancybox-group="thumb" rel="lightbox"><img src="https://cors.zfour.workers.dev/?http://dockone.io/uploads/article/20211213/db19252badad8521d3da68a6330edaaa.jpg" class="img-polaroid" title="5.jpg" alt="5.jpg" referrerpolicy="no-referrer"></a>
</div>
<br>
<em>Bottleneck analysis</em><br>
<br>我们来算一下。假设：<br>
<ol><li>每个机房（或 Pod）200 个机柜</li><li>每个机柜 10 台服务器</li></ol><br>
<br>那可以算出整个机房有 2000 台服务器。假设每个 Node 占用一到两个 IP 地址，核心交换机将<strong>占用 2K~4K 条表项</strong>。<br>
<br><strong>假设使用的核心交换机有 16K 条表项</strong>，其中能用到 70% 左右（太高会导致哈希冲突，包只能交由交换机 CPU 软件处理，导致 CPU 升高、丢包甚至宕机），因此<strong>有效表项 是 11K 条</strong>。<br>
<br><code class="prettyprint">2K~4K</code> 与 <code class="prettyprint">11K</code> 相比可以得出结论：转发表项非常充裕，核心交换机没有表项瓶颈。<br>
<h3>虚拟机（VM）时代</h3>2008 年以后，业界开始<strong>对计算资源做大规模虚拟化</strong>，我们来到了云计算时代。<br>
<h4>平台特点</h4><div class="aw-upload-img-list active">
<a href="http://dockone.io/uploads/article/20211213/410bee7599fe665aad2c532e36d56a49.jpg" target="_blank" data-fancybox-group="thumb" rel="lightbox"><img src="https://cors.zfour.workers.dev/?http://dockone.io/uploads/article/20211213/410bee7599fe665aad2c532e36d56a49.jpg" class="img-polaroid" title="6.jpg" alt="6.jpg" referrerpolicy="no-referrer"></a>
</div>
<br>
<em>Logical VM cluster topology and traffic patterns</em><br>
<br>这一时期的特点：<br>
<ol><li>将基础设施作为服务（IaaS）交付给用户，相应地，计算资源的基本单位也从物理机（BM）变成虚拟机（VM）。</li><li>计算资源的数量或部署密度比之前高了一个数量级。</li><li>典型的<strong>应用访问模式</strong>不再是之前的客户端-服务器模型，而是变成了<strong>微服务</strong>（micro-service）模式。</li><li>与微服务模型对应的一个重要网络变化是：<strong>东西向流量超过了南北向流量</strong>，成为数据中心的主要流量。</li></ol><br>
<br>代表性的计算平台有：<br>
<ol><li>Amazon Web Service（AWS），2008</li><li>OpenStack，2010</li></ol><br>
<br><h4>网络需求</h4>这时对网络的需求：<br>
<br>首先肯定虚拟机之间的网络连通性，需要考虑的问题包括：<br>
<ol><li>同宿主机、同网段的虚拟机之间如何通信；</li><li>同宿主机、跨网段的虚拟机之间如何通信；</li><li>跨宿主机、同网段的虚拟机之间如何通信；</li><li>跨宿主机、跨网段的虚拟机之间如何通信；</li><li>虚拟机和宿主机或物理机之间如何通信；</li><li>虚拟机进出集群（集群边界）如何通信等等。</li></ol><br>
<br>第二，虚拟机的 IP 和 MAC 地址要在虚拟机的生命周期内保持不变，这一点特别重要。本质上来说<strong>这是由 IaaS 计算模型决定的</strong>：在 IaaS 模型中，交付给用户的是一台一台的虚拟机资源，因此用户看到的就是<strong>虚拟机这一层抽象</strong>，而 IP 和 MAC 是虚拟机的资源属性，底层虚拟机迁移要做到用户无感知，因此 IP/MAC 地址不能变。<br>
<br>此外还有硬多租户（hard multi-tenancy）和安全，这两点也非常重要，但跟本文要讨论的关系不大，因此这里也不展开。<br>
<h4>网络解决方案</h4><div class="aw-upload-img-list active">
<a href="http://dockone.io/uploads/article/20211213/47eec4fa27ad295ddcca42fd76013245.jpg" target="_blank" data-fancybox-group="thumb" rel="lightbox"><img src="https://cors.zfour.workers.dev/?http://dockone.io/uploads/article/20211213/47eec4fa27ad295ddcca42fd76013245.jpg" class="img-polaroid" title="7.jpg" alt="7.jpg" referrerpolicy="no-referrer"></a>
</div>
<br>
<em>Typical network solution for VM platforms</em><br>
<br>针对以上需求，典型的解决方案是所谓的大二层网络，如上图所示：<br>
<br>首先，在每个宿主机内运行一个虚拟交换机（vswitch），虚拟交换机向上连接到物理交换机，向下连接到每个虚拟机。因此网络的边界从原来的接入交换机（顶置交换机）这一层，下沉到宿主机内部，我们开始有了网络虚拟化（network virtualization）。这样整张网络成为一个大二层网络。<br>
<br><blockquote><br>这里大二层的意思是，同网段的虚拟机，不管它们是否在同一台宿主机上，彼此都能够通过二层（MAC）访问到对方。即，逻辑上它们位于同一个二层网络（二层域）。</blockquote>和这种模型对应的是一个<strong>全局的</strong>（或中心式的）负责 IP 地址分配和管理的服务（IPAM）。<br>
<br>大二层网络可以基于数据中心网络实现，也可以在宿主机内用虚拟网络实现，例如 VxLAN 隧道，如果需要动态下发配置，还可以引入 SDN。<br>
<br>如果是基于数据中心网络的大二层，那核心交换机此时不仅需要记录宿主机的 IP/MAC 信息，还需要记录所有虚拟机的 IP/MAC 信息，这样才能支持虚拟机全网可迁移。OpenStack 的 provider network 模型就是这样的设计，如下图所示：<br>
<div class="aw-upload-img-list active">
<a href="http://dockone.io/uploads/article/20211213/89768e27d1d21441bf22fa5025512465.jpg" target="_blank" data-fancybox-group="thumb" rel="lightbox"><img src="https://cors.zfour.workers.dev/?http://dockone.io/uploads/article/20211213/89768e27d1d21441bf22fa5025512465.jpg" class="img-polaroid" title="8.jpg" alt="8.jpg" referrerpolicy="no-referrer"></a>
</div>
<br>
<em>OpenStack provider network model</em><br>
<br>OpenStack 网络方案中有如下几个组件：<br>
<ol><li>Neutron-server：这是一个全局的负责 IP、Network、Subnet 等网络资源的分配和管理的服务，即上面所说的 global IPAM。</li><li>每个计算节点上会运行一个 Neutron-OVS-agent，负责虚拟机创建/销毁时的网络配置。</li><li>每台计算节点上会运行 OVS，负责连接所有虚拟机、虚拟机流量的转发等数据平面的功能。实际的拓扑比这个还要更复杂一些，因为为了支持安全组还引入了一层 Linux bridge。</li></ol><br>
<br>Provider 模型中，网关配置在数据中心网络设备（例如核心交换机）上，所有跨网段的包要经过核心交换机转发。图中从 1 到 18 的数字连起来，就是跨网段的两个虚拟机之间的转发路径。<br>
<h4>网络瓶颈分析（计算规模相关）</h4><div class="aw-upload-img-list active">
<a href="http://dockone.io/uploads/article/20211213/a66cb3f8420d05a4ae0529433c601646.jpg" target="_blank" data-fancybox-group="thumb" rel="lightbox"><img src="https://cors.zfour.workers.dev/?http://dockone.io/uploads/article/20211213/a66cb3f8420d05a4ae0529433c601646.jpg" class="img-polaroid" title="9.jpg" alt="9.jpg" referrerpolicy="no-referrer"></a>
</div>
<br>
<em>Bottleneck analysis</em><br>
<br>我们来算一下此时核心交换机的表项规模。假如这时我们机房更大了一些：<br>
<ol><li>总共 300 个机柜，但其中只有 2/3 的节点用来做虚拟化，1/3 跑传统 BM 应用，例如数据库服务</li><li>每个 Node 平均 15 台虚拟机</li></ol><br>
<br>那可以算出总共有 <code class="prettyprint">30k</code> 虚拟机，需要占用核心交换机 <code class="prettyprint">~30K</code> 表项。<br>
<br>如果使用的是主流交换机，核心有 <code class="prettyprint">64K</code> 表项，乘以 70% 是 <code class="prettyprint">44K</code>，也就是能支撑 4 万左右实例，大于 <code class="prettyprint">~30K</code>，因此能满足需求。<br>
<br>所以我们得出结论，<strong>大二层方案是适合 VM 或 IaaS 模型的</strong>。<br>
<h3>容器时代</h3>2016 年开始，我们进入了大规模容器时代。容器也被称为轻量级虚拟机，所以<strong>它的很多网络需求与虚拟机类似</strong>，但<strong>部署密度高了一个数量级</strong>。典型的容器平台：<br>
<ol><li>Mesos</li><li>Kubernetes</li></ol><br>
<br><h4>沿用大二层模型</h4>如果还是用前面的大二层模型，只是将虚拟机换成容器，如下图所示：<br>
<div class="aw-upload-img-list active">
<a href="http://dockone.io/uploads/article/20211213/23aff773f850318f2a26ff518d22f9ee.jpg" target="_blank" data-fancybox-group="thumb" rel="lightbox"><img src="https://cors.zfour.workers.dev/?http://dockone.io/uploads/article/20211213/23aff773f850318f2a26ff518d22f9ee.jpg" class="img-polaroid" title="10.jpg" alt="10.jpg" referrerpolicy="no-referrer"></a>
</div>
<br>
<em>If still using large L2 network</em><br>
<br>如果没有一些很特殊的业务需求，只是单纯基于已有大二层网络实现这样一套容器方案，其技术难度和开发量都不是很大，例如，如果要将 Kubernetes 接入 OpenStack Neutron 网络，开发一个针对 Neutron+OVS 的 CNI 插件就可以了。<br>
<br>但前面提到，这种 <strong>global IPAM + central gateway</strong> 方案中，<strong>核心交换机需要记录每个实例的 IP/MAC 信息</strong>，再考虑到容器的部署密度，可以预见的是，交换机硬件表项将撑不住。我们来具体算一下。<br>
<br>假设还是 2/3 节点做虚拟化，平均每台 Node 部署 75 个容器，那总容器将达到 <code class="prettyprint">150K</code>，远远超过了核心的表项规模。所以<strong>这种物理的大二层网络是无法支撑容器规模的</strong>。<br>
<br>除了硬件交换机表项这个瓶颈，在软件上，其实 global IPAM 也已经无法在性能上满足容器频繁的漂移、创建、销毁的需求了。所以大二层方案在软件和硬件上都已经陷入了困境。<br>
<br>因此，网络在容器时代必须做出变化了。<br>
<h4>避免网络瓶颈</h4>刚才提到的 64K 表项交换机其实在今天还不算过时，那如何在这张物理网络上支撑 15万、20 万甚至 30 万以上容器呢（刚才的计算中，还有 1/3 的节点没有做虚拟化，而且容器部署密度可以进一步提高）？<br>
<br>显然，最重要的一点就是<strong>不能让核心交换机记录所有的容器的 IP 信息</strong>。怎么做到这一点呢？<br>
<div class="aw-upload-img-list active">
<a href="http://dockone.io/uploads/article/20211213/3803dde900d1a109fccf18619500e24b.jpg" target="_blank" data-fancybox-group="thumb" rel="lightbox"><img src="https://cors.zfour.workers.dev/?http://dockone.io/uploads/article/20211213/3803dde900d1a109fccf18619500e24b.jpg" class="img-polaroid" title="11.jpg" alt="11.jpg" referrerpolicy="no-referrer"></a>
</div>
<br>
<em>If still using large L2 network</em><br>
<br>比较主流的方式是：<br>
<ol><li>在每个 Node 内用<strong>虚拟路由器</strong>（vrouter）替换<strong>虚拟交换机</strong>（vswitch），将整张<strong>大二层网络</strong>拆分成众多的<strong>小二层网络</strong>。</li><li>每个 Node 管理一个网段，负责该节点内容器 IP 的分配和回收，即从原来的 global IPAM 变成了 local IPAM。</li><li>每个节点内是一个二层域，节点内容器之间走交换（bridging/switching）；跨节点就是跨二层域，需要走路由（routing）。</li><li>每个节点内运行一个 BGP agent，负责节点之间或节点和数据中心网络之间的路由同步。</li></ol><br>
<br>采用这样的设计之后：<br>
<ol><li>核心交换机就只需要记录 Node 本身的 IP 和它管理的网段，<strong>表项重新回到与宿主机数量同一量级</strong>，而与容器的数量没有直接关系。</li><li>IPAM 从中心式变成了分散式，性能瓶颈也解决了。</li></ol><br>
<br><h4>重新审视容器的网络需求</h4>前面看到，只要对网络方案做出一些变化，就可以避免交换机的硬件瓶颈和 IPAM 的软件瓶颈。<br>
<br>现在我们来重新审视一下容器的特点，以便从需求的角度来讨论什么样的方案才是最适合容器平台的。<br>
<div class="aw-upload-img-list active">
<a href="http://dockone.io/uploads/article/20211213/8a6cd6f3cac5f272eac648228d254907.jpg" target="_blank" data-fancybox-group="thumb" rel="lightbox"><img src="https://cors.zfour.workers.dev/?http://dockone.io/uploads/article/20211213/8a6cd6f3cac5f272eac648228d254907.jpg" class="img-polaroid" title="12.jpg" alt="12.jpg" referrerpolicy="no-referrer"></a>
</div>
<br>
<em>Container platform vs. VM paltform</em><br>
<br>容器编排平台，例如 Kubernetes，跟 OpenStack 这样的虚拟机编排平台相比，<strong>最大的不同之一就是抽象的层次更高</strong>。<br>
<ol><li>虚拟机平台交付的是虚拟机实例，抽象层次是计算资源本身这一层。</li><li>而容器平台交付的是服务，例如 Kubernetes 里的 Service，服务屏蔽掉了后面的计算实例的细节。客户端只需要知道 ServiceIP 等一些更上层的访问入口。</li></ol><br>
<br>这带来的一个重要变化就是：<strong>客户端不再关心服务的具体实例数量，以及每个实例的 IP/MAC 等信息</strong>。因此实例的 IP/MAC 地址没有那么重要了，例如 Kubernetes 里大部分类型的 Pod 在重新发布之后 IP 都会变化。<br>
<br>此时，容器的核心网络需求：<br>
<ol><li>规模：例如还是刚才那个机房，现在要能支持 15 万以上容器。</li><li>快速：包括 IPAM API 的相应时间、为容器创建和删除网络的时间等，否则网络部分会拖慢整体的容器发布效率。</li><li>弹性：动态、大规模扩缩容。</li><li>其他一些需求，例如软的多租户、安全等等，本文不展开。</li></ol><br>
<br><h4>网络解决方案</h4>综合以上需求，合理的容器网络解决方案就是：<br>
<ol><li>小二层网络，加每个 node local 的 IPAM，管理自己的网段</li><li>再加跨宿主机的直接路由或者隧道</li></ol><br>
<br>如下图所示：<br>
<div class="aw-upload-img-list active">
<a href="http://dockone.io/uploads/article/20211213/fa41808d1520fb98248055fdd5980ec4.png" target="_blank" data-fancybox-group="thumb" rel="lightbox"><img src="https://cors.zfour.workers.dev/?http://dockone.io/uploads/article/20211213/fa41808d1520fb98248055fdd5980ec4.png" class="img-polaroid" title="13.png" alt="13.png" referrerpolicy="no-referrer"></a>
</div>
<br>
<em>Typical network solution for container platforms</em><br>
<br>那么，和这种模型对应的容器网络方案有：<br>
<ol><li>Calico + BGP/VxLAN，前几年用的比较多</li><li>Cilium + BGP/VxLAN，最近一两年越来越火</li></ol><br>
<br>Spine-Leaf 架构：<br>
<br>实际上数据中心网络拓扑近些年也有一个变化，从原来的接入-汇聚-核心三级架构变成了现在的 Spine-Leaf 量级架构，如下图所示：<br>
<div class="aw-upload-img-list active">
<a href="http://dockone.io/uploads/article/20211213/c218406879a3c631d674dbb92d089530.jpg" target="_blank" data-fancybox-group="thumb" rel="lightbox"><img src="https://cors.zfour.workers.dev/?http://dockone.io/uploads/article/20211213/c218406879a3c631d674dbb92d089530.jpg" class="img-polaroid" title="14.jpg" alt="14.jpg" referrerpolicy="no-referrer"></a>
</div>
<br>
<em>Typical network solution for container platforms, with Spine-Leaf</em><br>
<br>Spine 层和 Leaf 层组成一个全连接网络，换句话说，任何一个 Leaf 都连接到了任何一个 Spine。这种架构的好处：<br>
<ol><li>横向扩展性非常好：任何一层有带宽瓶颈，只需要添加一台新设备，然后和另一层的所有设备连接起来。</li><li>链路利用率更高：通过三层（L3）组网，不需要 STP 协议（STP 避免了二层环路，但使可用带宽减半）。</li><li>东西向带宽更高：更适合现代微服务的场景。</li><li>故障域更小：挂掉一台设备，影响范围更小。</li></ol><br>
<br>Spine-Leaf 拓扑下，容器的网络方案是类似的，还是基于小二层加 local IPAM，只是 BGP 建连会稍有不同。<br>
<h4>云原生网络 Cilium+BGP</h4>这里稍微就 Cilium 网络展开一些讨论，这是最近一两年流行起来的网络方案。<br>
<div class="aw-upload-img-list active">
<a href="http://dockone.io/uploads/article/20211213/a4393470a1384526f84300beabaa363a.jpg" target="_blank" data-fancybox-group="thumb" rel="lightbox"><img src="https://cors.zfour.workers.dev/?http://dockone.io/uploads/article/20211213/a4393470a1384526f84300beabaa363a.jpg" class="img-polaroid" title="15.jpg" alt="15.jpg" referrerpolicy="no-referrer"></a>
</div>
<br>
<em>Cilium powered Kubernetes cluster</em><br>
<br>Cilium 的组件如上图所示，主要包括：<br>
<ol><li>cilium-agent：每台 Node 上跑一个，同时监听 Kubernetes API server 和 cilium kvstore。</li><li>cilium kvstore：存储 Cilium 的一些全局状态，例如 identity。</li><li>cilium-operator：每个集群一个，图中没画出来，在公有云上承担 IPAM 的功能。</li></ol><br>
<br>Cilium 的核心基于 eBPF，这是 4.8 内核引入的一项革命性技术：它使得内核变得可编程。这里可编程的意思是，以前要改变内核行为，需要修改内核代码、编译调试、重新打镜像、安装运行等等，而且还要维护自己的内核分支，现在可能写几行 eBPF 代码然后动态加载到内核就能实现同样的效果。<br>
<br>eBPF 目前主要用在两个方向：动态跟踪（tracing）和网络（networking）。<br>
<br>有了 eBPF/XDP 之后，我们可以看到数据平面处理（dataplane processing）有一个趋势：<br>
<ul><li>早期基于内核协议栈处理，更多地以功能为主</li><li>前些年内核到达性能瓶颈，于是一些团队尝试将部分网络处理放到用户态，预留专门的、独享的网卡、CPU、内存等资源来收发包，例如 DPDK。</li><li>最近几年开始重新回到内核。比较有代表性的是 Facebook 的 L4LB Katran，用 eBPF/XDP 重新之后比原来基于 IPVS 的版本快了 10 倍，性能已经和 DPDK 一个量级，而且还可以复用内核已有的基础设施和生态。而用户态方式最大的问题之一是原有的网络工具几乎都失效了，并且没有通用的 L4-L7 协议栈支持。</li></ul><br>
<br>Cilium 是基于 eBPF 实现的一个网络方案，主打高性能和安全。对内核要求较高，但也不是非常高，根据我们的使用经验：<br>
<ol><li>4.14 能用</li><li>4.19 够用</li><li>5.x 内核外做了一些优化，或者原生实现了某些高级功能</li></ol><br>
<br><h3>总结</h3>网络方案的演进是一个复杂的过程，涉及到很多因素，本文尝试从计算规模的角度对这一问题进行了分析。从中可以看出，每种网络方案或网络模型都不是凭空出现的，它们本质上都是业务需求在网络层的反映；而所有业务需求中，最重要的可能就是规模。<br>
<br>原文链接：<a href="https://arthurchiao.art/blog/network-evolves-zh/" rel="nofollow" target="_blank">https://arthurchiao.art/blog/network-evolves-zh/</a>
                                                                <div class="aw-upload-img-list">
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                </div>
                                
                                                                <ul class="aw-upload-file-list">
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            </ul>
                                                              
</div>
            