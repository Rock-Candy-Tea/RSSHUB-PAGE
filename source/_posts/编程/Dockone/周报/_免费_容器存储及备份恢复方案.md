
---
title: '_免费_容器存储及备份恢复方案'
categories: 
 - 编程
 - Dockone
 - 周报
headimg: 'https://cors.zfour.workers.dev/?http://dockone.io/uploads/article/20211115/cac76a8bcca2ff5917489500ab8371f9.png'
author: Dockone
comments: false
date: 2021-11-20 07:07:32
thumbnail: 'https://cors.zfour.workers.dev/?http://dockone.io/uploads/article/20211115/cac76a8bcca2ff5917489500ab8371f9.png'
---

<div>   
<br>云原生时代为什么还需要本地存储？<br>
<br>云原生时代，对于有存储应用的容器化上云，一般的解决思路是“计算存储分离”，计算层通过容器技术实现弹性伸缩，而相应的存储层也需要配合进行“动态挂载”，要实现动态挂载能力，使用基于网络的存储系统可能是最佳选择。然而，“网络”存储的磁盘IO性能差、自带高可用能力的中间件系统不需要存储层的“动态挂载” 等种种原因，业界对本地存储还是“青睐有加”。因此类似RabbitMQ、Kafka这样的中间件系统，优先使用本地盘，然后通过Kubernetes增强自动化运维能力，解决原来磁盘手动管理问题，实现动态分配、扩容、隔离。<br>
<br>有没有更适合Kubernetes的备份恢复方案？<br>
<br>传统的数据备份方案，一种是利用存储数据的服务端实现定期快照的备份，另一种是在每台目标服务器上部署专有备份agent并指定备份数据目录，定期把数据远程复制到外部存储上。这两种方式均存在“备份机制固化”、“数据恢复慢”等问题，无法适应容器化后的弹性、池化部署场景。我们需要更贴合Kubernetes容器场景的备份恢复能力，实现一键备份、快速恢复。<br>
<h3>整体计划</h3><ol><li>准备一个Kubernetes集群，master节点不跑workload，最好能有2个worker节点；</li><li>部署Carina云原生本地容器存储方案，测试本地盘自动化管理能力</li><li>部署Velero云原生备份方案，测试数据备份和恢复能力</li></ol><br>
<br><h3>Kubernetes环境</h3><ul><li>版本：v1.19.14</li><li>集群规模：1master 2worker</li><li>磁盘挂载情况：除了根目录使用了一块独立盘，初始状态未使用其他的磁盘</li></ul><br>
<br><h3>部署Carina</h3>1、部署脚本<br>
<br>可以参考<a href="https://github.com/carina-io/carina/blob/main/README_zh.md#%E5%BF%AB%E9%80%9F%E5%BC%80%E5%A7%8B">官方文档</a>，部署方式区分1.22版本前后，大概率由于1.22版本很多API发生变更。<br>
<br>2、本地裸盘准备：<br>
<br>为每个worker挂载一块裸盘，建议至少20G，Carina默认会占用10G磁盘空间，作为存储管理元数据。由于我自己使用了谷歌云，可以通过如下步骤实现：第一个是SSD，第二个是HDD。<br>
<div class="aw-upload-img-list active">
<a href="http://dockone.io/uploads/article/20211115/cac76a8bcca2ff5917489500ab8371f9.png" target="_blank" data-fancybox-group="thumb" rel="lightbox"><img src="https://cors.zfour.workers.dev/?http://dockone.io/uploads/article/20211115/cac76a8bcca2ff5917489500ab8371f9.png" class="img-polaroid" title="1.png" alt="1.png" referrerpolicy="no-referrer"></a>
</div>
<br>
<em>gcloud apply ssd</em><br>
<br><div class="aw-upload-img-list active">
<a href="http://dockone.io/uploads/article/20211115/ba11d6aa986904531b0b2bea655cb5ed.png" target="_blank" data-fancybox-group="thumb" rel="lightbox"><img src="https://cors.zfour.workers.dev/?http://dockone.io/uploads/article/20211115/ba11d6aa986904531b0b2bea655cb5ed.png" class="img-polaroid" title="2.png" alt="2.png" referrerpolicy="no-referrer"></a>
</div>
<br>
<em>gcloud apply hdd</em><br>
<br>3、确认Carina组件能够搜索并读取到新挂载的裸盘，需要通过修改Carina默认扫描本地盘的策略：<br>
<pre class="prettyprint"># 通过以下命令查看配置扫描策略的ConfigMap<br>
# 官网说明文档：https://github.com/carina-io/carina/blob/main/docs/manual/disk-manager.md<br>
> kubectl describe cm carina-csi-config -n kube-system<br>
Name:         carina-csi-config<br>
Namespace:    kube-system<br>
Labels:       class=carina<br>
Annotations:  <none><br>
<h1>Data</h1>config.json:<br>
----<br>
&#123;<br>
# 根据Google Cloud磁盘的命名规则作了更改，匹配以sd开头的磁盘<br>
"diskSelector": ["sd+"],<br>
# 扫描周期180秒<br>
"diskScanInterval": "180",<br>
"diskGroupPolicy": "type",<br>
"schedulerStrategy": "spradout"<br>
&#125; <br>
</pre><br>
4、通过查看carina组件状态，确认本地盘已经被识别<br>
<pre class="prettyprint">> kubectl get node u20-w1 -o template --template=&#123;&#123;.status.capacity&#125;&#125;<br>
map[carina.storage.io/carina-vg-hdd:200 carina.storage.io/carina-vg-ssd:20 cpu:2 ephemeral-storage:30308240Ki hugepages-1Gi:0 hugepages-2Mi:0 memory:4022776Ki pods:110]<br>
<br>
> kubectl get node u20-w1 -o template --template=&#123;&#123;.status.allocatable&#125;&#125;<br>
map[carina.storage.io/carina-vg-hdd:189 carina.storage.io/carina-vg-ssd:1 cpu:2 ephemeral-storage:27932073938 hugepages-1Gi:0 hugepages-2Mi:0 memory:3920376Ki pods:110]<br>
<br>
# 可以看到HDD容量已经变成200了、SSD容量变成20，为什么能区分SSD和HDD呢？这里先按下不表<br>
# 这里也能看到预留了10G空间不可使用，因为200G新盘刚加入只有189G（考虑到有磁盘单位换算带来的误差）可用。<br>
# 这些信息很重要，当PV创建时会从该Node信息中获取当前节点磁盘容量，然后根据PV调度策略进行调度<br>
<br>
# 还有个集中查看磁盘使用情况的入口<br>
> kubectl get configmap carina-node-storage -n kube-system -o json | jq .data.node<br>
[<br>
<br>
&#123;<br>
  "allocatable.carina.storage.io/carina-vg-hdd":"189",<br>
  "allocatable.carina.storage.io/carina-vg-ssd":"1",<br>
  "capacity.carina.storage.io/carina-vg-hdd":"200",<br>
  "capacity.carina.storage.io/carina-vg-ssd":"20",<br>
  "nodeName":"u20-w1"<br>
&#125;,<br>
&#123;<br>
  "allocatable.carina.storage.io/carina-vg-hdd":"189",<br>
  "allocatable.carina.storage.io/carina-vg-ssd":"0",<br>
  "capacity.carina.storage.io/carina-vg-hdd":"200",<br>
  "capacity.carina.storage.io/carina-vg-ssd":"0",<br>
  "nodeName":"u20-w2"<br>
&#125;<br>
] <br>
</pre><br>
5、为什么能自动识别HDD和SDD呢？<br>
<pre class="prettyprint"># carina-node服务启动时会自动将节点上磁盘按照SSD和HDD进行分组并组建成vg卷组<br>
# 使用命令lsblk --output NAME,ROTA查看磁盘类型，ROTA=1为HDD磁盘 ROTA=0为SSD磁盘<br>
# 支持文件存储及块设备存储，其中文件存储支持xfs和ext4格式<br>
# 下面是事前声明的storageclass，用来自动创建PV<br>
> k get sc csi-carina-sc -o json | jq .metadata.annotations<br>
&#123;<br>
"kubectl.kubernetes.io/last-applied-configuration":&#123;<br>
    "allowVolumeExpansion":true,<br>
    "apiVersion":"storage.k8s.io/v1",<br>
    "kind":"StorageClass",<br>
    "metadata":&#123;<br>
        "annotations":&#123;&#125;,<br>
        "name":"csi-carina-sc"<br>
    &#125;,<br>
    "mountOptions":[<br>
        "rw"<br>
    ],<br>
    "parameters":&#123;<br>
        "csi.storage.k8s.io/fstype":"ext4"<br>
    &#125;,<br>
    "provisioner":"carina.storage.io",<br>
    "reclaimPolicy":"Delete",<br>
    "volumeBindingMode":"WaitForFirstConsumer"<br>
&#125;<br>
&#125; <br>
</pre><br>
<h3>测试Carina自动分配PV能力</h3>1、想要Carina具备自动创建PV能力，需要先声明并创建StorageClass。<br>
<pre class="prettyprint"># 创建storageclass的yaml<br>
---<br>
apiVersion: storage.k8s.io/v1<br>
kind: StorageClass<br>
metadata:<br>
name: csi-carina-sc<br>
provisioner: carina.storage.io # 这是该CSI驱动的名称，不允许更改<br>
parameters:<br>
# 支持xfs,ext4两种文件格式，如果不填则默认ext4<br>
csi.storage.k8s.io/fstype: ext4<br>
# 这是选择磁盘分组，该项目会自动将SSD及HDD磁盘分组<br>
# SSD：ssd HDD: hdd<br>
# 如果不填会随机选择磁盘类型<br>
#carina.storage.io/disk-type: hdd<br>
reclaimPolicy: Delete<br>
allowVolumeExpansion: true # 支持扩容，定为true便可<br>
# WaitForFirstConsumer表示被容器绑定调度后再创建pv<br>
volumeBindingMode: WaitForFirstConsumer<br>
# 支持挂载参数设置，这里配置为读写模式<br>
mountOptions:<br>
- rw<br>
</pre><br>
kubectl apply后，可以通过以下命令确认：<br>
<pre class="prettyprint">> kubectl get sc csi-carina-sc -o json | jq .metadata.annotations<br>
&#123;<br>
"kubectl.kubernetes.io/last-applied-configuration":&#123;<br>
    "allowVolumeExpansion":true,<br>
    "apiVersion":"storage.k8s.io/v1",<br>
    "kind":"StorageClass",<br>
    "metadata":&#123;<br>
        "annotations":&#123;&#125;,<br>
        "name":"csi-carina-sc"<br>
    &#125;,<br>
    "mountOptions":[<br>
        "rw"<br>
    ],<br>
    "parameters":&#123;<br>
        "csi.storage.k8s.io/fstype":"ext4"<br>
    &#125;,<br>
    "provisioner":"carina.storage.io",<br>
    "reclaimPolicy":"Delete",<br>
    "volumeBindingMode":"WaitForFirstConsumer"<br>
&#125;<br>
&#125; <br>
</pre><br>
2、部署带存储的测试应用<br>
<br>测试场景比较简单，使用简单Nginx服务，挂载数据盘，存放自定义html页面。<br>
<pre class="prettyprint"># pvc for nginx html<br>
---<br>
apiVersion: v1<br>
kind: PersistentVolumeClaim<br>
metadata:<br>
name: csi-carina-pvc-big<br>
namespace: default<br>
spec:<br>
accessModes:<br>
- ReadWriteOnce<br>
resources:<br>
requests:<br>
  storage: 10Gi<br>
# 指定Carina的StorageClass名称<br>
storageClassName: csi-carina-sc<br>
volumeMode: Filesystem<br>
<br>
# nginx deployment yaml<br>
---<br>
apiVersion: apps/v1<br>
kind: Deployment<br>
metadata:<br>
name: carina-deployment-big<br>
namespace: default<br>
labels:<br>
app: web-server-big<br>
spec:<br>
replicas: 1<br>
selector:<br>
matchLabels:<br>
  app: web-server-big<br>
template:<br>
metadata:<br>
  labels:<br>
    app: web-server-big<br>
spec:<br>
  containers:<br>
    - name: web-server<br>
      image: nginx:latest<br>
      imagePullPolicy: "IfNotPresent"<br>
      volumeMounts:<br>
        - name: mypvc-big<br>
          mountPath: /usr/share/nginx/html # Nginx默认将页面内容存放在这个文件夹<br>
  volumes:<br>
    - name: mypvc-big<br>
      persistentVolumeClaim:<br>
        claimName: csi-carina-pvc-big<br>
        readOnly: false<br>
</pre><br>
查看测试应用运行情况：<br>
<pre class="prettyprint"># pvc<br>
> kubectl get pvc<br>
NAME                 STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS    AGE<br>
csi-carina-pvc-big   Bound    pvc-74e683f9-d2a4-40a0-95db-85d1504fd961   10Gi       RWO            csi-carina-sc   109s<br>
<h1>pv</h1>> kubectl get pv<br>
NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                        STORAGECLASS    REASON   AGE<br>
pvc-74e683f9-d2a4-40a0-95db-85d1504fd961   10Gi       RWO            Delete           Bound    default/csi-carina-pvc-big   csi-carina-sc            109s<br>
<br>
# Nginx Pod<br>
> kubectl get po -l app=web-server-big -o wide<br>
NAME                                    READY   STATUS    RESTARTS   AGE     IP          NODE     NOMINATED NODE   READINESS GATES<br>
carina-deployment-big-6b78fb9fd-mwf8g   1/1     Running   0          3m48s   10.0.2.69   u20-w2   <none>           <none><br>
<br>
# 查看相关Node的磁盘使用情况，可分配的大小已经发生变化，缩小了10<br>
> kubectl get node u20-w2 -o template --template=&#123;&#123;.status.allocatable&#125;&#125;<br>
map[carina.storage.io/carina-vg-hdd:179 carina.storage.io/carina-vg-ssd:0 cpu:2 ephemeral-storage:27932073938 hugepages-1Gi:0 hugepages-2Mi:0 memory:3925000Ki pods:110]<br>
</pre><br>
登录运行测试服务的Node节点，查看磁盘挂载情况：<br>
<pre class="prettyprint"># 磁盘分配情况如下，有meta和pool两个lvm<br>
> lsblk<br>
...<br>
sdb       8:16   0   200G  0 disk <br>
├─carina--vg--hdd-thin--pvc--74e683f9--d2a4--40a0--95db--85d1504fd961_tmeta<br>
│       253:0    0    12M  0 lvm  <br>
│ └─carina--vg--hdd-thin--pvc--74e683f9--d2a4--40a0--95db--85d1504fd961-tpool<br>
│       253:2    0    10G  0 lvm  <br>
│   ├─carina--vg--hdd-thin--pvc--74e683f9--d2a4--40a0--95db--85d1504fd961<br>
│   │   253:3    0    10G  1 lvm  <br>
│   └─carina--vg--hdd-volume--pvc--74e683f9--d2a4--40a0--95db--85d1504fd961<br>
│       253:4    0    10G  0 lvm  /var/lib/kubelet/pods/57ded9fb-4c82-4668-b77b-7dc02ba05fc2/volumes/kubernetes.io~<br>
└─carina--vg--hdd-thin--pvc--74e683f9--d2a4--40a0--95db--85d1504fd961_tdata<br>
    253:1    0    10G  0 lvm  <br>
└─carina--vg--hdd-thin--pvc--74e683f9--d2a4--40a0--95db--85d1504fd961-tpool<br>
    253:2    0    10G  0 lvm  <br>
├─carina--vg--hdd-thin--pvc--74e683f9--d2a4--40a0--95db--85d1504fd961<br>
│   253:3    0    10G  1 lvm  <br>
└─carina--vg--hdd-volume--pvc--74e683f9--d2a4--40a0--95db--85d1504fd961<br>
    253:4    0    10G  0 lvm  /var/lib/kubelet/pods/57ded9fb-4c82-4668-b77b-7dc02ba05fc2/volumes/kubernetes.io~<br>
# vgs<br>
> vgs<br>
VG            #PV #LV #SN Attr   VSize    VFree  <br>
carina-vg-hdd   1   2   0 wz--n- <200.00g 189.97g<br>
# lvs<br>
> lvs<br>
LV                                              VG            Attr       LSize  Pool                                          Origin Data%  Meta%  Move Log Cpy%Sync Convert<br>
thin-pvc-74e683f9-d2a4-40a0-95db-85d1504fd961   carina-vg-hdd twi-aotz-- 10.00g                                                      2.86   11.85                           <br>
volume-pvc-74e683f9-d2a4-40a0-95db-85d1504fd961 carina-vg-hdd Vwi-aotz-- 10.00g thin-pvc-74e683f9-d2a4-40a0-95db-85d1504fd961        2.86<br>
<br>
# 使用lvm命令行工具查看磁盘挂载信息<br>
> pvs<br>
PV         VG            Fmt  Attr PSize    PFree  <br>
/dev/sdb   carina-vg-hdd lvm2 a--  <200.00g 189.97g<br>
> pvdisplay<br>
--- Physical volume ---<br>
PV Name               /dev/sdb<br>
VG Name               carina-vg-hdd<br>
PV Size               <200.00 GiB / not usable 3.00 MiB<br>
Allocatable           yes <br>
PE Size               4.00 MiB<br>
Total PE              51199<br>
Free PE               48633<br>
Allocated PE          2566<br>
PV UUID               Wl6ula-kD54-Mj5H-ZiBc-aHPB-6RHI-mXs9R9<br>
<br>
> lvs<br>
LV                                              VG            Attr       LSize  Pool                                          Origin Data%  Meta%  Move Log Cpy%Sync Convert<br>
thin-pvc-74e683f9-d2a4-40a0-95db-85d1504fd961   carina-vg-hdd twi-aotz-- 10.00g                                                      2.86   11.85                           <br>
volume-pvc-74e683f9-d2a4-40a0-95db-85d1504fd961 carina-vg-hdd Vwi-aotz-- 10.00g thin-pvc-74e683f9-d2a4-40a0-95db-85d1504fd961        2.86                                   <br>
> lvdisplay<br>
--- Logical volume ---<br>
LV Name                thin-pvc-74e683f9-d2a4-40a0-95db-85d1504fd961<br>
VG Name                carina-vg-hdd<br>
LV UUID                kB7DFm-dl3y-lmop-p7os-3EW6-4Toy-slX7qn<br>
LV Write Access        read/write (activated read only)<br>
LV Creation host, time u20-w2, 2021-11-09 05:31:18 +0000<br>
LV Pool metadata       thin-pvc-74e683f9-d2a4-40a0-95db-85d1504fd961_tmeta<br>
LV Pool data           thin-pvc-74e683f9-d2a4-40a0-95db-85d1504fd961_tdata<br>
LV Status              available<br>
# open                 2<br>
LV Size                10.00 GiB<br>
Allocated pool data    2.86%<br>
Allocated metadata     11.85%<br>
Current LE             2560<br>
Segments               1<br>
Allocation             inherit<br>
Read ahead sectors     auto<br>
- currently set to     256<br>
Block device           253:2<br>
<br>
--- Logical volume ---<br>
LV Path                /dev/carina-vg-hdd/volume-pvc-74e683f9-d2a4-40a0-95db-85d1504fd961<br>
LV Name                volume-pvc-74e683f9-d2a4-40a0-95db-85d1504fd961<br>
VG Name                carina-vg-hdd<br>
LV UUID                vhDYe9-KzPc-qqJk-2o1f-TlCv-0TDL-643b8r<br>
LV Write Access        read/write<br>
LV Creation host, time u20-w2, 2021-11-09 05:31:19 +0000<br>
LV Pool name           thin-pvc-74e683f9-d2a4-40a0-95db-85d1504fd961<br>
LV Status              available<br>
# open                 1<br>
LV Size                10.00 GiB<br>
Mapped size            2.86%<br>
Current LE             2560<br>
Segments               1<br>
Allocation             inherit<br>
Read ahead sectors     auto<br>
- currently set to     256<br>
Block device           253:4<br>
</pre><br>
进入磁盘，添加自定义内容，查看存储情况：<br>
<pre class="prettyprint"># 进入容器内，创建自定义页面<br>
> kubectl exec -ti carina-deployment-big-6b78fb9fd-mwf8g -- /bin/bash<br>
/# cd /usr/share/nginx/html/<br>
/usr/share/nginx/html# ls<br>
lost+found<br>
/usr/share/nginx/html# echo "hello carina" > index.html<br>
/usr/share/nginx/html# curl localhost<br>
hello carina<br>
/usr/share/nginx/html# echo "test carina" > test.html<br>
/usr/share/nginx/html# curl localhost/test.html<br>
test carina<br>
<br>
# 登录Node节点，进入挂载点，查看上面刚刚创建的内容<br>
> df -h<br>
...<br>
/dev/carina/volume-pvc-74e683f9-d2a4-40a0-95db-85d1504fd961  9.8G   37M  9.7G   1% /var/lib/kubelet/pods/57ded9fb-4c82-4668-b77b-7dc02ba05fc2/volumes/kubernetes.io~csi/pvc-74e683f9-d2a4-40a0-95db-85d1504fd961/mount<br>
> cd /var/lib/kubelet/pods/57ded9fb-4c82-4668-b77b-7dc02ba05fc2/volumes/kubernetes.io~csi/pvc-74e683f9-d2a4-40a0-95db-85d1504fd961/mount<br>
> ll<br>
total 32<br>
drwxrwsrwx 3 root root  4096 Nov  9 05:54 ./<br>
drwxr-x--- 3 root root  4096 Nov  9 05:31 ../<br>
-rw-r--r-- 1 root root    13 Nov  9 05:54 index.html<br>
drwx------ 2 root root 16384 Nov  9 05:31 lost+found/<br>
-rw-r--r-- 1 root root    12 Nov  9 05:54 test.html<br>
</pre><br>
<h3>部署Velero</h3>1、下载Velero命令行工具，本次使用的是1.7.0版本<br>
<pre class="prettyprint">> wget <https://github.com/vmware-tanzu/velero/releases/download/v1.7.0/velero-v1.7.0-linux-amd64.tar.gz><br>
> tar -xzvf velero-v1.7.0-linux-amd64.tar.gz<br>
> cd velero-v1.7.0-linux-amd64 && cp velero /usr/local/bin/<br>
> velero<br>
Velero is a tool for managing disaster recovery, specifically for Kubernetes<br>
cluster resources. It provides a simple, configurable, and operationally robust<br>
way to back up your application state and associated data.<br>
<br>
If you're familiar with kubectl, Velero supports a similar model, allowing you to<br>
execute commands such as 'velero get backup' and 'velero create schedule'. The same<br>
operations can also be performed as 'velero backup get' and 'velero schedule create'.<br>
<br>
Usage:<br>
velero [command]<br>
<br>
Available Commands:<br>
backup            Work with backups<br>
backup-location   Work with backup storage locations<br>
bug               Report a Velero bug<br>
client            Velero client related commands<br>
completion        Generate completion script<br>
create            Create velero resources<br>
debug             Generate debug bundle<br>
delete            Delete velero resources<br>
describe          Describe velero resources<br>
get               Get velero resources<br>
help              Help about any command<br>
install           Install Velero<br>
plugin            Work with plugins<br>
restic            Work with restic<br>
restore           Work with restores<br>
schedule          Work with schedules<br>
snapshot-location Work with snapshot locations<br>
uninstall         Uninstall Velero<br>
version           Print the velero version and associated image<br>
</pre><br>
2、部署minio对象存储，作为Velero后端存储<br>
<br>为了部署Velero服务端，需要优先准备好一个后端存储。Velero支持很多类型的后端存储，详细看这里：<a href="https://velero.io/docs/v1.7/supported-providers/" rel="nofollow" target="_blank">https://velero.io/docs/v1.7/supported-providers/</a>。只要遵循AWS S3存储接口规范的，都可以对接，本次使用兼容S3接口的minio服务作为后端存储，部署minio方式如下，其中就使用到了carina storageclass提供磁盘创建能力：<br>
<pre class="prettyprint"># 参考文档：https://velero.io/docs/v1.7/contributions/minio/<br>
# 统一部署在minio命名空间<br>
---<br>
apiVersion: v1<br>
kind: Namespace<br>
metadata:<br>
name: minio<br>
# 为minio后端存储申请8G磁盘空间，走的就是carina storageclass<br>
---<br>
apiVersion: v1<br>
kind: PersistentVolumeClaim<br>
metadata:<br>
name: minio-storage-pvc<br>
namespace: minio<br>
spec:<br>
accessModes:<br>
- ReadWriteOnce<br>
resources:<br>
requests:<br>
  storage: 8Gi<br>
storageClassName: csi-carina-sc # 指定carina storageclass<br>
volumeMode: Filesystem<br>
---<br>
apiVersion: apps/v1<br>
kind: Deployment<br>
metadata:<br>
namespace: minio<br>
name: minio<br>
labels:<br>
component: minio<br>
spec:<br>
strategy:<br>
type: Recreate<br>
selector:<br>
matchLabels:<br>
  component: minio<br>
template:<br>
metadata:<br>
  labels:<br>
    component: minio<br>
spec:<br>
  volumes:<br>
  - name: storage<br>
    persistentVolumeClaim:<br>
      claimName: minio-storage-pvc<br>
      readOnly: false<br>
  - name: config<br>
    emptyDir: &#123;&#125;<br>
  containers:<br>
  - name: minio<br>
    image: minio/minio:latest<br>
    imagePullPolicy: IfNotPresent<br>
    args:<br>
    - server<br>
    - /storage<br>
    - --config-dir=/config<br>
    - --console-address ":9001" # 配置前端页面的暴露端口<br>
    env:<br>
    - name: MINIO_ACCESS_KEY<br>
      value: "minio"<br>
    - name: MINIO_SECRET_KEY<br>
      value: "minio123"<br>
    ports:<br>
    - containerPort: 9000<br>
    - containerPort: 9001<br>
    volumeMounts:<br>
    - name: storage<br>
      mountPath: "/storage"<br>
    - name: config<br>
      mountPath: "/config"<br>
# 使用nodeport创建SVC，提供对外服务能力，包括前端页面和后端API<br>
---<br>
apiVersion: v1<br>
kind: Service<br>
metadata:<br>
namespace: minio<br>
name: minio<br>
labels:<br>
component: minio<br>
spec:<br>
# ClusterIP is recommended for production environments.<br>
# Change to NodePort if needed per documentation,<br>
# but only if you run Minio in a test/trial environment, for example with Minikube.<br>
type: NodePort<br>
ports:<br>
- name: console<br>
  port: 9001<br>
  targetPort: 9001<br>
- name: api<br>
  port: 9000<br>
  targetPort: 9000<br>
  protocol: TCP<br>
selector:<br>
component: minio<br>
<br>
# 初始化创建Velero的bucket<br>
---<br>
apiVersion: batch/v1<br>
kind: Job<br>
metadata:<br>
namespace: minio<br>
name: minio-setup<br>
labels:<br>
component: minio<br>
spec:<br>
template:<br>
metadata:<br>
  name: minio-setup<br>
spec:<br>
  restartPolicy: OnFailure<br>
  volumes:<br>
  - name: config<br>
    emptyDir: &#123;&#125;<br>
  containers:<br>
  - name: mc<br>
    image: minio/mc:latest<br>
    imagePullPolicy: IfNotPresent<br>
    command:<br>
    - /bin/sh<br>
    - -c<br>
    - "mc --config-dir=/config config host add velero <http://minio:9000> minio minio123 && mc --config-dir=/config mb -p velero/velero"<br>
    volumeMounts:<br>
    - name: config<br>
      mountPath: "/config"<br>
</pre><br>
部署minio完成后如下所示：<br>
<pre class="prettyprint">> kubectl get all -n minio<br>
NAME                         READY   STATUS    RESTARTS   AGE<br>
pod/minio-686755b769-k6625   1/1     Running   0          6d15h<br>
<br>
NAME            TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)                          AGE<br>
service/minio   NodePort   10.98.252.130   <none>        36369:31943/TCP,9000:30436/TCP   6d15h<br>
<br>
NAME                    READY   UP-TO-DATE   AVAILABLE   AGE<br>
deployment.apps/minio   1/1     1            1           6d15h<br>
<br>
NAME                               DESIRED   CURRENT   READY   AGE<br>
replicaset.apps/minio-686755b769   1         1         1       6d15h<br>
replicaset.apps/minio-c9c844f67    0         0         0       6d15h<br>
<br>
NAME                    COMPLETIONS   DURATION   AGE<br>
job.batch/minio-setup   1/1           114s       6d15h<br>
</pre><br>
打开minio页面，使用部署时定义的账号登录：<br>
<div class="aw-upload-img-list active">
<a href="http://dockone.io/uploads/article/20211115/4e29b3eaa68d3a07fc674aaf3ded1d6f.png" target="_blank" data-fancybox-group="thumb" rel="lightbox"><img src="https://cors.zfour.workers.dev/?http://dockone.io/uploads/article/20211115/4e29b3eaa68d3a07fc674aaf3ded1d6f.png" class="img-polaroid" title="3.png" alt="3.png" referrerpolicy="no-referrer"></a>
</div>
<br>
<em>minio login</em><br>
<br><div class="aw-upload-img-list active">
<a href="http://dockone.io/uploads/article/20211115/02a192057cc8fe5e9bd70e3328e0b5bb.png" target="_blank" data-fancybox-group="thumb" rel="lightbox"><img src="https://cors.zfour.workers.dev/?http://dockone.io/uploads/article/20211115/02a192057cc8fe5e9bd70e3328e0b5bb.png" class="img-polaroid" title="4.png" alt="4.png" referrerpolicy="no-referrer"></a>
</div>
<br>
<em>minio dashboard</em><br>
<br>3、安装Velero服务端<br>
<pre class="prettyprint"># 注意添加--use-restric，开启PV数据备份<br>
# 注意最后一行，使用了k8s svc域名服务，<http://minio.minio.svc:9000><br>
velero install     <br>
--provider aws     <br>
--plugins velero/velero-plugin-for-aws:v1.2.1     <br>
--bucket velero     <br>
--secret-file ./minio-cred     <br>
--namespace velero    <br>
--use-restic <br>
--use-volume-snapshots=false  <br>
--backup-location-config region=minio,s3ForcePathStyle="true",s3Url=http://minio.minio.svc:9000<br>
</pre><br>
安装完成后如下所示：<br>
<pre class="prettyprint"># 除了velero之外，<br>
# 还有restic，它是负责备份PV数据的核心组件，需要保证每个节点上的PV都能备份，因此它使用了DaemonSet模式<br>
# 详细参考：<https://velero.io/docs/v1.7/restic/><br>
> kubectl get all -n velero<br>
NAME                          READY   STATUS    RESTARTS   AGE<br>
pod/restic-g5q5k              1/1     Running   0          6d15h<br>
pod/restic-jdk7h              1/1     Running   0          6d15h<br>
pod/restic-jr8f7              1/1     Running   0          5d22h<br>
pod/velero-6979cbd56b-s7v99   1/1     Running   0          5d21h<br>
<br>
NAME                    DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE<br>
daemonset.apps/restic   3         3         3       3            3           <none>          6d15h<br>
<br>
NAME                     READY   UP-TO-DATE   AVAILABLE   AGE<br>
deployment.apps/velero   1/1     1            1           6d15h<br>
<br>
NAME                                DESIRED   CURRENT   READY   AGE<br>
replicaset.apps/velero-6979cbd56b   1         1         1       6d15h<br>
</pre><br>
<h3>使用Velero备份应用及其数据</h3>1、使用velero backup命令备份带PV的测试应用<br>
<pre class="prettyprint"># 通过--selector选项指定应用标签<br>
# 通过--default-volumes-to-restic选项显式声明使用restric备份pv数据<br>
> velero backup create nginx-backup --selector app=web-server-big --default-volumes-to-restic<br>
I1109 09:14:31.380431 1527737 request.go:655] Throttling request took 1.158837643s, request: GET:<https://10.140.0.8:6443/apis/networking.istio.io/v1beta1?timeout=32s><br>
Backup request "nginx-pv-backup" submitted successfully.<br>
Run `velero backup describe nginx-pv-backup` or `velero backup logs nginx-pv-backup` for more details.<br>
<br>
# 无法查看执行备份的日志，说是无法访问minio接口，很奇怪<br>
> velero backup logs nginx-pv-backup<br>
I1109 09:15:04.840199 1527872 request.go:655] Throttling request took 1.146201139s, request: GET:<https://10.140.0.8:6443/apis/networking.k8s.io/v1beta1?timeout=32s><br>
An error occurred: Get "<http://minio.minio.svc:9000/velero/backups/nginx-pv-backup/nginx-pv-backup-logs.gz?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=minio%2F20211109%2Fminio%2Fs3%2Faws4_request&X-Amz-Date=20211109T091506Z&X-Amz-Expires=600&X-Amz-SignedHeaders=host&X-Amz-Signature=b48c1988101b544329effb67aee6a7f83844c4630eb1d19db30f052e3603b9b2>": dial tcp: lookup minio.minio.svc on 127.0.0.53:53: no such host<br>
<br>
# 可以看到详细的备份执行信息，<br>
# 这个备份是会过期的，默认是1个月有效期<br>
> velero backup describe nginx-pv-backup<br>
I1109 09:15:25.834349 1527945 request.go:655] Throttling request took 1.147122392s, request: GET:<https://10.140.0.8:6443/apis/pkg.crossplane.io/v1beta1?timeout=32s><br>
Name:         nginx-pv-backup<br>
Namespace:    velero<br>
Labels:       velero.io/storage-location=default<br>
Annotations:  velero.io/source-cluster-k8s-gitversion=v1.19.14<br>
          velero.io/source-cluster-k8s-major-version=1<br>
          velero.io/source-cluster-k8s-minor-version=19<br>
<br>
Phase:  Completed<br>
<br>
Errors:    0<br>
Warnings:  1<br>
<br>
Namespaces:<br>
Included:  *<br>
Excluded:  <none><br>
<br>
Resources:<br>
Included:        *<br>
Excluded:        <none><br>
Cluster-scoped:  auto<br>
<br>
Label selector:  app=web-server-big<br>
<br>
Storage Location:  default<br>
<br>
Velero-Native Snapshot PVs:  auto<br>
<br>
TTL:  720h0m0s<br>
<br>
Hooks:  <none><br>
<br>
Backup Format Version:  1.1.0<br>
<br>
Started:    2021-11-09 09:14:32 +0000 UTC<br>
Completed:  2021-11-09 09:15:05 +0000 UTC<br>
<br>
Expiration:  2021-12-09 09:14:32 +0000 UTC<br>
<br>
Total items to be backed up:  9<br>
Items backed up:              9<br>
<br>
Velero-Native Snapshots: <none included><br>
<br>
Restic Backups (specify --details for more information):<br>
Completed:  1<br>
</pre><br>
2、打开minio页面，查看备份数据的明细<br>
<br>在<strong>backups</strong>文件夹下有个<strong>nginx-pv-backup</strong>文件夹，里面有很多压缩文件，有机会可以分析下。<br>
<div class="aw-upload-img-list active">
<a href="http://dockone.io/uploads/article/20211115/be353a086160badb9053d3c87bb819b8.png" target="_blank" data-fancybox-group="thumb" rel="lightbox"><img src="https://cors.zfour.workers.dev/?http://dockone.io/uploads/article/20211115/be353a086160badb9053d3c87bb819b8.png" class="img-polaroid" title="5.png" alt="5.png" referrerpolicy="no-referrer"></a>
</div>
<br>
在restric文件夹下，产生了一堆数据，查了资料，它是加密保存的，因此无法显性看出备份PV的数据。接下来我们尝试下恢复能力，就能验证其数据备份能力。<br>
<div class="aw-upload-img-list active">
<a href="http://dockone.io/uploads/article/20211115/3f923e7b5b6d7fab9166dd41f71e3d55.png" target="_blank" data-fancybox-group="thumb" rel="lightbox"><img src="https://cors.zfour.workers.dev/?http://dockone.io/uploads/article/20211115/3f923e7b5b6d7fab9166dd41f71e3d55.png" class="img-polaroid" title="6.png" alt="6.png" referrerpolicy="no-referrer"></a>
</div>
<br>
<h3>使用vVelero恢复应用及其数据</h3>1、删除测试应用及其数据<br>
<pre class="prettyprint"># delete nginx deployment<br>
> kubectl delete deploy carina-deployment-big<br>
<br>
# delete nginx pvc<br>
# 检查PV是否已经释放<br>
> kubectl delete pvc csi-carina-pvc-big<br>
</pre><br>
2、通过velero restore恢复测试应用和数据<br>
<pre class="prettyprint"># 恢复<br>
> velero restore create --from-backup nginx-pv-backup<br>
Restore request "nginx-pv-backup-20211109094323" submitted successfully.<br>
Run `velero restore describe nginx-pv-backup-20211109094323` or `velero restore logs nginx-pv-backup-20211109094323` for more details.<br>
root@u20-m1:/home/legendarilylwq# velero restore describe nginx-pv-backup-20211109094323<br>
I1109 09:43:50.028122 1534182 request.go:655] Throttling request took 1.161022334s, request: GET:<https://10.140.0.8:6443/apis/networking.k8s.io/v1?timeout=32s><br>
Name:         nginx-pv-backup-20211109094323<br>
Namespace:    velero<br>
Labels:       <none><br>
Annotations:  <none><br>
<br>
Phase:                                 InProgress<br>
Estimated total items to be restored:  9<br>
Items restored so far:                 9<br>
<br>
Started:    2021-11-09 09:43:23 +0000 UTC<br>
Completed:  <n/a><br>
<br>
Backup:  nginx-pv-backup<br>
<br>
Namespaces:<br>
Included:  all namespaces found in the backup<br>
Excluded:  <none><br>
<br>
Resources:<br>
Included:        *<br>
Excluded:        nodes, events, events.events.k8s.io, backups.velero.io, restores.velero.io, resticrepositories.velero.io<br>
Cluster-scoped:  auto<br>
<br>
Namespace mappings:  <none><br>
<br>
Label selector:  <none><br>
<br>
Restore PVs:  auto<br>
<br>
Restic Restores (specify --details for more information):<br>
New:  1<br>
<br>
Preserve Service NodePorts:  auto<br>
<br>
> velero restore get<br>
NAME                             BACKUP            STATUS      STARTED                         COMPLETED                       ERRORS   WARNINGS   CREATED                         SELECTOR<br>
nginx-pv-backup-20211109094323   nginx-pv-backup   Completed   2021-11-09 09:43:23 +0000 UTC   2021-11-09 09:44:14 +0000 UTC   0        2          2021-11-09 09:43:23 +0000 UTC   <none><br>
</pre><br>
3、验证测试应用和数据是否恢复<br>
<pre class="prettyprint"># 查看po、pvc、pv是否自动恢复创建<br>
> kubectl get po<br>
NAME                                    READY   STATUS    RESTARTS   AGE<br>
carina-deployment-big-6b78fb9fd-mwf8g   1/1     Running   0          93s<br>
kubewatch-5ffdb99f79-87qbx              2/2     Running   0          19d<br>
static-pod-u20-w1                       1/1     Running   15         235d<br>
<br>
> kubectl get pvc<br>
NAME                 STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS    AGE<br>
csi-carina-pvc-big   Bound    pvc-e81017c5-0845-4bb1-8483-a31666ad3435   10Gi       RWO            csi-carina-sc   100s<br>
<br>
> kubectl get pv<br>
NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                        STORAGECLASS    REASON   AGE<br>
pvc-a07cac5e-c38b-454d-a004-61bf76be6516   8Gi        RWO            Delete           Bound    minio/minio-storage-pvc      csi-carina-sc            6d17h<br>
pvc-e81017c5-0845-4bb1-8483-a31666ad3435   10Gi       RWO            Delete           Bound    default/csi-carina-pvc-big   csi-carina-sc            103s<br>
<br>
> kubectl get deploy<br>
NAME                    READY   UP-TO-DATE   AVAILABLE   AGE<br>
carina-deployment-big   1/1     1            1           2m20s<br>
kubewatch               1/1     1            1           327d<br>
<br>
# 进入容器，验证自定义页面是否还在<br>
> kubectl exec -ti carina-deployment-big-6b78fb9fd-mwf8g -- /bin/bash<br>
/# cd  /usr/share/nginx/html/<br>
/usr/share/nginx/html# ls -l<br>
total 24<br>
-rw-r--r-- 1 root root    13 Nov  9 05:54 index.html<br>
drwx------ 2 root root 16384 Nov  9 05:31 lost+found<br>
-rw-r--r-- 1 root root    12 Nov  9 05:54 test.html<br>
/usr/share/nginx/html# curl localhost<br>
hello carina<br>
/usr/share/nginx/html# curl localhost/test.html<br>
test carina<br>
</pre><br>
4、仔细分析恢复过程<br>
<br>下面是恢复后的测试应用的详细信息，有个新增的init container，名叫<strong>restric-wait</strong>，它自身使用了一个磁盘挂载。<br>
<pre class="prettyprint">k describe po carina-deployment-big-6b78fb9fd-mwf8g<br>
Name:         carina-deployment-big-6b78fb9fd-mwf8g<br>
Namespace:    default<br>
Priority:     0<br>
Node:         u20-w2/10.140.0.13<br>
Start Time:   Tue, 09 Nov 2021 09:43:47 +0000<br>
Labels:       app=web-server-big<br>
          pod-template-hash=6b78fb9fd<br>
          velero.io/backup-name=nginx-pv-backup<br>
          velero.io/restore-name=nginx-pv-backup-20211109094323<br>
Annotations:  <none><br>
Status:       Running<br>
IP:           10.0.2.227<br>
IPs:<br>
IP:           10.0.2.227<br>
Controlled By:  ReplicaSet/carina-deployment-big-6b78fb9fd<br>
Init Containers:<br>
restic-wait:<br>
Container ID:  containerd://ec0ecdf409cc60790fe160d4fc3ba0639bbb1962840622dc20dcc6ccb10e9b5a<br>
Image:         velero/velero-restic-restore-helper:v1.7.0<br>
Image ID:      docker.io/velero/velero-restic-restore-helper@sha256:6fce885ce23cf15b595b5d3b034d02a6180085524361a15d3486cfda8022fa03<br>
Port:          <none><br>
Host Port:     <none><br>
Command:<br>
  /velero-restic-restore-helper<br>
Args:<br>
  f4ddbfca-e3b4-4104-b36e-626d29e99334<br>
State:          Terminated<br>
  Reason:       Completed<br>
  Exit Code:    0<br>
  Started:      Tue, 09 Nov 2021 09:44:12 +0000<br>
  Finished:     Tue, 09 Nov 2021 09:44:14 +0000<br>
Ready:          True<br>
Restart Count:  0<br>
Limits:<br>
  cpu:     100m<br>
  memory:  128Mi<br>
Requests:<br>
  cpu:     100m<br>
  memory:  128Mi<br>
Environment:<br>
  POD_NAMESPACE:  default (v1:metadata.namespace)<br>
  POD_NAME:       carina-deployment-big-6b78fb9fd-mwf8g (v1:metadata.name)<br>
Mounts:<br>
  /restores/mypvc-big from mypvc-big (rw)<br>
  /var/run/secrets/kubernetes.io/serviceaccount from default-token-kw7rf (ro)<br>
Containers:<br>
web-server:<br>
Container ID:   containerd://f3f49079dcd97ac8f65a92f1c42edf38967a61762b665a5961b4cb6e60d13a24<br>
Image:          nginx:latest<br>
Image ID:       docker.io/library/nginx@sha256:644a70516a26004c97d0d85c7fe1d0c3a67ea8ab7ddf4aff193d9f301670cf36<br>
Port:           <none><br>
Host Port:      <none><br>
State:          Running<br>
  Started:      Tue, 09 Nov 2021 09:44:14 +0000<br>
Ready:          True<br>
Restart Count:  0<br>
Environment:    <none><br>
Mounts:<br>
  /usr/share/nginx/html from mypvc-big (rw)<br>
  /var/run/secrets/kubernetes.io/serviceaccount from default-token-kw7rf (ro)<br>
Conditions:<br>
Type              Status<br>
Initialized       True <br>
Ready             True <br>
ContainersReady   True <br>
PodScheduled      True <br>
Volumes:<br>
mypvc-big:<br>
Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)<br>
ClaimName:  csi-carina-pvc-big<br>
ReadOnly:   false<br>
default-token-kw7rf:<br>
Type:        Secret (a volume populated by a Secret)<br>
SecretName:  default-token-kw7rf<br>
Optional:    false<br>
QoS Class:       Burstable<br>
Node-Selectors:  <none><br>
Tolerations:     node.kubernetes.io/not-ready:NoExecute op=Exists for 300s<br>
             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s<br>
Events:<br>
Type     Reason                  Age    From                     Message<br>
----     ------                  ----   ----                     -------<br>
Warning  FailedScheduling        7m20s  carina-scheduler         pod 6b8cf52c-7440-45be-a117-8f29d1a37f2c is in the cache, so can't be assumed<br>
Warning  FailedScheduling        7m20s  carina-scheduler         pod 6b8cf52c-7440-45be-a117-8f29d1a37f2c is in the cache, so can't be assumed<br>
Normal   Scheduled               7m18s  carina-scheduler         Successfully assigned default/carina-deployment-big-6b78fb9fd-mwf8g to u20-w2<br>
Normal   SuccessfulAttachVolume  7m18s  attachdetach-controller  AttachVolume.Attach succeeded for volume "pvc-e81017c5-0845-4bb1-8483-a31666ad3435"<br>
Normal   Pulling                 6m59s  kubelet                  Pulling image "velero/velero-restic-restore-helper:v1.7.0"<br>
Normal   Pulled                  6m54s  kubelet                  Successfully pulled image "velero/velero-restic-restore-helper:v1.7.0" in 5.580228382s<br>
Normal   Created                 6m54s  kubelet                  Created container restic-wait<br>
Normal   Started                 6m53s  kubelet                  Started container restic-wait<br>
Normal   Pulled                  6m51s  kubelet                  Container image "nginx:latest" already present on machine<br>
Normal   Created                 6m51s  kubelet                  Created container web-server<br>
Normal   Started                 6m51s  kubelet                  Started container web-server<br>
</pre><br>
感兴趣希望深入研究整体恢复应用的过程，可以参考官网链接：<a href="https://velero.io/docs/v1.7/restic/#customize-restore-helper-container" rel="nofollow" target="_blank">https://velero.io/docs/v1.7/re ... ainer</a>，代码实现看这里：<a href="https://github.com/vmware-tanzu/velero/blob/main/pkg/restore/restic_restore_action.go" rel="nofollow" target="_blank">https://github.com/vmware-tanz ... on.go</a>。<br>
<br>以上通过step by step演示了如何使用Carina和Velero实现容器存储自动化管理和数据的快速备份和恢复，这次虽然只演示了部分功能（还有如磁盘读写限速、按需备份恢复等高级功能等待大家去尝试），但从结果可以看出，已实现了有存储应用的快速部署、备份和恢复，给Kubernetes集群异地灾备方案带来了“简单”的选择。我自己也准备把Wordpress博客也用这个方式备份起来。<br>
<br>参考：<a href="https://mp.weixin.qq.com/s/VC6kVfcBCUQfG6RwM6F1QA" rel="nofollow" target="_blank">https://mp.weixin.qq.com/s/VC6kVfcBCUQfG6RwM6F1QA</a><br>
<br>原文链接：<a href="https://davidlovezoe.club/wordpress/archives/1260" rel="nofollow" target="_blank">https://davidlovezoe.club/wordpress/archives/1260</a>
                                                                <div class="aw-upload-img-list">
                                                                                                                                                                                                                                                                                                                                                                                                                                                                </div>
                                
                                                                <ul class="aw-upload-file-list">
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </ul>
                                                              
</div>
            