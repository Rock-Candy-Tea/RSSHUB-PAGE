
---
title: 'ncnn 发布 20210507 版本，int8 量化推理大幅优化超 500%'
categories: 
 - 编程
 - 开源中国
 - 资讯
headimg: 'https://static.oschina.net/uploads/space/2021/0509/075333_o8uO_4252687.png'
author: 开源中国
comments: false
date: Sun, 09 May 2021 07:54:00 GMT
thumbnail: 'https://static.oschina.net/uploads/space/2021/0509/075333_o8uO_4252687.png'
---

<div>   
<div class="content">
                                                                                            <p>ncnn是腾讯开源为手机端极致优化的高性能神经网络前向计算框架。</p> 
<p>仰赖ncnn社区开发者的贡献，ncnn在2019年年初便已实现int8模型量化和推理。但因后来失去社区开发者的持续投入，ncnn的int8量化推理效率迟迟没有加速。</p> 
<p>ncnn github issue区大家关于int8量化后速度的质疑：</p> 
<p><img alt height="444" src="https://static.oschina.net/uploads/space/2021/0509/075333_o8uO_4252687.png" width="300" referrerpolicy="no-referrer"></p> 
<p>引用zhihu用户John Hexa关于《如何看待国内开源项目的不可持续性？》的一段回答：</p> 
<p><em>“开源的生命力并不在于“开源”，而在于为人类好好的、真正的解决掉一个问题。</em></p> 
<p><em>不是自己提供一个半成品，指望别人完成成品。</em></p> 
<p><em>而是自己要提供一个成品，让别人可以完成更好的成品。</em></p> 
<p><em>不是自己提出一个问题，指望别人给你答案。</em></p> 
<p><em>而是自己给出一个答案，让别人可以找到更好的答案。”</em></p> 
<p>本以为，ncnn的开发者社区足够活跃，可以等到一位愿意为ncnn优化加速int8计算的贡献者。</p> 
<p><strong>这确实是我错了。</strong></p> 
<p>本次20210507版本，ncnn的int8量化工具和整个int8推理计算架构被进行了彻底重构，作者删除了老旧的kernel实现，亲自写了大量arm neon汇编，4w+行代码，用上armv8.2 dot指令加速。最终，在int8量化和推理加速上，ncnn提供了一个成品，给出了一个答案。</p> 
<p>ncnn 20210507版本下载地址(linux/windows/macos/android/ios/webassembly，cpu+gpu)</p> 
<p><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2FTencent%2Fncnn%2Freleases%2Ftag%2F20210507" target="_blank">https://github.com/Tencent/ncnn/releases/tag/20210507</a></p> 
<p><strong>继续保持优秀的接口稳定性和兼容性</strong></p> 
<ul> 
 <li> <p>API接口完全不变</p> </li> 
 <li> <p>量化校准table完全不变</p> </li> 
 <li> <p>int8模型量化流程完全不变</p> </li> 
</ul> 
<p><strong>ncnn int8量化工具(ncnn2table)新特性</strong></p> 
<ul> 
 <li> <p>支持 kl aciq easyquant 三种量化策略</p> </li> 
 <li> <p>支持多输入的模型量化</p> </li> 
 <li> <p>支持RGB/RGBA/BGR/BGRA/GRAY输入的模型量化</p> </li> 
 <li> <p>大幅改善多线程效率</p> </li> 
 <li> <p>离线进行(反量化-激活-量化)->(requantize)融合，实现端到端int8量化推理</p> </li> 
</ul> 
<p><strong>ncnn int8量化推理新特性</strong></p> 
<ul> 
 <li> <p>conv/convdw/fc 量化推理支持附带任意激活层</p> </li> 
 <li> <p>int8特征数据自动转换为elempack=8内存布局，提高访存效率</p> </li> 
 <li> <p>实现全部pack1/pack1to4/pack4/pack8to4等的int8 sgemm kernel优化</p> </li> 
 <li> <p>实现int8 winograd-f43的kernel优化</p> </li> 
 <li> <p>运行时检测armv8.2 dot指令支持，并调用优化的kernel</p> </li> 
 <li> <p>启用fp16/bf16的情况下，遇到非conv/convdw/fc层，自动回退到fp16/bf16而不是fp32计算</p> </li> 
</ul> 
<p><strong>ncnn 20210507版本的其他更新</strong></p> 
<ul> 
 <li> <p>数学函数 log/exp/sin/cos/tanh 的 risc-v v 扩展指令优化</p> </li> 
 <li> <p>由社区贡献者贡献的 ncnn x86 卷积优化和 FAQ 文档</p> </li> 
 <li> <p>改善模型转换器兼容性，fp16和vulkan运算bugfix，等等</p> </li> 
</ul> 
<p>mobilenet int8模型，ncnn自带的benchncnn评测工具，测试手机：oneplus 7t(qcom855+）</p> 
<p><img alt height="242" src="https://static.oschina.net/uploads/space/2021/0509/075158_zc1V_4252687.png" width="700" referrerpolicy="no-referrer"></p> 
<p><img alt height="634" src="https://static.oschina.net/uploads/space/2021/0509/075213_AEVp_4252687.png" width="700" referrerpolicy="no-referrer"><img height="1" src="https://static.oschina.net/uploads/space/2021/0509/075115_GzYb_4252687.gif" width="1" referrerpolicy="no-referrer"></p>
                                        </div>
                                      
</div>
            