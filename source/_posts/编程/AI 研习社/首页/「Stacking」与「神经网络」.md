
---
title: '「Stacking」与「神经网络」'
categories: 
    - 编程
    - AI 研习社
    - 首页

author: AI 研习社
comments: false
date: Wed, 07 Nov 2018 06:33:12 GMT
thumbnail: 'https://static.leiphone.com/uploads/new/sns/article/201811/1541572195122190.jpg'
---

<div>   
<blockquote><p>本文原载于微调的知乎专栏<a href="https://www.zhihu.com/people/breaknever/posts">「数据说」</a>。</p></blockquote><p>* 本文偏理论，需要较多背景知识，请选择性阅读。</p><p>* 文末提供了一些开放的研究方向，欢迎讨论与合作研(灌)究(水)。</p><hr><p>Stacking是Kaggle比赛中常见的集成学习框架。一般来说，就是训练一个多层(一般是两层，本文中默认两层)的学习器结构，第一层(也叫学习层)用n个不同的分类器(或者参数不同的模型)将得到预测结果合并为新的特征集，并作为下一层分类器的输入。一个简单的示意图如下：</p><p style="text-align: center;"><img src="https://static.leiphone.com/uploads/new/sns/article/201811/1541572195122190.jpg" title="1541572195122190.jpg" alt="1.jpg" referrerpolicy="no-referrer"></p><p style="text-align: center;"><span style="font-size: 12px; color: rgb(127, 127, 127);">图片来源：https://www.quora.com/What-is-stacking-in-machine-learning</span></p><p>给出一个比较简单的例子：你想要预测明天是否会下雨，并有相关的气象数据。你训练了十个分类器比如逻辑回归，svm，knn等。你通过stacking的方法在第一层将这十个分类器的结果作为了第二层训练器的数据，通过第二层的输出训练器得到了最终预测结果。Stacking本身不是文章的讨论重点，因此建议读者可以阅读以下科普文章了解什么是stacking：</p><ul class=" list-paddingleft-2"><li><p><a href="https://zhuanlan.zhihu.com/p/27493821">Kris Zhao：【干货】比赛后期大招之stacking技术分享</a></p></li><li><p><a href="https://zhuanlan.zhihu.com/p/29418981">峰峰：详解Stacking</a></p></li><li><p><a href="http://link.zhihu.com/?target=http%3A//www.cnblogs.com/yucaodie/p/7044737.html">Stacked Regression的详细步骤和使用注意事项</a></p></li></ul><p>本文的核心观点是提供一种对于stacking的理解，即与神经网络对照来看。当然，在<a href="https://www.zhihu.com/question/59769987/answer/269367049">阿萨姆：为什么做stacking之后，准确率反而降低了？</a>中我已经说过stacking不是万能药，但往往很有效。通过与神经网络的对比，读者可以从另一个角度加深对stacking的理解。</p><p>1. Stacking是一种表示学习(representation learning)</p><p>表示学习指的是模型从原始数据中自动抽取有效特征的过程，比如<span class="k_pendant" data-id="5">深度学习</span>就是一种表示学习的方法。关于表示学习的理解可以参考：<a href="https://www.zhihu.com/question/264417928/answer/283087276">阿萨姆：人工智能（AI）是如何处理数据的？</a></p><p>原始数据可能是杂乱无规律的。在stacking中，通过第一层的多个学习器后，有效的特征被学习出来了。从这个角度来看，stacking的第一层就是特征抽取的过程。在[1]的研究中，上排是未经stacking的数据，下排是经过stacking(多个无监督学习<span class="k_pendant" data-id="1324">算法</span>)处理后的数据，我们显著的发现红色和蓝色的数据在下排中分界更为明显。<em>* 数据经过了压缩处理</em>。这个小例子说明了，有效的stacking可以对原始数据中的特征有效的抽取。</p><p style="text-align: center;"><img src="https://static.leiphone.com/uploads/new/sns/article/201811/1541572262977140.jpg" title="1541572262977140.jpg" alt="2.jpg" referrerpolicy="no-referrer"></p><p><br></p><p>2. Stacking和神经网络从某种角度看有异曲同工之妙，神经网络也可以被看作是集成学习</p><p>承接上一点，stacking的学习能力主要来自于对于特征的表示学习，这和神经网络的思路是一致的。这也是为什么我说“第一层”，“最后一层”。</p><p>而且神经网络也可以被看做是一种集成学习，主要取决于不同神经元、层对于不同特征的理解不同。从浅层到深层可以理解为一种从具体到抽象的过程。</p><p>Stacking中的第一层可以等价于神经网络中的前 n-1层，而stacking中的最终分类层可以类比于神经网络中最后的输出层。不同点在于，stacking中不同的分类器通过异质来体现对于不同特征的表示，神经网络是从同质到异质的过程且有分布式表示的特点(distributed representation)。Stacking中应该也有分布式的特点，主要表现在多个分类器的结果并非完全不同，而有很大程度的相同之处。</p><p>但同时这也提出了一个挑战，多个分类器应该尽量在保证效果好的同时尽量不同，stacking集成学习框架的对于基分类器的两个要求：</p><ul class=" list-paddingleft-2"><li><p>差异化(diversity)要大</p></li><li><p>准确性(accuracy)要高</p></li></ul><p>3. Stacking的输出层为什么用逻辑回归？</p><p>如果你看懂了上面的两点，你应该可以理解stacking的有效性主要来自于特征抽取。而表示学习中，如影随形的问题就是过拟合，试回想深度学习中的过拟合问题。</p><p>在[3]中，周志华教授也重申了stacking在使用中的过拟合问题。因为第二层的特征来自于对于第一层数据的学习，那么第二层数据中的特征中不该包括原始特征，以降低过拟合的风险。举例：</p><ul class=" list-paddingleft-2"><li><p>第二层数据特征：仅包含学习到的特征</p></li><li><p>第二层数据特征：包含学习到的特征 + 原始特征</p></li></ul><p>另一个例子是，stacking中一般都用交叉验证来避免过拟合，足可见这个问题的严重性。</p><p>为了降低过拟合的问题，第二层分类器应该是较为简单的分类器，广义线性如逻辑回归是一个不错的选择。在特征提取的过程中，我们已经使用了复杂的非线性变换，因此在输出层不需要复杂的分类器。这一点可以对比神经网络的激活函数或者输出层，都是很简单的函数，一点原因就是不需要复杂函数并能控制复杂度。</p><p>因此，stacking的输出层不需要过分复杂的函数，用逻辑回归还有额外的好处：</p><ul class=" list-paddingleft-2"><li><p>配合L1正则化还可以进一步防止过拟合</p></li><li><p>配合L1正则化还可以选择有效特征，从第一层的学习器中删除不必要的分类器，节省运算开销。</p></li><li><p>逻辑回归的输出结果还可被理解为概率</p></li></ul><p>4. Stacking是否需要多层？第一层的分类器是否越多越好？</p><p>通过以上分析，stacking的表示学习不是来自于多层堆叠的效果，而是来自于不同学习器对于不同特征的学习能力，并有效的结合起来。一般来看，2层对于stacking足够了。多层的stacking会面临更加复杂的过拟合问题，且收益有限。</p><p>第一层分类器的数量对于特征学习应该有所帮助，经验角度看越多的基分类器越好。即使有所重复和高依赖性，我们依然可以通过特征选择来处理，问题不大。</p><p>这提出了另一个stacking与深度学习不同之处:</p><ul class=" list-paddingleft-2"><li><p>stacking需要宽度，深度学习不需要</p></li><li><p>深度学习需要深度，而stacking不需要</p></li></ul><p>但stacking和深度学习都共同需要面临：</p><ul class=" list-paddingleft-2"><li><p>黑箱与解释问题</p></li><li><p>严重的过拟合问题</p></li></ul><p>如果stacking和深度学习都是一种表示学习，如何选择？我认为和样本量有关：</p><ul class=" list-paddingleft-2"><li><p>小样本上深度学习不具备可操作性，stacking或许可以</p></li><li><p>大样本上stacking的效率理当不如深度学习，这也是一种取舍</p></li></ul><p>5. 现阶段我正在研究的问题：</p><ul class=" list-paddingleft-2"><li><p>stacking中用逻辑回归，或者MLR一直是被证明比较好的方法 [4]。但跳出这个框架，新的对于过拟合较为鲁棒的boosting模型是否可以替代？</p></li><li><p>stacking中的过拟合是因为在第一层使用监督学习，与第二层的监督学习目标相同。如果第一层使用无监督学习，那么是否可以有效的降低过拟合问题？同理，是否可以用stacking的框架做半监督学习？</p></li><li><p>stacking往往面临很大的运算开销，在预测时需要运行多个模型，是否可以用更有效的特征选择来控制运算开销。比如仅选择分类器x，y，z构成的特征空间？</p></li><li><p>stacking中第二层的特征空间由第一层分类器决定，如何有效的压缩这个特征空间？</p></li></ul><p>Stacking相关的文献和研究都还有限，得到广泛关注的有[2, 4, 5, 6, 7, 8]。本文提供了一个不同的角度，属于个人观点，请批判的看。欢迎评论、私信、以及合作研(灌)究(水)。</p><hr><p>[1] Micenkova, B., McWilliams, B. and Assent, I., 2014. Learning Representations for Outlier Detection on a Budget. arXiv preprint. <em>arXiv preprint arXiv:1507.08104</em>.</p><p>[2] Seewald, A.K., 2002, July. How to make stacking better and faster while also taking care of an unknown weakness. In <em>Proceedings of the nineteenth international conference on machine learning</em> (pp. 554-561). Morgan Kaufmann Publishers Inc.</p><p>[3] Zhou, Z.H., 2012. <em>Ensemble methods: foundations and algorithms</em>. CRC press.</p><p>[4] Džeroski, S. and Ženko, B., 2004. Is combining classifiers with stacking better than selecting the best one?. <em>Machine learning</em>, <em>54</em>(3), pp.255-273.</p><p>[5] Van der Laan, M.J., Polley, E.C. and Hubbard, A.E., 2007. Super learner. <em>Statistical applications in genetics and molecular biology</em>, <em>6</em>(1).</p><p>[6] Wolpert, D.H., 1992. Stacked generalization. <em>Neural networks</em>, <em>5</em>(2), pp.241-259.</p><p>[7] Breiman, L., 1996. Stacked regressions. <em>Machine learning</em>, <em>24</em>(1), pp.49-64.</p><p>[8] Ting, K.M. and Witten, I.H., 1999. Issues in stacked generalization. <em>J. Artif. Intell. Res.(JAIR)</em>, <em>10</em>, pp.271-289.</p><p style="text-align: center;"><img src="https://static.leiphone.com/uploads/new/sns/article/201809/1538126929964269.png" alt="asamu.png" referrerpolicy="no-referrer"></p>  
</div>
            