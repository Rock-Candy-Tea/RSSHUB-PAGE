
---
title: '网易互娱AI Lab技术主管：那些被强化学习解决的痛点'
categories: 
 - 新媒体
 - 游戏葡萄
 - 文章
headimg: 'https://cors.zfour.workers.dev/?http://cdn.youxiputao.com/attach/news/2022/04/21/1650515854556211.png'
author: 游戏葡萄
comments: false
date: Wed, 20 Apr 2022 04:40:47 GMT
thumbnail: 'https://cors.zfour.workers.dev/?http://cdn.youxiputao.com/attach/news/2022/04/21/1650515854556211.png'
---

<div>   
<p>2022N.GAME网易游戏开发者峰会于「4月18日-4月21日」举办，在技术驱动场中，网易互娱AI Lab的技术主管曾广俊分享了强化学习在互娱游戏中的落地。</p><p style="text-align:center;"><img class="rich_pages wxw-img" src="https://cors.zfour.workers.dev/?http://cdn.youxiputao.com/attach/news/2022/04/21/1650515854556211.png" style="margin:0px;padding:0px;height:auto;vertical-align:bottom;width:578px;" alt="1650515854556211.png" referrerpolicy="no-referrer"></p><p>以下为曾广俊的分享内容，为方便阅读，有部分删减与调整：</p><p>互娱AI Lab成立于2017年底，主要目标是将3D、CV、RL、NLP和语音技术落地到游戏当中，解决游戏的痛点。今天我们来讲一下强化学习是什么、可以在游戏里做些什么，以及在其他领域能如何应用。</p><p><br></p><p></p><h2><span style="color:rgb(0,176,80);">01 强化学习是什么？</span></h2><p>假如现在我们要训练一条小狗做出坐下的动作，我们可能会做出一个指令让它坐下。如果小狗做对了，我们就会奖励它一个食物；如果没做到就不会给。通过这样的多次反馈、迭代，小狗最终会知道——要做出坐下这个动作才能获得奖励。</p><p style="text-align:center;"><img class="rich_pages wxw-img" src="https://cors.zfour.workers.dev/?http://cdn.youxiputao.com/attach/news/2022/04/21/1650515854724433.png" style="margin:0px;padding:0px;height:auto;vertical-align:bottom;width:578px;" alt="1650515854724433.png" referrerpolicy="no-referrer"></p><p>同样，我们会把状态信息发送给AI，让AI做出动作，由游戏反馈这个动作是否会有奖惩，AI得到反馈后就会据此调整它的策略。</p><p>通过多轮迭代，它就会知道在某个时刻要做什么动作才能获得最大回报。由于我们需要和环境大量交互，所以强化学习通常需要很多时间让机器探索。通过缩小探索空间、提高样本利用率来提高强化学习的训练效果，是一个重要的方向。</p><p><br></p><p></p><h2><span style="color:rgb(0,176,80);">02 强化学习可以在游戏里面做些什么？</span></h2><p>游戏要接入强化学习非常简单，只要实现两个接口：reset接口和step接口。reset接口会从游戏中返回初始状态；step接口会从AI中获取对应动作。之后游戏环境会返回下一个状态的信息，和一些回报的奖励信息。最后把这个游戏打包成动态库或者docker，就可以交给强化学习AI训练。</p><p style="text-align:center;"><img class="rich_pages wxw-img" src="https://cors.zfour.workers.dev/?http://cdn.youxiputao.com/attach/news/2022/04/21/1650515855118068.png" style="margin:0px;padding:0px;height:auto;vertical-align:bottom;width:578px;" alt="1650515855118068.png" referrerpolicy="no-referrer"></p><p><span style="color:rgb(0,176,80);"><strong>1、强化学习的主要应用：对战型AI</strong></span></p><p>强化学习最主要的应用是对战型AI。在互娱游戏中我们落地了NPC的对战AI。先以《天下3》的山海绘——一个人机对战的卡牌游戏为例，玩家可以选多个难度，跟机器人进行对战。策划需要在短时间内完成人机对战AI，这个AI要适应多种难度，且难度要能动态调整，以适应玩家的水平。</p><p>如果我们用行为树做山海绘的AI，会需要策划用很多时间去列举每个状态的信息，然后根据状态节点做对应动作。如果要做分级难度，就更需要细致地划分，这样花费的时间将会更加巨大，上线后每一次更新，都需要策划重新花费大量时间修改行为树。</p><p>更关键的是，行为树的AI强度通常都达不到一般玩家的水平。如果用强化学习去做，我们就可以很快速地生成AI，尤其是当新卡更新到新的游戏环境中，强化学习可以很快地适应新环境，只需要重新再Finetune一下就可以了。</p><p>强化学习训练本身，是在做机器人的自对弈行为。在这样的过程中，会自然而然地批量生成大量不同难度的AI。这些AI可以无缝迁移，满足玩家对于难度选择的需求。最关键的一点是，它最后的强度能远超玩家水平。</p><p>我们的强化学习训练和一般的强化学习类似，主要由采样器、训练器组成。采样器在CPU上执行AI决策，通过与游戏环境交互生成大量样本。这些样本又可以送到GPU的训练器上优化，优化出来的模型再放到模型池里，让AI选择对手。</p><p>通过迭代模型池强度，当前训练的AI也会逐渐变强。且模型池的模型也可以作为不同难度的AI模型让玩家选择。这个AI的难点主要在动作空间上，刚才提到训练强化学习其实是试错的过程，如果我们可选的动作太多，找到合适的动作就需要很长时间。</p><p>比如我要出AABCCD这样一个动作，打第一张牌就会有几十种选择，第二张牌也一样。这么多选择组合起来，树型结构会使动作空间成指数级增长。</p><p>我们的解决方案是把单步决策变成序列决策。也就是说，我们从游戏环境中得到状态，让AI决策第一张牌出什么，再把第一张牌和环境状态输入AI，再做一次决策，之后的决策以此类推。</p><p>最后，我们就可以输出一个持续的决策，再统一返回到游戏环境中。这样就可以把一步决策化为多步决策，把游戏空间从指数级降到常数级。</p><p>我们对比一下强化学习跟行为树的AI：在加入新卡牌之后，强化学习需要的时间明显会比行为树少很多。且行为树只有3-5个等级，而强化学习具有100多个较为平滑的难度等级。另外，行为树还可能存在较大的难度跳跃，而强化学习能做到远高于玩家的水平。</p><p style="text-align:center;"><img class="rich_pages wxw-img" src="https://cors.zfour.workers.dev/?http://cdn.youxiputao.com/attach/news/2022/04/21/1650515855306914.png" style="margin:0px;padding:0px;height:auto;vertical-align:bottom;width:578px;" alt="1650515855306914.png" referrerpolicy="no-referrer"></p><p>我们也挑战了更难的游戏，参加了NeurIPS举办的MineRL Diamond比赛,这个比赛已经举办三届，每次都吸引了大量工业界、学术界的强队参加。其目的就是在《MineCraft》中，从砍树造工具开局，一直到实现挖掘钻石的目标。</p><p>比赛举办以来，基本上没有队伍能在这种环境中挖到钻石。大多数队伍都选择了基于玩家数据训练，比如模仿学习，或在模仿学习组上再进行强化学习。</p><p>但官方提供的数据其实不多，玩家的水平也参差不齐，所以其中会有很多无效操作。我们也尝试过用官方数据集训练，实际效果并不好。那我们能不能直接用强化学习从零开始训练呢？</p><p>可以，但要解决几个难点：环境输出以图像信息为主，而图像信息是3x64x64的图片，其信息维度是非常大的。要AI去遍历这么一个大空间的数据会非常困难，因此我们采用了CNN网络，以尽量降低复杂度、提出一些关键特征。</p><p>另外，这个比赛需要AI有长期规划能力。比如它需要先从产木头开始，产够了木头才能去做木镐挖石头，挖到石头后制造石搞去挖铁……要做出这样一环扣一环的操作，需要AI在每个时刻都能知道自己的策略和下一步需要干什么。而在这么长的一段时间里，让AI盲目探索、直接训练，也会面临巨大挑战。我们做的主要工作，就是缩减它的探索空间。</p><p>首先是动作编码，我们会把动作精简到只有20个，并根据当前局面屏蔽一些不需要的动作。这样一来，AI每次可选的动作就会非常少；</p><p>其次，跳帧也是一个关键点。通过跳帧，我们可以把本来很长的游戏压缩成得较短，也让AI需要做的决策数目大大缩小。这就让我们能在较短时间内训练出较好的效果。</p><p>更重要的是，要设置合理的奖励。原始的奖励方法，是在第一次采集资源获得奖励之后不能再获得奖励。这样的话，我们的AI可能就学不到要重复收集足够资源这一点。但如果每次都有奖励，AI又可能会学出刷分行为。</p><p>所以我们细致地调整了它的奖励方法。比如一开始会需要较多木头，后面就用不到了。所以我们一开始会给出重复奖励，到一定程度后才停止。另外，挖钻石的行为其实跟挖矿、挖石头的操作类似，都要不断挖掘。我们要鼓励这样的操作，所以挖石头、铁矿我们都会给出无限奖励。</p><p>通过这样的策略，可以看到随着AI迭代，它的累积回报和挖到钻石的概率都在飞快上升。最后，我们以历史最高分获得了冠军，也是比赛举办以来第一次有队伍挖到钻石。</p><p><span style="color:rgb(0,176,80);"><strong>2、用强化学习进行辅助游戏设计</strong></span></p><p>在辅助游戏设计方面，我们也利用强化学习做出了探索。比如竞速游戏需要测试赛车的圈速、漂移性能，赛道的通过性、弯道的难度……人工验证要花费很多时间。首先要用若干天让测试者熟悉游戏、掌握技巧，其次还要把每辆验证赛车和每条赛道的组合都跑一遍。</p><p style="text-align:center;"><img class="rich_pages wxw-img" src="https://cors.zfour.workers.dev/?http://cdn.youxiputao.com/attach/news/2022/04/21/1650515856151210.png" style="margin:0px;padding:0px;height:auto;vertical-align:bottom;width:578px;" alt="1650515856151210.png" referrerpolicy="no-referrer"></p><p>即便如此做了，在策划重新设计赛车或者赛道后，我们还需要人工重新适应新特性。另外人工验证也会存在偏差，因为不可能保证每次测试都是人类最高水平，而且人固有的操作习惯也会影响评测。</p><p>强化学习的一个重点，就是要加速AI训练。做到这点，我们才能更好地适应新配置，在更短时间内完成跑测。因此，我们的主要工作就是把一些不合理的动作屏蔽掉，减少探索空间、动态地提前结束游戏。</p><p>把相似的赛道同时训练，也有助于让AI学习它们之间的联系，加速收敛过程，在CPU机器上快速输出结果——即便是在CPU机器上训练，我们也可以缩短90%的测试时间。利用AI可以在同一个赛道同时输出多辆赛车，观察它在每一个时刻的位置、速度、档位，还有赛车的引擎信息，方便策划调试。</p><p><span style="color:rgb(0,176,80);"><strong>3、用强化学习进行游戏的平衡性分析</strong></span></p><p>除了验证竞速游戏以外，我们还可以做一些游戏的平衡性分析。比如在策略游戏中，想要事前分析英雄组合的平衡性，用人工验证可能会出现遗漏，准确率较低；用程序模拟组合，结果会非常准确，但因为组合数非常多，通常需要数月甚至不可接受的时间。</p><p>用强化学习则可以得到折中的时间。我们并不会枚举所有组合，而是通过启发式搜索找到一些可能较强的组合。因为AI并没有人因为先验知识产生的一些偏见，所以AI的结果会更准确。</p><p style="text-align:center;"><img class="rich_pages wxw-img" src="https://cors.zfour.workers.dev/?http://cdn.youxiputao.com/attach/news/2022/04/21/1650515856518800.png" style="margin:0px;padding:0px;height:auto;vertical-align:bottom;width:578px;" alt="1650515856518800.png" referrerpolicy="no-referrer"></p><p>强化学习的训练都离不开环境，在游戏平衡性分析的场景中，我们用模型池和游戏模拟器组成环境——每次AI从游戏环境中获取到当前要对战的阵容，再搜索出组合返回到战斗模拟器模拟对战，最后把赛果返回到AI。</p><p>通过这样的反馈，AI就能了解一个阵容是否合理，在多轮迭代之后，AI会学会如何搭配阵容才能打败对面组合。我们的阵容池也会收纳组合，并逐渐淘汰弱组合、留下强组合。</p><p>这样一来，阵容池里会留下一大批可能潜在过强的英雄组合，我们把这样的流程搭成一个自助跑测平台，策划只需要上传更新的游戏属性文件，点一下运行就可以直接跑出来所需结果。包括每个阵容的实际对战结果、每个英雄的实际强度与对比，还有阵容中每个英雄的出场率，都可以验证其强度是否符合预期。</p><p><br></p><p></p><h2><span style="color:rgb(0,176,80);">03 强化学习在其他领域的应用</span></h2><p>前面说了很多强化学习在互娱游戏里的落地，其实我们也可以观察到在国外有一些公司用强化学习和对抗学习生成游戏关卡。另外，强化学习不止在游戏里有应用，也在自动驾驶和机器人的控制方面有大量应用。</p><p>最近，Deepmind还提出了用强化学习控制核聚变反应堆。我相信这样的技术，可以促进最终核聚变的应用。</p><p><br></p><blockquote><p>网易游戏2022N.GAME峰会将于4月18日-21日每天下午三点直播，点击下方链接即可查看。<br></p><p><a href="https://game.academy.163.com/event/nGame">https://game.academy.163.com/event/nGame</a></p></blockquote><p><br></p>
                      
</div>
            