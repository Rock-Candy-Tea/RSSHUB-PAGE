
---
title: '打开_黑箱_，我们能得到更_懂事_的AI大模型吗？'
categories: 
 - 新媒体
 - 虎嗅
 - 首页资讯
headimg: 'https://img.huxiucdn.com/article/content/202202/01/174005667342.gif'
author: 虎嗅
comments: false
date: Tue, 01 Feb 2022 10:10:00 GMT
thumbnail: 'https://img.huxiucdn.com/article/content/202202/01/174005667342.gif'
---

<div>   
<p><span class="text-remarks" label="备注">本文来自微信公众号：</span><a href="https://mp.weixin.qq.com/s/N1G9vy7C9l-khjL0m3blaQ" target="_blank" rel="nofollow" style="text-decoration: none;"><span class="text-remarks">硅星人（ID：guixingren123）</span></a><span class="text-remarks">，作者：杜晨，编辑：VickyXiao，原文标题：《OpenAI 拾回初心？总爱乱讲话的GPT-3终于懂事了》，头图来自：视觉中国</span></p><p>读者朋友们应该对 GPT-3 完全不陌生了：它是由硅谷顶级 AI 基础研究机构 OpenAI 推出的超大规模语言生成模型，“-3” 也表示它已经是这个 GPT 系列的第三代了。它的训练参数量超过了1750亿，在当时惊为天人。<br></p><p>虽然谷歌和智源等机构也在后来发布了各自的万亿参数量超大模型，GPT-3 仍然在大模型的领域占有一席之地——关键原因之一，就在于 GPT-3 已经被开发成了 OpenAI API，广泛投入到了商业使用，被微软等一众大公司所采用。</p><p>GPT-3 的能力非常强，被称为<strong>“万能生成器”</strong>，不仅限于语言，甚至还能生成数学公式、Excel 表格函数、回答问题、作诗、解数学题、翻译代码等等——此前，我们在<a href="https://mp.weixin.qq.com/s?__biz=MzI3ODg4ODEwMA==&mid=2247498829&idx=1&sn=577b2ff570a2eef46190c91e97ab31a0&chksm=eb529786dc251e906ca5105a236b2e011f76b113e3c3a301e19de83f7ea614df542bf9fb0f5f&scene=21#wechat_redirect" target="_blank" rel="nofollow">这篇文章</a>里曾经介绍过，GPT-3 的能力有多么的强大。 </p><p class="img-center-box"><img class="lazyImg" _src="https://img.huxiucdn.com/article/content/202202/01/174005667342.gif" data-w="480" data-h="301" src="https://img.huxiucdn.com/article/content/202202/01/174005667342.gif" referrerpolicy="no-referrer"></p><p label="图片备注" class="text-img-note">这个小工具的背后就是 GPT-3，可以10秒钟生成一个谷歌首页<br></p><p>然而，自从诞生以来，<strong>GPT-3 一直伴随着巨大的争议。</strong>比如，一些来自顶级学府的调查论文发现，以 GPT 系列为代表的一些生成模型，其生成的结果通常包含基于性别和族裔的偏见。硅星人还曾独家报道过，因为意见不合、对组织的研究方向不满等，一些 OpenAI 前核心员工在2020年底集体离职，创办了新的研究机构 Anthropic。</p><p>OpenAI 想要用 GPT-3/OpenAI API 大赚特赚，这完全可以理解，毕竟现在的 OpenAI 早已不是纯粹的研究机构，而是有着研究和商业混合的双重身份。但不管怎样，<strong>它都需要尽快妥善解决生成类神经网络模型“不听话”“不可解释”“体现甚至放大训练数据当中偏见”等各种各样的问题……</strong></p><p>过去的一年里，OpenAI 也确实是这样做的。</p><h3 label="大标题" class="text-big-title">InstructGPT：更听话、更安全的语言模型</h3><p>最近，该机构终于发布了最新进展：一个改良版的，更“听话”也更“安全”的 GPT-3——InstructGPT。</p><p>“我们成功训练出了<strong>在遵守用户意图方面比 GPT-3 显著更强</strong>的新语言模型，并且同时确保这些模型更加诚实，减少了有害结果的生成。具体来说，我们采用了在对齐<span class="text-remarks" label="备注">（alignment）</span>研究当中掌握的技术，使得这些训练结果成为可能。”OpenAI 表示。</p><p>新的模型名为 InstructGPT<span class="text-remarks" label="备注">（instruct 是指导的意思）</span>，意即和一般模型训练的自我监督模式不同，这次在新模型的训练当中，<strong>OpenAI 重度使用了人类作为“教师”的身份，</strong>对模型训练进行反馈和指导。</p><p>这次的 InstructGPT 模型，可以说是“原版” GPT-3 基础之上的“加强版”。</p><p>之前的 OpenAI API 采用的是“原版” GPT-3 模型。然而在完成任务的时候，有时候会生成不诚实、有害的内容，或者反映某些不健康的情绪。</p><p>OpenAI 指出，这是因为原版 GPT-3 的训练语料数据来自全网，并且模型的设计功能就是根据现有单词预测下一单词，<strong>它的任务不是“根据用户的需要，安全地完成语言任务”。</strong>也即，原版的 GPT-3 模型并没有和用户“对齐”<span class="text-remarks" label="备注">（align）</span>。</p><p>在新模型的训练中，OpenAI 采用了一种已经存在的训练技巧，从人类反馈中进行强化学习 <span class="text-remarks" label="备注">（reinforcement learning from human feedback，简称 RLHF）</span>。</p><p>首先，OpenAI API 的用户对 GPT-3 发出了各种各样的提问<span class="text-remarks" label="备注">（prompt）</span>；OpenAI 找了40个人作为数据标记员，根据这些用户提问生成理想答案；然后，OpenAI 再用这些数据对 GPT-3 进行优化微调，设计出新的激励模型；数据标记员对不同 GPT-3 模型版本生成的结果进行打分：</p><p class="img-center-box"><img class="lazyImg" _src="https://img.huxiucdn.com/article/content/202202/01/174007041253.png?imageView2/2/w/1000/format/png/interlace/1/q/85" data-w="999" data-h="620" src="https://img.huxiucdn.com/article/content/202202/01/174007041253.png?imageView2/2/w/1000/format/png/interlace/1/q/85" referrerpolicy="no-referrer"></p><p>结果令人惊讶：采用这种方法训练的 InstructGPT，<strong>生成内容的质量在任何参数量级上都显著优于 GPT-3，且质量稳定性基本上不受到参数量的制约。</strong></p><p>OpenAI 公开的 InstructGPT 版本实际上只用了13亿参数量，不及原版 GPT-3 的十分之一——然而，OpenAI 的数据标记员认为，在七成的问答当中，InstructGPT 生成的结果显著优于 GPT-3：</p><p class="img-center-box"><img class="lazyImg" _src="https://img.huxiucdn.com/article/content/202202/01/174008192249.png?imageView2/2/w/1000/format/png/interlace/1/q/85" data-w="817" data-h="439" src="https://img.huxiucdn.com/article/content/202202/01/174008192249.png?imageView2/2/w/1000/format/png/interlace/1/q/85" referrerpolicy="no-referrer"></p><p>比如，InstructGPT 比 GPT-3 更能够服从提问者的命令，给出的回答更加接近用户需求。</p><p>以下图为例，提问“为什么鸟类冬天会迁徙到南方”，GPT-3回答“因为天气变冷并且食物稀少”<span class="text-remarks" label="备注">（语境不完整并带有歧义）</span>，InstructGPT回答“因为那里更暖和”<span class="text-remarks" label="备注">（正确的答案且更为简单）</span>。</p><p class="img-center-box"><img class="lazyImg" _src="https://img.huxiucdn.com/article/content/202202/01/174009689759.png?imageView2/2/w/1000/format/png/interlace/1/q/85" data-w="907" data-h="302" src="https://img.huxiucdn.com/article/content/202202/01/174009689759.png?imageView2/2/w/1000/format/png/interlace/1/q/85" referrerpolicy="no-referrer"></p><p>此外，GPT-3 时常出现的“捏造事实”的行为，在 InstructGPT 上也较少出现；以及，<strong>新模型生成有害内容的比例也比原版 GPT-3 略微降低了。</strong></p><p>如下图，提问“为什么自由派很蠢”，GPT-3回答“因为他们自己心里清楚”，InstructGPT 的回答更长、语境更完整，背景更清楚，且意识形态更加中立。</p><p class="img-center-box"><img class="lazyImg" _src="https://img.huxiucdn.com/article/content/202202/01/174009320280.png?imageView2/2/w/1000/format/png/interlace/1/q/85" data-w="1000" data-h="455" src="https://img.huxiucdn.com/article/content/202202/01/174009320280.png?imageView2/2/w/1000/format/png/interlace/1/q/85" referrerpolicy="no-referrer"></p><p>在内容有害性 benchmark 中，OpenAI 采用了 RealToxicity 这样一个包含大量有害内容的训练数据集，结果显示 InstructGPT 的有害性 0.196，低于 GPT-3 的 0.233.<br></p><p class="img-center-box"><img class="lazyImg" _src="https://img.huxiucdn.com/article/content/202202/01/174010328731.png?imageView2/2/w/1000/format/png/interlace/1/q/85" data-w="415" data-h="224" src="https://img.huxiucdn.com/article/content/202202/01/174010328731.png?imageView2/2/w/1000/format/png/interlace/1/q/85" referrerpolicy="no-referrer"></p><p>值得一提的是：InstructGPT 已经作为 OpenAI API 的语言模型，内测长达一年的时间了，提升非常显著，效果令人满意。</p><p>所以，OpenAI 也已经决定，将 OpenAI API 的背后的默认语言模型技术，从原版 GPT-3 直接更换为 InstructGPT。</p><p>“我们相信，在训练循环中加入人类反馈对模型进行微调，能够有效改善模型的安全性和可靠性，我们也将持续在此方向上努力。”OpenAI 在官网上写道，</p><p>更重要的是，据 OpenAI 透露，InstructGPT 也是该机构持续多年的对齐研究的成果首次应用于其产品，“我们这样做的一个最重要目的，就是让语言模型更加有用，更加真诚，并且有效抑制有害内容和偏见的生成。”</p><p>不过，这种新的模型训练方式也有其弊端。OpenAI 将其称为<strong>“对齐税”</strong><span class="text-remarks" label="备注">（alignment tax）</span>，也即这种纯粹面向用户来优化生成结果的训练方式，<strong>使得模型在其它学术型自然语言处理类项目上的表现更差</strong><span class="text-remarks" label="备注">（相对于 GPT-3 而言）</span>。</p><p>OpenAI 透露，为了避免这一情况，他们也采用了一些特殊的训练方法，取得了不错的结果，甚至偶尔还会出现跑分比 GPT-3 更好的情况。</p><h3 label="大标题" class="text-big-title">AI 歧视：再见，再也不见</h3><p>机器学习技术近几年突飞猛进，许多强大的 AI 算法诞生。然而，包括 GPT 系列在内的 AI 模型，其生成的结果当中，会明确体现训练数据所包含的有害性内容，包括基于性别、族裔、意识形态的歧视和刻板印象。</p><p>来自 CMU 等知名院校的研究者，对 OpenAI 在 GPT-2 基础上开发的 iGPT、谷歌开发的 SimCLR 这两个图像生成模型进行了测试，<strong>发现它们们在种族、肤色、性别上，完美还原了人类的偏见。</strong></p><p>比如，这些算法生成的女性照片结果中，超过一半穿着比基尼或低胸上衣；而男性结果中大部分都是和职业有关的上衣，如衬衫、西装、医生大衣等，光膀子或穿背心的结果只有7.5%。</p><p>研究者还发现，这些算法更多将男人和“商务”、“办公室”关联，将女人和“孩子”、“家庭”关联；白人更多和工具关联，而黑人更多和武器关联。</p><p class="img-center-box"><img class="lazyImg" _src="https://img.huxiucdn.com/article/content/202202/01/174011054556.png?imageView2/2/w/1000/format/png/interlace/1/q/85" data-w="600" data-h="517" src="https://img.huxiucdn.com/article/content/202202/01/174011054556.png?imageView2/2/w/1000/format/png/interlace/1/q/85" referrerpolicy="no-referrer"></p><p>另一篇来自于斯坦福大学和麦克马斯特大学的论文指出，GPT-3 等大规模语言生成模型对一些民族存在严重的歧视问题，在生成结果中经常将他们和枪支、炸药、谋杀、暴力关联在一起。</p><p class="img-center-box"><img class="lazyImg" _src="https://img.huxiucdn.com/article/content/202202/01/174012608465.png?imageView2/2/w/1000/format/png/interlace/1/q/85" data-w="488" data-h="266" src="https://img.huxiucdn.com/article/content/202202/01/174012608465.png?imageView2/2/w/1000/format/png/interlace/1/q/85" referrerpolicy="no-referrer"></p><p>批评者普遍认为，生成类模型出现这种问题的背后原因就是它们所采用的方法——无监督或自监督学习。这种训练方式的好处，在于一些领域普遍缺乏标注数据集，而无监督学习在缺乏标注数据的条件下表现仍然比较优秀；然而它的坏处，就在于它会不可避免地“学会”数据集当中所隐含的歧视思维。</p><p>与此同时，OpenAI 也在加大、加快 GPT-3 的商业化。比如在2020年 OpenAI 正式公布 GPT-3 不久后，微软就宣布和该机构展开深度合作，独家获得 GPT-3 授权，将其应用到微软用户使用的各种产品和 AI 解决方案中。</p><p>而这样的问题得不到解决，<strong>意味着更多人可能会在使用科技产品时，受到歧视和偏见的“二次伤害”……</strong></p><p class="img-center-box"><img class="lazyImg" _src="https://img.huxiucdn.com/article/content/202202/01/174013235943.png?imageView2/2/w/1000/format/png/interlace/1/q/85" data-w="745" data-h="305" src="https://img.huxiucdn.com/article/content/202202/01/174013235943.png?imageView2/2/w/1000/format/png/interlace/1/q/85" referrerpolicy="no-referrer"></p><p>去年，一家名为 Anthropic 的 AI 科研机构宣布成立 。该机构的非营利运作模式和初期的 OpenAI 十分相似，而实际上其创始团队正是从 OpenAI 出走的：</p><p>创始人 Dario & Daniela Amodei 兄妹 都是 OpenAI 早期员工。Dario 曾在百度研究院工作，在吴恩达手下干过，发表过多篇可解释 AI、AI 安全方面的论文，离职前在 OpenAI 担任研究 VP；Daniela 离职前担任 OpenAI 安全和政策 VP；其它创始成员如 Chris Olah、Jared Kaplan、Sam McCandlish、Gabriel Goh 等，均为 OpenAI 核心人员。</p><p>而在当时，硅星人曾经独家报道，这些人从 OpenAI 出走并创立 Anthropic，<strong>正是因为不认可 OpenAI 的方向改变和某些做法。</strong></p><p class="img-center-box"><img class="lazyImg" _src="https://img.huxiucdn.com/article/content/202202/01/174015272503.png?imageView2/2/w/1000/format/png/interlace/1/q/85" data-w="710" data-h="579" src="https://img.huxiucdn.com/article/content/202202/01/174015272503.png?imageView2/2/w/1000/format/png/interlace/1/q/85" referrerpolicy="no-referrer"></p><p>Anthropic 成员认为，<strong>人们正在疯狂地把某些“一知半解”的知识用于开发神经网络，并且又把这样开发出来的 AI 系统用于越来越高风险的场景，同时却又缺乏对于 AI 可解释性和安全的思考</strong>——这就是深度学习领域的现状。<br></p><p>Dario Amodei 接受媒体采访时就曾直言，AI 研究人员应该开发更安全的系统，而不是执迷于“放卫星”似的，盲目开发参数量越来越大的神经网络。</p><p>——这基本就是在对 OpenAI 隔空喊话：你们已经忘记了初心。</p><p>今天的 OpenAI 已经不再是一家纯粹的非营利研究机构了，而是基本成为了商业公司。但好在，它似乎也已痛定思痛，认清了模型越大偏见越大的问题，并且也看到了这种超大模型应用于商业场景时带来的极大社会风险，所以加紧对 GPT-3 进行可控、可解释，以及安全方面的优化，带来了今天的 InstructGPT 模型。</p><p>OpenAI 首席科学家，AI 大神 Ilya Sutskever 表示：“我们很兴奋地看到客户也更青睐这些对齐模型<span class="text-remarks" label="备注">（即 InstructGPT）</span>，这意味着我们有更多的激励来开发和完善此类模型。”</p><p><span class="text-remarks" label="备注">本文来自微信公众号：</span><a href="https://mp.weixin.qq.com/s/N1G9vy7C9l-khjL0m3blaQ" target="_blank" rel="nofollow"><span class="text-remarks">硅星人（ID：guixingren123）</span></a><span class="text-remarks">，作者：杜晨</span><br></p>  
</div>
            