
---
title: '能自主编程的AI，会抢程序员的饭碗吗？'
categories: 
 - 新媒体
 - 虎嗅
 - 首页资讯
headimg: 'https://img.huxiucdn.com/article/content/202203/09/175302062973.png?imageView2/2/w/1000/format/png/interlace/1/q/85'
author: 虎嗅
comments: false
date: Wed, 09 Mar 2022 12:12:00 GMT
thumbnail: 'https://img.huxiucdn.com/article/content/202203/09/175302062973.png?imageView2/2/w/1000/format/png/interlace/1/q/85'
---

<div>   
<p><span class="text-remarks" label="备注">还是说，它会解放程序员？本文来自微信公众号：</span><a href="https://mp.weixin.qq.com/s/yBzBn-7OVhRt1tN8Mj_DTw" target="_blank" rel="nofollow" style="text-decoration: none;"><span class="text-remarks">极客鹏友说（ID：geekpys）</span></a><span class="text-remarks">，口述：瘦驼、肖涵、袁进辉、张鹏，整理：汤一涛，头图来自：Unsplash</span></p><p>春节期间，开发出 AlphaGo 的人工智能公司 DeepMind 又发布一个能够自主编程的新模型——AlphaCode。</p><p>在编程竞赛网站 Codeforces 举办的 10 场比赛中，AlphaCode 取得了前 54.3% 成绩。这意味着，<a href="https://www.huxiu.com/article/495486.html" target="_blank">它打败了将近一半的人类选手。</a>更为关键的是，<strong>比赛中的所有代码都是由 AlphaCode 自动生成的，全程无需人工监督。</strong></p><p>AlphaCode 取得的成绩意味着什么？它会抢走程序员的饭碗吗？在基础科学领域，AI 发挥了怎样的作用？该如何理解人类和 AI 的关系？</p><p>上周，《今夜科技谈》邀请到了科普作家瘦驼、Jina AI 创始人兼 CEO 肖涵、一流科技 OneFlow 创始人兼 CEO 袁进辉，聊了聊和 AI 有关的话题。</p><p class="img-center-box"><img class="lazyImg" _src="https://img.huxiucdn.com/article/content/202203/09/175302062973.png?imageView2/2/w/1000/format/png/interlace/1/q/85" data-w="512" data-h="288" src="https://img.huxiucdn.com/article/content/202203/09/175302062973.png?imageView2/2/w/1000/format/png/interlace/1/q/85" referrerpolicy="no-referrer"></p><h3 label="大标题" class="text-big-title">如何评价AlphaCode取得的成绩？它还需要人工训练吗？<br></h3><p><strong>肖涵：</strong>AlphaCode 其实不是一个单一的算法，它是根据 GPT-3 模型搭建出来的一个系统。<strong>所以我们不认为 AlphaCode 是AI算法上的突破，它的突破在于产生了一个能够胜任比较复杂任务的系统。</strong></p><p>就算法训练来说，人工智能的第一步是<strong>预训练</strong>。就是把 GitHub 上的程序都拉下来，让这个模型对编程有一个初步的认识。</p><p>第二步是<strong>微调</strong>。因为预训练得到的知识并不都是有用的，它们给出了一个大体的世界观，但如果要解决具体的问题，就需要定向调整。</p><p>AlphaCode 做的另外一件事就是构建了一套<strong>评判系统</strong>。因为基于给定的题目，AlphaCode 可以生成大量答案，但它并不知道这些答案正确与否。所以把问题答案和他给出的答案输入到这个系统中，它就可以评判答案正确与否。</p><p>整个过程确实就像 AlphaCode 发布时所宣称的，是可以做到完全无监督的。</p><p><strong>袁进辉：</strong>我认为 AlphaCode 是<strong>里程碑式的进展</strong>，根据自然语言描述，自动生成可解决算法问题的完整程序，这很了不起。</p><p>其实我对 AlphaCode，经历了从低估到高估再到低估的心理转变。</p><p>我在看到论文细节之前，低估了 AlphaCode。和 GPT-3 训练自然语言类似，AlphaCode 本质上也是抓取了 GitHub 上的几百亿行代码，捕捉语言结构，再生成。并且，相比于非常不规律的自然语言，<strong>代码的语法是非常规律的。</strong>从这个角度来说，AlphaCode 和以前解决问题的方法差不多。</p><p>但令我感到意外的是，我本来以为 AlphaCode 的原理是自动搜索代码库中已有的代码片段，<strong>但实际上这些代码都是预训练模型自动生成的。</strong>传统上我们认为 AI 解决的通常是低精度的问题，比如图片识别准确率 90% 已经很高了，但让 AI 做到 100% 的准确率是极难的。而代码恰恰要求 100%，即使是写错一个变量名称，或者少打一个“;”，程序也会报错。所以这时我又高估了 AlphaCode。</p><p>后来我又看了 AlphaCode 的论文，发现它确实可以自己生成完整程序，但中间也用了一些取巧的成分。比如说它会为一个问题生成 100 万份程序，其中当然有对有错。AlphaCode 会筛选掉 99% 的代码，留下 1%。在这 1% 的几千份代码中，AlphaCode 又通过聚类筛选等方式，筛选出 10 个答案提交到 Codeforces。只要这 10 个答案中有一个正确答案，就算过了。</p><p>所以 AlphaCode 并不是一次性生成解决问题的程序，<strong>而是为一个问题生成数十万、上百万的程序，经过简单测试、样例筛选，最终筛选出正确答案。</strong></p><h3 label="大标题" class="text-big-title">对比Codex、Github Copilot这些辅助编程工具，AlphaCode有什么不同？</h3><p><strong>袁进辉：</strong>我觉得有两点，一是解决的问题不同，二是方法不同。</p><p>AlphaCode 解决的问题，还是有一定难度的问题。</p><p>二是 AlphaCode 不是网上搜索的现成代码片段，是自己生成的。像 Github Copilot，就是搜索的现成代码片段。之前有人做过实验，发现它会从 Stack Overflow<span class="text-remarks" label="备注">（代码问答网站）</span>上抓取代码，因为它把程序员写在代码里的注释都抓取过来了。</p><h3 label="大标题" class="text-big-title">这些AI编程工具，会取代程序员吗？<br></h3><p><strong>肖涵：</strong>其实这些 AI 辅助编程工具，<strong>都是为了帮助开发者能有更好的编程体验。</strong>只不过 AI 发展到今天已经非常强大，它可以自动把代码写完，而不仅仅是填函数名这么简单。</p><p>但是今天所有的AI都没有到达取代开发者的程度，开发者本身还是那个最终的决策人。AI 所生成的代码，仅仅是一个参考。</p><p><strong>瘦驼：</strong>作为一个文字工作者，我还是想把这件事映射到“自然语言”的领域来。</p><p>首先，<strong>让 AI 生成一段没有任何语法错误的代码没有那么难。</strong>在自然语言里有大量不符合逻辑和语法的东西，我们在说话的时候并不是严格遵循某种规律的。但代码本身是严格遵守语法的，它有一套通用的逻辑。</p><p>第二点，我觉得码农其实不必太紧张，对于文字创作者来说，我们在前几年已经碰到过类似的挑战了。现在一些固定格式的文本已经大量地由 AI 生成，比如比赛结果、股市播报、天气预报。对于这种有规律可循的文本来说，AI 写得比人快多了。但是想让 AI 写一些有创造性的东西还是非常困难的，<strong>因为创造性本质上是对现有逻辑和体系的挑战，甚至创造性包含了允许 AI 犯错。</strong>对 AI 来说，它很难保持一定的个性，比如《红楼梦》里，贾宝玉的诗和林黛玉的诗就有明显的不同。这种能力，AI 现在可能还很难做到。</p><p><strong>肖涵：</strong><strong>我觉得性格的差别，无非就是训练语料的不同。</strong>如果我想生成一个朦胧派诗人的风格，我就把所有朦胧派诗人的语料收集起来，训练一下就可以了。</p><p>所以我觉得对 AI 最重要的还是<strong>数据</strong>。算法模型如果能更好地挖掘数据，把数据的价值充分利用起来，那其实所谓的性格也就达成了。</p><p><strong>袁进辉：</strong>我来补充一个反方向。按照肖涵的逻辑，那如果没有朦胧派的语料，就生成不出朦胧派的 AI；如果没有梵高这个人，就训练不出梵高风格的绘画。所以 AI 在创造层面，<strong>本质上还是更像一种记忆，</strong>差别在于是一种机械的记忆还是比较聪明的记忆，<strong>但其实都跳脱不出原有的范畴。</strong></p><p>专业术语里有两个词，“exploitation”和“exploration”。exploitation 指的是保守地在应有的范围内工作，exploration 指的是在范围之外探索。</p><p>也许我们需要给 AI 一点犯错空间。</p><h3 label="大标题" class="text-big-title">在AI崛起的背景下，怎么做一个有价值的程序员？<br></h3><p><strong>袁进辉：</strong>AI 比较擅长做比较机械的工作，但写代码也是需要创造性的，写到一定程度，我们也把它称之为艺术。</p><p>代码里面也有好坏，写出来的代码是不是足够简洁优雅，是否有创造性的审美，这可能还是人类的优势。</p><p><strong>张鹏：</strong>但这种美，是不是也要能体现在效率上高于丑和复杂的东西，商业世界才会认可？</p><p><strong>袁进辉：</strong><strong>这种美，确实在效率上有体现。</strong>判断代码美丑的一个标准，就是<strong>可复用性</strong>。简单的代码有更好的扩展性，未来就可以在更多的地方复用。如果我的代码只在当前的任务可以用，就是不太好的实践。</p><p><strong>肖涵：</strong>我想起来之前国外论坛上有一个特别火的帖子，就是说一个律所雇佣了一个小伙子做类似报表整理之类的工作。恰好这个小伙子懂一点编程，就把这个工作自动化了，程序运行 5 分钟就可以做完原来一周的工作。两周之后，这个小伙子内心有点愧疚，就把这件事发到了论坛上，问大家该不该告诉老板这件事。</p><p>我觉得这件事反映了一点——不管他有没有告诉老板，<strong>机械化的工作一定会被取代。</strong>甚至他自己也觉得做这种工作没有意义，否则就不会有这种纠结。人生总要有点追求，何必在这里浪费时间？</p><p>其次是我觉得出现这些代码辅助 AI 不是坏事。人类发展到今天，不管是工业革命还是流水线的引入，人类总会从事更高级的职业，会创造更高的价值。总体来说，我觉得这是一件正向的事。</p><h3 label="大标题" class="text-big-title">怎么理解当下很火热的低代码、软件2.0这些概念？</h3><p><strong>肖涵：</strong>低代码出现的原因，<strong>其实是过去几十年我们已经积累了大量的代码资本。</strong>今天任何一个软件，都不是从头开始写的，它们都有自己的上游依赖关系——<strong>软件库。</strong></p><p>实际上我们构建现代软件的时候，最重要的往往不是创新，而是可复用性。可复用性指的就是，这个软件完成之后，一定要成为更大型软件中的一个组件，而不是从头开始重复造轮子。</p><p>当“可复用性”这种概念深入人心之后，于是才有了低代码、无代码这种概念。今天我们构建的代码越来越高级，不再是操作系统这种底层软件，更多的是面向 C 端用户的高级软件。这种情况下就非常凸显低代码、无代码的重要性。</p><p>如果纵观整个人类的工程史，其实可复用性就是非常关键的转折点——一旦一个东西可以被复用，人类的文明就会发展到一个新高度。我们可以假想一下，一个原始人拿着两块石头碰触了火花，这是一个偶然吗？还是说它成为了可以被复用的经验，于是人类就此掌握火的使用？</p><p>所以我更想强调的是，低代码和无代码肯定是发展趋势。但趋势背后的原因在于，<strong>我们现在面向的是更高级的软件开发，这种开发尤其强调复用性。</strong></p><p><strong>袁进辉：</strong>我来补充一下软件 2.0。</p><p>软件 1.0 说的是<strong>代码是数据</strong>。我们在代码的基础上，基于数据训练一个 AI 模型。</p><p>软件 2.0 指的是<strong>模型是代码</strong>。在 AlphaCode 之前，AI 的模型早就开始为人写代码了。比如图形识别模型，它的原理是计算机视觉科学家写了一堆规则——图片里有哪些特征，那这张图片就是汽车。但是这么做了几十年之后，发现这种方法识别率并不高。所以现在的做法是从一堆数据中训练出一个模型。</p><p>以前代码必须由程序员理解问题之后转化成计算机理解的语言，计算机才能帮我们解决问题，本质上是一个从物理世界迁移到数字世界的过程。模型即代码的意思就是，现在我们不需要经过人脑了，只要收集一堆数据，计算机就能自动挖掘数据中的规律，生成模型。</p><h3 label="大标题" class="text-big-title">可复用性是判断代码好坏的一个重要标准。现在代码间复用的情况也越来越普遍，但如果引用的底层代码本身就有问题，该怎么办？</h3><p><strong>肖涵：</strong>现实世界中确实发生过这样的事。前几个月 <a href="https://www.huxiu.com/article/484104.html" target="_blank">Log4j 软件包出错</a>，就造成了非常多软件公司的恐慌。这样的问题，在低代码和无代码的环境下会更难发觉。因为没有多少人会去写这种底层的代码了，大家关注的更多是更高级的软件业务逻辑。</p><p>之前在 Javasript 社区还发生了一件事。维护底层代码的程序员因为觉得别人在网上攻击他，一怒之下<a href="https://www.huxiu.com/article/489517.html" target="_blank">把代码删了</a>。这导致的后果就是整个复用的链条断了。</p><p>所以说在低代码、无代码的环境下，<strong>一定要保证上游足够稳健。即使出错也要能够及时修复，</strong>这一点非常重要。</p><p><strong>袁进辉：</strong>其实 AI 的模型是很脆弱的，非常容易被攻击。实际上训练 AI 模型的就是一堆数据，简单理解就是高维空间中的<strong>一个方向</strong>。如果进来的数据是沿着这个方向的，AI 就判断得很准。如果进入的数据和那个方向垂直，那 AI 的判断就会出错。</p><p>前几年有人在特斯拉的激光雷达上做了一个实验，用一个小纸片稍微改变一下传进去的信号，特斯拉的识别就出错了。</p><p><strong>张鹏：</strong>前段时间我还听到过这种观点。就图像识别技术而言，在最初构建 ImageNet<span class="text-remarks" label="备注">（视觉影像网数据库）</span>的时候，因为当时数据的局限，导致现在形成了一些偏差，比如 AI 对人类的歧视问题。但今天 ImageNet 已经成为了一个基础设施，无数人在这之上构建了新的设施，这种偏差就成为了一个<strong>逐级感染</strong>的问题，越来越难以克服。</p><p>可能在某些时刻，<strong>需要一个根本性的重启，</strong>才能解决这个问题。</p><h3 label="大标题" class="text-big-title">在基础科学领域，AI能够发挥哪些作用？</h3><p><strong>瘦驼：</strong>对于很多数据敏感型的行业来说，AI 确实解放了很多科学家，让这些科学家可以去做更多有创造性的工作。</p><p>之前讨论过，在人类的天文学家中，最容易被 AI 取代的可能是埃德文·哈勃。他花了几十年时间，从大量数据中发现了星系的“红移-距离关系”。这本质上是一个数据相关性的问题，放在今天的 AI 身上，可以立刻发现这种数据间的规律。</p><p>又比如说快速射电暴，它以前经常被人忽略。因为快速射电暴太短暂了，很容易被认为是数据中出现的一个异常。但是有了 AI 这样的工具后，它就可以从这些不断出现的偶然异常中发现规律。</p><p><strong>我觉得 AI 的出现改变了科学发现的范式，让我们具备了从数据中找出被忽略的规律的能力。</strong></p><h3 label="大标题" class="text-big-title">AI公司的商业化都很困难，但DeepMind在去年盈利了，怎么理解这家公司？</h3><p><strong>肖涵：</strong>首先我对 DeepMind 是非常崇敬的，但我不会做这种公司，我个人认为 DeepMind 风险性是非常高的，回本的机率很低。</p><p>首先在深度学习的前提下，算力成本和存储成本的投入是非常高的，这个很好理解。</p><p>第二个是人员原因。每年 DeepMind 都有一两篇轰动性的论文面世，很重要的原因就在于，它储备了大量全世界最顶尖的人才。这种人才成本，不是每个公司都能承担的。<strong>深度学习发展到今天，实际上拉大了大公司和小公司间的贫富差距。</strong></p><p>我个人其实更看重 AI 在工程领域的突破，如何更好地解决已有的问题，而不是找到一个从没被解决过的新问题去突破。</p><p><strong>袁进辉：</strong>DeepMind 的确不太典型，他理论上不是一家商业公司。大部分商业公司，一定是要做一个可复制的商业化产品，更多的是考虑市场规模等一系列更实际的问题。</p><p><strong>DeepMind 更像一家科研机构。</strong>只不过它不像科研机构申请经费那么困难，DeepMind 背后有谷歌源源不断地投入资金。</p><h3 label="大标题" class="text-big-title">这些年，AI的发展思路有什么变化？</h3><p><strong>肖涵：</strong>我觉得有两点，一是 AI 的解释性，二是 AI 的训练过程。</p><p><strong>1. 解释性。AI 这些年其实经历了从可解释到不可解释的过程。</strong></p><p>早期的 AI 是基于一套规则生成的，比如最早的问答机 Eliza，它的逻辑在于，识别你说的话包含哪些字符，经过因果判断，返回特定的答案。这些回答都是可以解释的，因为程序是写死的。</p><p>后来 AI 发展到 2000 年左右，出现了参数化模型。参数化模型会把图片、声音、文本等信息描述成一个数据函数，AI 所要做的，就是填入系数。这一阶段的 AI 也是可解释的。</p><p>自 2010 年以来，AI 逐渐转向了深度学习框架，AI 开始变得不可解释。因为深度学习框架拆分到最细，其实是由一个一个非线性函数叠加而成的。如果只有一个非线性函数的话还比较好解释，但叠加在一起的非线性函数有点类似蝴蝶效应，基本上是不可能溯源的。</p><p>我们整个深度学习网络是一个<strong>非常深的非线性系统</strong>，比如 AlphaCode 就包含 400 多亿个参数，实际上无法追根溯源，到底是哪个参数产生的影响。这就好像我们无法判断，到底是哪一个神经元令人脑产生了意识一样。</p><p>但是今天，又有一些人要求 AI 具有可解释性。因为随着 AI 越来越聪明，<strong>我们要求它承担它的社会责任了，</strong>比如不能歧视黑人。当出现问题时，就可以找到出现问题的原因，这就要求 AI 可以被解释。</p><p><strong>2. 训练过程。</strong>AI 的训练过程，从一开始的“端到端”过程，拆分成了“预训练”和“微调”两步，专业术语叫<strong>迁移学习</strong>。</p><p>其实我本人非常喜欢迁移学习，因为它为机器学习指明了一个大方向。以往的机器学习，每解决一个问题，就要专门构建一个模型。即使是解决两个非常相似的问题，比如识别是篮球新闻还是足球新闻，哪怕都是自然语言处理，在传统机器看来都是不同的任务。</p><p>机器学习将端到端训练的过程拆分成了两部分。一部分是预训练，从大规模的语料中学到一个相对通用的知识。然后是微调，将通用的知识去解决特定的细节问题。</p><p>这样拆分的好处在于，<strong>大规模语料训练的模型可以得到复用。</strong>因为不是所有公司都有能力构建这种超大规模的模型。可以复用之后，中小公司就可以拿这个模型针对自身特定的领域微调，就能产生子领域上的业务价值，节省了大量的人力、物力和时间成本。</p><p><strong>袁进辉：</strong>我想补充的是，<strong>这种大模型还没到头，以后还会越来越大。</strong>像 GPT-3，参数已经达到 1700 亿，但和人脑的神经元连接数相比，还差 1000~10000 倍左右。</p><p>一个可能的猜想是，<strong>智能或许没有那么神奇，只是一个规模的问题，</strong>最后会由量变产生质变。</p><p>另一种观点认为，预训练模型在脑科学和神经科学上也有一定的支撑。人之所以这么聪明，有一部分是后天习得的，但主要还是先天决定的。在一个婴儿出生之前，大脑皮层间的连接和神经元的突起，就已经大致由基因决定了。在婴儿出生看到这个物理世界后，神经元之间的连接会根据物理信号微调——有的连接会越来越强，不太使用的连接就会变弱。</p><p>这整个过程都非常类似预训练和微调的模式。所以从某种意义上来说，大模型预训练，的确有生物的合理性。</p><h3 label="大标题" class="text-big-title">AI和人类会是怎样的关系？<br></h3><p><strong>肖涵：</strong>我对 AI 能力的增强是非常有信心的。但是，在目前这套方法之下，我认为 AI 可能最终无法产生自我意识。</p><p>可话说回来，<strong>难道AI一定要有自我意识吗？</strong></p><p>50 年后可能 AI 仍然没有自我意识，但它可以解决非常多重要的问题，要比人类解决得好得多，这个时候你会愿意承认这种形态的东西是“智能”吗？</p><p><strong>袁进辉：</strong>我倾向于从正面理解这件事，就是 AI 可以解放人类，<strong>让人类去追求更本质的东西，</strong>就好像蒸汽机把我们从体力劳动中解放出来一样。</p><p>这让我想起了刘慈欣的小说《朝闻道》中的一个设定：地球人可以向外星人提任何问题，外星人会告诉你正确答案。但代价是知道答案后，这个提问者会立刻死去。但最终有一个人问了外星人这样一个问题：宇宙的目的是什么？</p><p>外星人也不知道，于是他崩溃了。</p><p><strong>张鹏：</strong>虽然我们被冠以“拥有自我意识”，但全世界也有大量的人不知道自己的目的是什么。所以我觉得以自我意识来定义也许是一种<strong>人类沙文主义</strong>。</p><p><strong>瘦驼：</strong>我觉得对于 AI 的思考有几个层面。</p><p>一个是哲学层面上的。我认为<strong>如果未来AI变得像人一样，那一定是失败的 AI。</strong>他有人的缺点，有人自己都搞不清的逻辑错误，那我们为什么要造 AI？直接造人不是简单得多吗？我们之所以要造 AI，一定是它可以解决人类解决不了的问题，这样 AI 才有意义。</p><p>另外我也有一些应用层面上的顾虑。刚刚几位也说到，AI 很脆弱。如果我们已经高度依赖 AI 了，一旦底层的东西有问题，会造成非常大的影响。</p><p>然后是关于解释性的困局。解释性的困局其实是用来解放人类自己的，就是想求一个安心。但即便这个说法本身没有意义，也一定会让很多人心生警惕。这种警惕，从人作为社会性动物的角度来说，就会对 AI 的发展产生影响。我们需要有一定的准备，<strong>不要让这种警惕发酵到一个比较尖锐的地步，</strong>那到时状况就比较糟糕了。</p><p><strong>张鹏：我觉得真正的问题是，AI今天还有大量的基础工作要做好。</strong>我们可以确定，AI 会成为人类未来文明进程中重要的伙伴，但它距离这个角色，其实还有很长的路要走。</p><p>今天我们对于 AI 的探讨，如果是针对“不要犯一些基础错误”，是有意义的。但在 AI 能力还比较羸弱的时候过度讨论“自我意识”之类的话题，其实是很务虚的。</p><p><span class="text-remarks" label="备注">本文来自微信公众号：</span><a href="https://mp.weixin.qq.com/s/yBzBn-7OVhRt1tN8Mj_DTw" target="_blank" rel="nofollow" style="text-decoration: none;"><span class="text-remarks">极客鹏友说（ID：geekpys）</span></a><span class="text-remarks">，口述：瘦驼（科普作家）、肖涵（Jina AI 创始人）、袁进辉（一流科技 OneFlow 创始人）、张鹏（极客公园总裁），整理：汤一涛</span></p>  
</div>
            