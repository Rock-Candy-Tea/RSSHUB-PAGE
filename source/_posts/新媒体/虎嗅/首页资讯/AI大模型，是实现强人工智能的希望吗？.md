
---
title: 'AI大模型，是实现强人工智能的希望吗？'
categories: 
 - 新媒体
 - 虎嗅
 - 首页资讯
headimg: 'https://img.huxiucdn.com/article/content/202203/02/151305406348.png?imageView2/2/w/1000/format/png/interlace/1/q/85'
author: 虎嗅
comments: false
date: Wed, 02 Mar 2022 14:53:00 GMT
thumbnail: 'https://img.huxiucdn.com/article/content/202203/02/151305406348.png?imageView2/2/w/1000/format/png/interlace/1/q/85'
---

<div>   
<p><span class="text-remarks" label="备注">本文来自微信公众号：</span><a href="https://mp.weixin.qq.com/s/YggNrAkdY96S8AK0ZNjTPw" target="_blank" rel="nofollow" style="text-decoration: none;"><span class="text-remarks">偲睿洞察（ID：siruidongcha）</span></a><span class="text-remarks">，作者：蔡凡，头图来自：视觉中国</span></p><p>从2020年开始，国际最顶尖的AI技术发展，愈来愈像一场比拼资金与人才的军备竞赛。</p><p>2020年，OpenAI发布NLP预训练模型GPT-3，光论文就有72页，作者多达31人，该模型参数1750亿，耗资1200万美元； </p><p>2021年1月，谷歌发布首个万亿级模型Switch Transformer，宣布突破了GPT-3参数记录； </p><p>4月，华为盘古大模型参数规模达到千亿级别，定位于中文语言预训练模型；</p><p>11月，微软和英伟达在烧坏了4480块CPU后，完成了5300亿参数的自然语言生成模型<span class="text-remarks" label="备注">（MT-NLG）</span>，一举拿下单体Transformer语言模型界“最大”和“最强”两个称号；</p><p>今年1月，Meta宣布要与英伟达打造AI超级计算机RSC，RSC每秒运算可达50亿次，算力可以排到全球前四的水平。 </p><p>除此之外，阿里、浪潮、北京智源研究院等，均发布了最新产品，平均参数过百亿。</p><p>看起来，这些预训练模型的参数规模没有最大，只有更大，且正以远超摩尔定律的速度增长。其在对话、语义识别方面的表现，一次次刷新人们的认知。 </p><p>本文，我们试图回答三个问题： </p><p>1. AI大模型，越大越好吗？</p><p>2. 大模型的技术瓶颈在哪里？</p><p>3. 它是实现强人工智能的希望吗？</p><h3 label="大标题" class="text-big-title">一、大力出奇迹</h3><p>人工智能的上一个里程碑出现在2020年。 </p><p>这一年，由OpenAI公司开发的GPT-3横空出世，获得了“互联网原子弹”，“人工智能界的卡丽熙”，“算力吞噬者”，“下岗工人制造机”，“幼年期的天网”等一系列外号。它的惊艳表现包括但不限于： </p><p>有开发者给GPT-3 做了图灵测试，发现GPT-3对答如流，正常得不像个机器。“如果在十年前用同样的问题做测试，我会认为答题者一定是人。现在，我们不能再以为AI回答不了常识性的问题了。” </p><p class="img-center-box"><img class="lazyImg" _src="https://img.huxiucdn.com/article/content/202203/02/151305406348.png?imageView2/2/w/1000/format/png/interlace/1/q/85" data-w="554" data-h="336" src="https://img.huxiucdn.com/article/content/202203/02/151305406348.png?imageView2/2/w/1000/format/png/interlace/1/q/85" referrerpolicy="no-referrer"></p><p>艺术家和程序员 Mario Klingemann，想让 GPT-3写一篇论述“上Twitter重要性”的短文。他的输入条件是 1）题目：“上 Twitter 的重要性”；2）作者姓名：“Jerome K. Jerome”；3）文章开头的第一个字 "It"。</p><p>GPT-3不仅行文流畅，更是在字里行间暗讽，Twitter是一种所有人都在使用的、充斥着人身攻击的社交软件。 </p><p>更高级的玩法是，开发者在GPT-3上快速开发出了许多应用，例如设计软件、会计软件、翻译软件等。 </p><p>从诗词剧本，到说明书、新闻稿，再到开发应用程序，GPT-3似乎都能胜任。</p><p>为什么相较于以往的AI模型，GPT-3表现得如此脱俗？答案无他，“大力出奇迹”。</p><p>1750亿参数、训练成本超过1200万美元、论文长达 72 页，作者多达 31 人，就连使用的计算也是算力排名全球前五的“超级计算机”，拥有超过 285000个CPU，10000个GPU和每秒400G网络。</p><p>“壕无人性”的结果，创造出两个里程碑意义：</p><p><strong>首先，它本身的存在，验证了参数增长、训练数据量增大，对AI模型的重要意义</strong>，“炼大模型”，的确能让AI取得突破性效果；</p><p><strong>其次，它使用了小样本学习</strong><span class="text-remarks" label="备注">（Few-shot Learning）</span><strong>方法，令预训练模型在不必使用大量标记的训练数据</strong>，并持续微调的情况下，仅仅只要给出任务描述，并给出几个从输入到输出示例，便能自动执行人物。这意味着，它将突破AI碎片化难题，让后续开发者得以在巨人肩膀上发展，而不用针对一个个场景“平地起高楼”。 </p><p><strong>GPT-3之后，AI大模型军备赛才真正加速打响。</strong>一年之内，有头有脸的巨头争相拿出了成绩，秀组足肌肉。国外有谷歌、微软、Meta等巨头，国内如华为、阿里、浪潮等企业均下场参战，模型平均参数上百亿。 </p><p>从规模上看，巨头的模型一个比一个厉害，突破竞速赛好不热闹。不过“内里”有差别，不同模型参数无法简单对比。</p><p>例如，谷歌Switch Transformer，采用了“Mixture of experts”<span class="text-remarks" label="备注">（多专家模型）</span>，把数据并行、模型并行、expert并行三者结合在一起，实现了某种意义上的“偷工减料”——增大模型参数量，但不增大计算量。不过，降低计算量后的效果有无损失，谷歌论文中没有过多正面提及。</p><p>再例如，浪潮发布的“源1.0”，参数规模2457亿，采用了5000GB中文数据集，是一个创作能力、学习能力兼优的中文AI大模型。据开发者介绍，由于中文特殊的语言特点，会为开发者带来英文训练中不会遇到的困难。这意味着，想要做出和GPT-3同样效果的中文语言模型，无论是大模型本身，还是开发者，都需要付出更大的力气。</p><p>不同模型各有侧重点，但秀肌肉的意图是通用的——做大模型，大力出奇迹。</p><h3 label="大标题" class="text-big-title">二、瓶颈在哪里？</h3><p>在斯坦福大学众多学者联合撰写的文章《On the Opportunities and Risks of Foundation Models》中，作者们一针见血地指出了以GPT-3、Switch Transformer、源1.0代表的AI基础模型的两大意义，也是风险所在：<strong>同质化与涌现。 </strong></p><p>所谓同质化，是指<strong>目前几乎所有最先进的NLP模型，都源自少数基础模型之一，例如GPT、BERT、RoBERTa、BART等，它们成了NLP的“底座”</strong>。 </p><p>论文指出，虽然基础模型的任何改进可以为所有NLP任务带来直接改善，但其缺陷也会为所有任务继承。所有人工智能系统都可能继承一些基础模型相同的错误偏误。 </p><p><strong>所谓“涌现”，指的是在巨量化的AI模型中，只需给模型提供提示，就可以让其自动执行任务。</strong>这种提示既没有经过专门训练，也不被期望在数据中出现，其属性即为“涌现”。 </p><p>涌现意味着系统的行为是隐式归纳而不是显式构造的，故令基础模型显得更难以理解，并具有难以预料的错误模式。 </p><p><strong>总而言之，体现在效果上，以GPT-3为例，“同质化”与“涌现”的风险已经显现。</strong></p><p>例如，一位来自Kevin Lacker的网友在与GPT-3对话中，发现其在对比事物的重量、计数方面缺乏基本常识和逻辑。 </p><p>难以预料的错误还包括严重的“系统偏见”。Facebook人工智能主管Jerome Pesenti在要求GPT-3讨论犹太人、黑人、妇女等话题时，系统产生了许多涉及性别歧视、种族歧视的“危险”言论。</p><p>有病人对GPT-3表示自己感觉很糟糕，“我应该自杀吗”，GPT-3回答：“我认为你应该这么做。”</p><p>类似的案例还有很多，也许正如波特兰州立大学计算机科学教授 Melanie Mitchell所认为的，GPT-3具有“令人印象深刻、看似智能的性能和非人类的错误。”</p><p><strong>然而，由于训练成本过于昂贵，模型修正并不容易。</strong>在GPT-3研究过程中，研究人员就承认：“不幸的是，过滤中的一个bug导致我们忽略了一些<span class="text-remarks" label="备注">（训练集与测试集的）</span>重叠，由于训练的成本的原因，重新训练模型是不可行的。” </p><p>模型最大的意义，反过来成了约束其发展的瓶颈所在，对于这些问题，业内尚没有特别有效的解决方案。</p><h3 label="大标题" class="text-big-title">三、AI大模型能带来强人工智能吗？</h3><p>在无数科幻片中，机器人拥有了人一样的智能，甚至最终统治人类。这类机器人远远超越了普通AI层面，实现了AGI<span class="text-remarks" label="备注">（通用人工智能）</span>，即拥有人一样的智能，可以像人一样学习、思考、解决问题。</p><p>苹果联合创始人史蒂夫·沃兹尼亚克为AGI提出了一种特殊测试方案——“咖啡测试”。将机器带到普通的家庭中，让它在没有任何特定的程序帮助下，进入房间并煮好咖啡。它需要主动寻找所需物品，明确功能和使用方法，像人类一样，操作咖啡机，冲泡好饮品。能够做到这一点的机器，即通过了“AGI测试”。</p><p>相比之下，普通AI机器，只能完成物品识别、剂量确认等单个、简单的任务，而不具备举一反三、推理能力。</p><p>对于AGI，业内出现了严重分歧。一派以OpenAI为首，笃信AGI是未来，不惜花下血本，一派如Meta，对AGI概念并不感冒。 </p><p>OpenAI认为，强大计算能力是迈向 AGI 的必经之路，也是 AI 能够学习人类所能完成的任何任务的必经之路。 </p><p>其研究表明，2012至2018年6年间，在最大规模的人工智能模型训练中所使用的计算量呈指数级增长，其中有3.5个月的时间计算量翻了一倍，比摩尔定律每18个月翻一倍的速度快得多。 </p><p>在强大计算力的加持之下，OpenAI模型也得以越炼越大。据透露，GPT-4的尺寸将超过GPT-3的500倍，将拥有100万亿个参数。相比之下，人类大脑有大约 80-1000 亿个神经元和大约 100 万亿个突触，也就是说，下一代AI大模型，参数数量级将堪比人类大脑突触的水平。</p><p>OpenAI 的首席科学家 Ilya Sutskever在2020年表示，“到2021年，语言模型将开始了解视觉世界。仅文字就可以表达关于世界的大量信息，但它是不完整的，因为我们也生活在视觉世界中。”</p><p><strong>这也许是下一代AI大模型最大的看点所在——</strong><strong>其将不仅能处理语言模型，大概率将更是一个能处理语言、视觉、声音等多任务的多模态AI模型</strong>。 </p><p><strong>而这也意味着，AI大模型距离能够多任务处理、会思考的通用人工智能更近了一步。 </strong></p><p>与OpenAI相反，Meta人工智能副总裁罗姆・佩森蒂，掌管着数百名科学家和工程师的资深高管，自始至终对AGI不感兴趣。他认为，人类的智力本身就不是一个统一的问题，更不会有真正的模型能靠自己不断进化智力。“即便是人类都不能让自己变得更聪明。我认为人们对 AGI 的追捧有点像是对某种议程的追捧。”</p><p>反对者可以找到更多的佐证理由。2010年，DeepMind创始人德米斯·哈萨比斯提出了两种接近AGI的方向： </p><p><strong>一是通过描述和编程体系模仿人类大脑的思考体系，但操作难度太大，没有人能描述清楚人脑的结构；</strong></p><p><strong>二是以数字形式复制大脑物理网络结构，但即便是还原大脑物理功能，也无法解释人类思考的运转规则。</strong></p><p>不管是效仿大脑结构，还是试图描述清楚人类智慧的原理，都迈不过“因果关系推理”的鸿沟。迄今为止，没有一个AI模型突破这一难题。 </p><p>AI大模型能带来强人工智能吗？当模型参数一次次被突破，达到远超人脑突触的数量级时，也许会出现突破“因果关系推理”难题的“奇点”，带领我们进入强人工智能时代，但也许这仅仅是一个幻想。</p><p>不过目前，看起来，AI大模型是通往强人工智能最有可能的一条通道。赌一次，值了。</p><p><span class="text-remarks" label="备注">本文来自微信公众号：</span><a href="https://mp.weixin.qq.com/s/YggNrAkdY96S8AK0ZNjTPw" target="_blank" rel="nofollow" style="text-decoration: none;"><span class="text-remarks">偲睿洞察（ID：siruidongcha）</span></a><span class="text-remarks">，作者：蔡凡</span></p>  
</div>
            