
---
title: '可解释AI，如何打开算法的黑箱？'
categories: 
 - 新媒体
 - 虎嗅
 - 首页资讯
headimg: 'https://picsum.photos/400/300?random=8418'
author: 虎嗅
comments: false
date: Thu, 17 Feb 2022 02:26:00 GMT
thumbnail: 'https://picsum.photos/400/300?random=8418'
---

<div>   
<p><span class="text-remarks" label="备注">本文来自微信公众号：</span><a href="https://mp.weixin.qq.com/s/pcm7PZxVD3Zk0RswCyIFdg" target="_blank" rel="nofollow" style="text-decoration: none;"><span class="text-remarks">腾讯研究院（ID：cyberlawrc）</span></a><span class="text-remarks">，作者：腾讯研究院，原文标题：《可解释AI为什么是下一次关键浪潮？》，头图来自：视觉中国</span></p><p>随着以机器学习为代表的新一代人工智能技术不断朝着更加先进、复杂、自主的方向发展，我们的经济和社会发展都纷纷迎来了变革性的机遇。但与此同时，AI算法的透明度、可解释性问题也为公众信任、公共安全等诸多领域带来了前所未有的挑战。</p><p>1月11日～14日，“腾讯科技向善创新周”在线上举办。“透明可解释AI——打开黑箱的理念与实践”专题论坛即聚焦于此。论坛发布了《<a href="https://mp.weixin.qq.com/s?__biz=MjM5OTE0ODA2MQ==&mid=2650954038&idx=1&sn=0ba51f215548267f183359dbc96c1477&chksm=bcc916448bbe9f5240b663c88b5c83e81c77d3b47ff626a79b26f3e94449f4cb8f395e4e5064&scene=21#wechat_redirect" target="_blank" rel="nofollow">可解释AI发展报告2022</a>》，随后由专家学者<span class="text-remarks" label="备注">（见文末）</span>共同参与了圆桌讨论。以下为整理文章：</p><h3 label="大标题" class="text-big-title">可解释AI的概念共识<br></h3><p><strong>姚新：</strong><br></p><p>大家在讨论AI算法的透明性和可解释性的时候，首先应该考虑三个W的问题——Who，What和Why的问题。</p><p>首先，<strong>到底是对谁讲透明和可解释？</strong>因为从科学研究来说，任何一个研究都必须透明，都必须可解释，否则这个论文是发不出来的。所以我猜过去讲透明性和可解释性，可能不是对科学家来说的可解释性或者透明性，因为对科学家的透明性和可解释性，不一定对大众透明和可解释。第二是<strong>解释什么？</strong>解释模型做出来的结果还是解释这个模型的工作原理。第三，解释总是有一个目的，<strong>目的是要追责还是理解这个模型的科学原理。</strong></p><p>根据对这三个W不同的答案，会得出非常不一样的透明性和可解释性，相应的解决办法可能也完全不一样。不管怎样，考虑透明性和可解释性的时候，<strong>首先大家要有一个概念上的共识，</strong>使得我们知道我们是讲同样一件事情，而不是用了同样一个名词，大家在不同的抽象层次讲不同的问题。</p><p><strong>吴保元：</strong></p><p>可解释是可信AI的重要组成部分，是可信的前提条件之一，但是相比于鲁棒性、公平性等可信特性，<strong>我觉得可解释不是独立存在的概念。</strong>就是姚老师刚才提到的，我们到底在解释什么？其他的特性都是有自己明确的数学定义，比如鲁棒性、公平性等，但是可解释性是没有的，因为我们单独提到它的时候，背后默认的更可能是对模型准确度的可解释性。或许这也可以解释为什么当前的可解释研究思路这么多，但是好像没有一个明确的框架，我觉得最主要的原因是它的解释对象不一样，没有办法统一到一起。</p><p>基于这种理解，我个人有一点小的想法，不应该把它称为可解释性，<strong>把它称为可解释力或许更准确。</strong>可解释性，大家可能误认为它是一种独立存在的性质；可解释力是一种可解释的能力，就像我们说的理解力、领导力等等，它是一种手段，一种行为，一种操作存在，需要跟别的绑在一起。我觉得以后提到它的时候，应该准确地描述它是针对什么特性的可解释力，而不是笼统地说可解释性如何。</p><h3 label="大标题" class="text-big-title">可解释AI的价值何在？</h3><p><strong>朱菁：</strong></p><p>人们对于人工智能系统可解释性、透明性的要求，大致有四个层次：</p><p>第一个针对的是直接用户，用户需要了解人工智能产品、服务背后的原理是什么，这是建立可信任AI的重要基础。可解释AI，<strong>实际上支撑了可信任AI。</strong></p><p>第二个层次，对于政策和监管部门，他们希望通过解释原理来了解人工智能产品的<strong>公平性、可问责性</strong>，归因的过程是我们进一步问责、追究责任的基础。所以，可解释AI也与负责任的AI、可问责的AI是联系在一起的。</p><p>第三个层次就是技术工程与科学层次，<strong>我们希望了解为什么某些算法能够成功，</strong>它成功背后的奥秘是什么，它的应用范围是什么，它能否在更大的范围内使用这样一些算法或者是一些技术。</p><p>第四个是公众理解AI，如果社会大众大多数关心的话，他也能够在这方面了解相应的技术、系统大体的工作原理方式是什么。</p><p><strong>何凤翔：</strong></p><p>在现在的AI系统中，其实很多算法背后运作机制是未知的，是不清楚的，这种未知带来了未知的、难以管理的风险，包括安全性、鲁棒性、隐私保护、公平性等等。</p><p>这些点关系到了社会运转中非常关键、人命关天的领域，比如医疗、自动驾驶。这会带来很大的应用方面的困难，以及社会对AI的不信任。因为当AI算法运作机制是未知的时候，它的风险机制、风险大小、风险尺度就是未知的，<strong>我们就难以去管理风险，进而去控制风险。</strong></p><h3 label="大标题" class="text-big-title">可解释AI的挑战何在？<br></h3><p><strong>姚新：</strong><br></p><p>原来我一个学生跟我做了一点关于公平性的工作，跟其他的文献发现的点非常一致，就是说<strong>模型的准确性和公平性之间是相互矛盾的</strong>。性能最好的模型从公平性的角度来说，按指标来测量不见得最好，你要把模型做得都是最公平，用指标来衡量的话，它的性能就会受到损失。实际上可解释性非常类似现在有各版的可解释性指标，但是要真正考虑这些指标的话，模型的性能总是会掉下来，要考虑在实际过程中怎么来找一个折中的方案。</p><p><strong>吴保元：</strong></p><p>针对可解释性本身的不可行、不可取，这也是值得我们思考的问题。比如说我们在研究犯罪率或者说疾病的传播率、发病率等，如果我们就拿现成的统计数据，比如在不同种族、不同地域采集的数据，很有可能会得出来某些种族或者某些地域犯罪率很高，这是因为数据采集的时候就是这样的。这样一来，如果可解释给出的类似结论被公开，可能会造成种族或者地域歧视。但实际上<strong>数据背后是我们在采集的时候没有采集其他特性，</strong>比如说为什么这个地域的传播率很高呢？很有可能是政府投入不足，或者说其他的因素。</p><p>所以这也启发我们可解释性本身它的可信性是什么，它的准确性，它的公平性，它是否忽略了某些特征，或者夸大了某些特征，它的鲁棒性，是不是把样本变化一点，它的可解释性截然相反，这些需要我们进一步思考。</p><p>另外，我跟很多研究可解释的专家聊过，他们的困惑在于<strong>现在的可解释性方法是不可印证的</strong>，甚至是矛盾的，这就引出了可解释性方法本身的可信度的问题。</p><p><strong>何凤翔：</strong></p><p>在我看来，理解深度学习算法的运作机制，大致有理论和实践两条路径。在理论方面，当前的研究无法完全解释理论上泛化性较差的深度模型为何能在多领域取得如此的成功。这种理论与实践的矛盾，就像曾经物理学中的乌云一样，<strong>反映出来了人们对于机器学习理解的缺失，</strong>而这是现在在理论上提升算法可解释性的一个难点。</p><p>而在实验角度上，<strong>很多实验学科中的做法可以作为对于机器学习研究的启发，</strong>比如说物理学、化学，以及刚才提到的医疗。比如说药物研发流程中的合格检验，要做双盲实验；在物理学、化学的研究中，对控制变量实验有严格要求。类似的机制是否能在AI研究中严格执行呢？我觉得这可能是另外一条路径。在我看来，现有的很多对于AI算法的解释是启发式的，而在关键领域中我们需要的是证据，这需要在理论和实验两方面做很多工作。</p><h3 label="大标题" class="text-big-title">可解释AI如何实现？<br></h3><p><strong>朱菁：</strong><br></p><p>前面很多专家都指出对于解释有不同的目标，不同的对象，不同的要求，所以实际上关于人工智能的可解释性问题可能是属于多元性的，<strong>就是要允许有多种不同层次不同方式的解释在这里面起作用，</strong>针对不同的领域、不同的对象，使用不同解释的方式。</p><p>当可解释性有它的局限或者和其他的目标、要求，需要做出权衡取舍的时候，我们想也可以从多个层面来进行替代性的，或者说是补偿性、补充性的策略。比方说针对监管部门，它对于可解释性的要求，和面向公众或者专家层面的，会有所不同，所以这个可以通过若干个层次，比如说监管部门的，行业的，市场的，以及传播普及层面的，对于安全性、鲁棒性要求更高一些，或者在专家层面上有更好的沟通理解，而对于社会公众而言，这里面就需要有一些转换，同时有需要一些权威部门，有公信力的部门，向社会做一些说明和认定。</p><p><strong>姚新：</strong></p><p>深度神经网络可以解决特别复杂的问题，我觉得现在大家用深度网络有一个原因，即所针对的问题本身可能就比较复杂。这是一个假设。假如这个假设是对的话，那么相应的可解释性不会特别好理解。<strong>因为需要对付这些复杂性，相应的模型就必然是要复杂。</strong></p><p>所以我总觉得透明性、可解释性和性能之间是有一个固有的矛盾，如果现在把从技术上讨论的方向，是怎么找一个折中方案，根据不同的场景、可解释的目的，找不同折中方案，这样导致有可能会出来一些比较具体的技术，或者可以促进这些技术往落地的方向走。</p><p><strong>吴保元：</strong></p><p>我们尝试过一些从技术上可行的方案去量化各种可信特性，但是，要实现统一量化很困难，比如说公平性和鲁棒性都有不同的量化准则和指标。当把不同的特性简单组合到一起的时候很难优化，因为它们的准则是高度不对齐的，差异非常大，这就涉及怎么去对齐这些特性坐标。我认为想要找到一个全局坐标系是非常困难的。我们可以<strong>从局部出发</strong>，针对某种场景，比如医疗场景，首先把隐私性当做前提，在金融或者自动驾驶，我们把鲁棒性当做前提，然后再去研究其他特性，或许一步一步能够找到这种坐标系。</p><h3 label="大标题" class="text-big-title">可解释AI的技术现状？<br></h3><p><strong>郑冶枫：</strong></p><p>总体来说，因为我们现在还缺乏非常好的理论框架，所以可能针对问题，我们创造性地想一些算法，试图提高本身这个系统的可解释性，给大家举两个例子来说明一下我们天衍实验室在这方面的探索。</p><p><strong>深度学习</strong>可能有千亿、万亿的参数，这对于医生来说太复杂了，他很难理解这个算法的底层原理，算法本身可能缺乏一个全局的可解释性。但是深度学习框架准确率非常高，所以我们不可能不用。而可解释性非常好的模型就是<strong>回归模型</strong>，这类模型主要的问题就是准确率太低。所以我们做了一个探索，<strong>我们希望把这两个模型结合起来，它具有非常高的准确率，还有一定的可解释性，</strong>不是完全可解释性。</p><p>我们把这个混合模型用于疾病风险预测，就是根据病人历次的就诊记录，我们预测病人在未来6个月之内得某个重大疾病的概率，比如他得卒中的概率。病人每一次的就诊记录包含大量信息，这里面我们需要提取一些跟预测目标相关的重要信息，我们知道生物学习网络最擅长的就是自动特征学习。所以我们利用深度学习网络把一次就诊记录压缩成一个特征的向量，接着我们利用回归模型，把病人多次就诊记录综合起来预测未来6个月之内这个病人得脑卒中的风险。</p><p>这里我们用的是悉数线性回归模型，从病人几十次、上百次过去历年就诊记录里面，选取几次与预测目标最相关的就诊，选择这几例就诊，我们会给它相应的权重。所以这种悉数线性回归模型的可解释性非常好，因为它只关注很少的变量，普通人都能很好理解它，用哪几次就诊记录，做信息加权，得出最后的风险估计。这是一个全局性的可解释性，比深度学习好很多。</p><p><strong>杨强：</strong></p><p>我们在审视各个算法和它对应的可解释性的关联问题上，发现一个有趣的现象，比方说在机器学习里面，深度学习就是属于效率非常高的，但是它却对应的可解释性很差。同样，线性模型没有那么高，但是它的可解释性相对强一些，树状模型也是，因果模型更是这样。所以往往我们确实得做一个取舍，就是我们在可解释这个维度和高效率这个维度，在这个空间里面选择哪一个点，现在并没有在两个维度都高的这样一个算法。</p><h3 label="大标题" class="text-big-title">可解释AI的行业实践<br></h3><p><strong>郑冶枫：</strong><br></p><p>各行业对可解释性和透明性的要求不同，我结合医疗AI这个场景给大家分享一下我的体会和理解。大家知道医疗在全世界范围内都是被强监管的领域，一款医疗产品要上市必须拿到医疗器械注册证，辅助诊断算法AI产品属于三类医疗医疗，也就是监管最严格的级别，所以我们要披露的信息很多，大致包括数据集和临床算法验证两方面。<strong>前者主要强调数据集的公平多样性和广泛覆盖性，后者则重视披露我们的算法真正在临床试验中、真正临床应用的时候它的性能。</strong></p><p>此外，我们的测试样本也需要有很好的多样性，覆盖不同医院，不同区域，不同病人群体、厂商、扫描参数等等。临床实验更加严格，首先我们要固化算法的代码，在临床试验期间是不能改代码的，因为你不能一边做实验一边改代码，这就失去了临床试验的意义。</p><p>所以医疗AI的监管是非常强的，药监局需要我们披露很多信息，提高医疗AI产品的透明性，它有非常严格甚至苛刻的书面要求。因为我们知道智能学习网络天然不具有很好的解释性，虽然你可以做一些中间增强，可以一定程度上改善这些事情，监管也可以理解这个解释性差一点，<strong>正因为解释性差，要求的透明性就越高。</strong></p><p><strong>何凤翔：</strong></p><p>我觉得提供AI系统的说明书有两个路径：<strong>第一个路径从生成AI系统的过程出发。</strong>这一点现在有一些实践，比如开源代码，说明使用了什么数据，数据是如何使用的、如何预处理的。这会提升人们对AI的信任和理解，这也像刚才郑老师提到，申请医疗相关的资质的时候，我们需要把生产细节汇报给相关机构。</p><p><strong>第二种方式就是从生成的AI系统所做出的预测以及决策的指标来入手做算法的说明书。</strong>比方对AI系统做一些测评。对于刚才我们提到的指标，包括可解释性、鲁棒性、准确性、隐私保护、公平性，找到一些比较好的量化指标、找到一些评测算法，把这些指标作为AI系统的使用说明书。</p><h3 label="大标题" class="text-big-title">可解释AI的未来发展<br></h3><p><strong>杨强：</strong>我期待在未来人工智能的治理，在人工智能，人和机器这种和谐共存，共同解决我们要解决问题的前提下，会越来越成熟。我是非常看好这个领域的。</p><p><strong>朱菁：</strong>我期待这个领域进一步的探讨，不同领域的学者都能够参与进来。比如说像我自己做的主要是哲学，<strong>科技哲学。</strong>在科技哲学，实际上对于解释有将近一百年的积累和探索，这里面应该有很多可以发掘借鉴的资源，参与到目前这样一个很有意思很有挑战性的话题里面。</p><p><strong>何凤翔：</strong><strong>AI本身是一个跨学科领域，</strong>它可能会用到很多数学、统计、物理、计算机等各个知识的领域，今天提到的很多点，包括隐私保护、公平性，很多也是来源于人文学科、法律、社会学这些方面。所以这就意味着研究可信AI以及可解释性等等方面会需要各个学科领域的人合作起来一起去做的一件事情，会非常需要大家的通力合作，共同推进这个领域的发展。</p><p><strong>姚新：</strong>对于做研究来说，我希望将来可以有一点聚焦的讨论。我刚才讲的3W，到底我们要解决透明性、可解释性的哪一部分，对谁而言。假如对医疗而言，是对法规的制定者来说还是对医生来说，还是对病人来说，还是对这个系统的开发者来说？我觉得在这里面有非常多可以发挥自己的想象力和能力的地方。</p><p><strong>吴保元：</strong>希望今后的AI研究者具备综合的思考能力，不仅仅关注于某一个特性，比如说关注准确度。<strong>希望把可信当做一个前提条件，</strong>多种特性之间的关联，大家是值得思考的。希望AI的研究者和人文学者多多交流，开拓视野。对于公众和政府来讲，希望通过讨论也可以了解到当前发展的现状，希望有一种包容的心态了解这个领域。</p><p><strong>郑冶枫：</strong>对算法人员来说，当然我们希望将来科学家们找到非常好的，具有良好可解释性，同时准确性非常高的算法，真正做到鱼和熊掌兼得。</p><p><span class="text-remarks" label="备注">论坛主持：腾讯集团副总裁、腾讯研究院总顾问杨健；报告发布人：腾讯研究院秘书长张钦坤、优图实验室人脸识别技术负责人丁守鸿；参与讨论专家：加拿大皇家科学院&加拿大工程院两院院士杨强老师、南方科技大学计算机科学与工程系系主任姚新老师、厦门大学人文学院朱菁院长、腾讯AI Lab顾问吴保元老师、京东探索研究院算法科学家何凤翔老师、天衍实验室负责人郑冶枫老师</span></p><p><span class="text-remarks" label="备注">本文来自微信公众号：</span><a href="https://mp.weixin.qq.com/s/pcm7PZxVD3Zk0RswCyIFdg" target="_blank" rel="nofollow" style="text-decoration: none;"><span class="text-remarks">腾讯研究院（ID：cyberlawrc）</span></a><span class="text-remarks">，作者：腾讯研究院</span></p>  
</div>
            