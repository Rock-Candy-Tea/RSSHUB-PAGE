
---
title: 'AI算法是服务了你，还是操纵了你？'
categories: 
 - 新媒体
 - 虎嗅
 - 首页资讯
headimg: 'https://www.huxiu.com/article/undefined'
author: 虎嗅
comments: false
date: Mon, 12 Jul 2021 08:49:00 GMT
thumbnail: 'https://www.huxiu.com/article/undefined'
---

<div>   
<img alt="AI算法是服务了你，还是操纵了你？" data-v-ea68b0d0 src="https://www.huxiu.com/article/undefined" referrerpolicy="no-referrer"><p><span class="text-remarks" label="备注">本文来自微信公众号：</span><a href="https://mp.weixin.qq.com/s/RFjy2FNt9l_0PLyXGCPHuw" target="_blank" rel="nofollow" style="text-decoration: none;"><span class="text-remarks">高山书院（ID：gasadaxue）</span></a><span class="text-remarks">，作者：Stuart Russell，根据Stuart Russell于2020年9月24~25日在“GMIC 在线 Pro”的演讲内容节选整理而成，头图来自：《机器管家》剧照</span></p><p>去年，我写了一本书，叫《人类兼容：人工智能与控制问题》<span class="text-remarks" label="备注">（Human Compatible: AI and the Problem of Control）</span>。<strong>人类兼容，是什么意思？</strong></p><h3 label="大标题" class="text-big-title">机器反客为主</h3><p>1950年，计算机科学之父<span class="text-remarks" label="备注">（也可以说是人工智能之父）</span>艾伦·图灵，发布了一篇论文，阐述了人工智能的基础概念，包括著名的图灵测试。</p><p>相比之下，较鲜为人知的是他1951年电台上的一场演讲。演讲中他提到：</p><blockquote class="js_blockquote_wrap" data-type="2" data-url data-author-name data-content-utf8-length="73" data-source-title><p>一旦机器掌握了思考方式，它们就会超越人类，人类的力量将显得微不足道，这只是时间上的事……从某个阶段开始，我们必须预想机器掌握了局面的控制权的情形。<br></p></blockquote><p>图灵并没有给出解决方案，仿佛无奈地接受了这个无解的结局。</p><p>为什么早在机器诞生之初，图灵就有这样的忧虑？从今天的技术来看，这是杞人忧天，还是有迹可循?</p><p>今天，人工智能的场景正在迅速拓展。前些年，过去一直被视为人类智慧顶点的围棋被攻克了，接下来，自动驾驶汽车也就快现实化了。沿着这样的趋势发展下去，无疑，AI将持续覆盖更多的领域，做出比人类更好的现实决策。</p><p>进行良好决策的智慧，是过去我们掌握世界的权柄。而<strong>打造AI，本质上，是建立比我们自己更强大的智慧系统。由此，显而易见的一个问题是：我们要如何对一个比自己强大的对象永远保有支配权？</strong></p><p>霍金也曾经在一个社论中说过类似的话：</p><blockquote class="js_blockquote_wrap" data-type="2" data-url data-author-name data-content-utf8-length="49" data-source-title><p>创造出人工智能，将是人类史上最重大的事件，但这不幸，也将是最后一个——除非我们懂得避开当中的陷阱。</p></blockquote><p>但相比图灵，霍金提到了一线生机：避开陷阱。</p><h3 label="大标题" class="text-big-title">算法弄巧成拙</h3><p>人工智能的陷阱是什么？根源在哪里？</p><p>过去我们了解人工智能的方式，我称之为AI标准模型——实际上，这不但是人工智能的标准模型，也是控制理论、运筹学、经济学等的标准模型，更是20世纪绝大部分创新所依赖的模型。</p><p>在这个模型里，我们创建一个机制或制造一台机器，然后指明我们所要达成的目标，从外部植入机器，让它找出优化的解决方案。</p><p>比如自驾汽车。我们先对系统指定好目标：“送我去机场”，然后它靠自己找到最优方案，并执行。</p><p>值得注意的是，这个目标不是人工智能自己设定的，而是我们植入的。但问题是，当我们从简单的实验室场景走入复杂的现实世界，我们往往无法完整而正确地指定好这些目标。</p><p>实际上，这也不是一个新的概念，这是一个几千年的共识。</p><p>在古希腊传说中，迈达斯国王向神明请愿，让他点物成金，而神明成全了他——在这里，神明就像是替我们完成目标的优化机器。故事的结果是他的食物、饮料甚至家人全都变成了金子，而他在饥渴及痛苦中死去。</p><p>歌德的《魔法师的弟子》也是异曲同工：弟子想让扫帚给自己拿水，但一时忘了指明拿多少，结果带来了洪灾；而每一次当他想施法挽救，指示都有失明确，把事态越弄越糟。所幸的是，他最终在法师的帮助下得救。</p><p>在阿拉丁神话里，擦神灯后，人们对精灵许三个愿，第三个愿望总是解除前两个愿望，因为它们带来了自己意想不到却懊悔遗憾的毁灭性结局。</p><p>诸如此类的故事几乎在每一个文化里都有，它不断提醒人们：我们要求的，不一定是我们真正想要的。但不幸的是，在AI的标准模型里，我们要求什么，就将得到什么。</p><p>再看一个重要的现代实例——社交媒体灾难。</p><p>如今社交媒体的筛选算法，很大程度上决定着用户查阅的内容<span class="text-remarks" label="备注">（如新闻、视频、文章等）</span>，占据了亿万人每一天的精力和时间。而这个算法是以最大化点击率、转化率之类的标准为目标的。</p><p>表面上看，这没什么问题。为了达到这个目标，AI会主动学习人们要什么，以便推送我们喜好的内容。但实际上，这不是他们所做的。</p><p>如果我们了解强化学习的算法，它不单是学习人们要什么，而是通过强化反馈去改变世界<span class="text-remarks" label="备注">（即你的大脑）</span>，把你导向更易预测的行为模式。<strong>一旦我们变得更可测，它就更容易对我们推送内容，让我们点击，从而达到它的目标。</strong></p><p>换句话说，<strong>我们想让它调整自己去满足人类，但它却调动了人类来配合自己。</strong></p><p>在这个过程中，即便它不带价值倾向，但对人类的重复强化反馈，无形中也把人引向恐怖主义、暴力意识、新法西斯主义等极端的行为模式。</p><p>这个教训告诉我们：在AI的标准模型以及有误的指定目标下，更强的人工智能反而会带来更坏的结果。更可怕的是，在执行目标的过程中，它不单会介入世界的秩序，还会阻止你反干预。</p><p>从某个角度上来讲，这像是我们以世界的命运作为了赌注，安排了一场和机器的对弈。</p><h3 label="大标题" class="text-big-title">人类悬崖勒马</h3><p>相反，AI的新模型——我称之为可证有益人工智能<span class="text-remarks" label="备注">（Provably Beneficial AI）</span>，在正确的原则下操作可以避开上述的陷阱，成为真正对人类有益的人工智能。</p><p>经典的阿西莫夫机器人三定律提到：</p><ul class=" list-paddingleft-2" style="list-style-type: disc;"><li><p>不伤害定律：机器人不得伤害人类个体，或者目睹人类个体将遭受危险而袖手不管；</p></li><li><p>服从定律：机器人必须服从人给予它的命令，当该命令与第一定律冲突时例外；</p></li><li><p>自保定律：机器人在不违反第一、第二定律的情况下要尽可能保护自己的生存。</p></li></ul><p>遵循这个框架，我们重新定义了当中的三原则：</p><p><strong>第一，AI的唯一目标，是实现人类的价值与偏好。</strong></p><p>这指的不单是“一个人喜欢吃什么、看什么”这些具体单一的偏好，而是包含了你在长远的未来中在乎的所有事项；而且不仅是个人，还可以是全体人类。当然，在个体之间或社会之间，这些价值与偏好会存在巨大的差异。</p><p><strong>第二，AI知道自己并不确定人类的价值与偏好。</strong></p><p><strong>第三，AI会持续以人类每一个做与不做的行为与决定，作为这些价值与偏好的依据。</strong></p><p>不过当然，它不是完美的证据，因为人不是完全理性的，我们的行为不总是吻合某个底层的偏好。</p><p>把这三个原则集合在一起，就形成一个正规的数学框架——称之为辅助游戏<span class="text-remarks" label="备注">（assistance game）</span>，定义出一个问题让机器解答。当我们检视这辅助游戏的数学解，我们发现几点：</p><p>首先，这些机器必然地会服从人类的偏好；其次，当系统处于不确定的时候<span class="text-remarks" label="备注">（比如“为了解决温室效应，是否把海洋变成硫酸”）</span>，在执行前它会主动寻求批准；另外，它会允许自己被关闭——这其实也是人工智能控制问题里最核心的一环，如果这点做不到，就难逃图灵预测的人类结局：“游戏结束”。</p><p>让我以机器人关闭装置问题作为例子进一步阐述。所有的大机器人必须有关闭装置。</p><p class="img-center-box"><img class="lazyImg" _src="https://img.huxiucdn.com/article/content/202107/12/151407276579.png?imageView2/2/w/1000/format/png/interlace/1/q/85" data-w="288" data-h="338" src="https://img.huxiucdn.com/article/content/202107/12/151407276579.png?imageView2/2/w/1000/format/png/interlace/1/q/85" referrerpolicy="no-referrer"></p><p label="图片备注" class="text-img-note">这是著名的PR2机器人<br></p><p>假设我们将一个机器人的目标设置为“到星巴克领取咖啡”。</p><p>根据过去的AI标准模型，为了完成领取咖啡的目标，机器人必须阻止别人将它关闭<span class="text-remarks" label="备注">（因为”一旦死了，我就无法领取咖啡”的因果逻辑）</span>，甚至电击扫除所有妨碍它领取咖啡的人事物。</p><p>在新的Provably Beneficial AI模型下，机器人从你过去的经验了解了你对咖啡的偏好；但它知道它并不确定你可能存在的其他偏好，比如你对咖啡厅里其他人的福利安危，在这样的不确定下，它会以迥然不同的行为模式运作。</p><p>在是否关闭装置的问题上，机器人的设定是：</p><p>（1）它知道当自己违反了人类意愿;</p><p>（2）人类可能把它关掉;</p><p>（3）然而它不确定人类意愿是什么，同时它也知道它不能违反人类意愿;</p><p>（4）所以，它有充分的意愿和动力让人类将它关闭。</p><p>如果把这4项转化成数学的标准范式<span class="text-remarks" label="备注">（如下图的希腊文）</span>，可以严谨论证出得出：这种设计模型的机器人是可证地有益<span class="text-remarks" label="备注">（provably beneficial）</span>的。</p><p class="img-center-box"><img class="lazyImg" _src="https://img.huxiucdn.com/article/content/202107/12/151408493918.png?imageView2/2/w/1000/format/png/interlace/1/q/85" data-w="480" data-h="308" src="https://img.huxiucdn.com/article/content/202107/12/151408493918.png?imageView2/2/w/1000/format/png/interlace/1/q/85" referrerpolicy="no-referrer"></p><p label="图片备注" class="text-img-note">（严谨证明，可以查看相关的论文：https://people.eecs.berkeley.edu/~dhm/papers/off_switch_AAAI_ws.pdf）<br></p><h3 label="大标题" class="text-big-title">AI何去何从？</h3><p>对人类而言，这无疑是佳音。在这个基础上，其实有非常多相关研究已经完成或正在进行，但也还有一些有待我们继续努力，把这个新模型付诸实现。</p><p>其中一个需要跟进的方向，是<strong>怎么让人工智能从代表一个人，延伸到代表很多人，甚至一个社会、多个社会。</strong>实际上，这也是一个穿越千年的哲学命题，从公元前五世纪中国的墨子，到古希腊柏拉图、亚里士多德，再到边沁与密尔的功利主义理论，以及现代经济学等等。</p><p><strong>另一个需要解决的问题，在于人类的不完全理性。</strong>像前面所说，人类的表象行为，不总是体现我们的深层偏好。</p><p>比如李世石在和阿尔法狗的对弈中，就频频下出非最佳下法。如果系统假设他是理性的，那么系统就必须推断他想输；但事实上这是因为他的计算力并不完善。</p><p>总的来说，不管是谁，人类在各方各面都存在认知上的限制。所以，新模型若想透过表象行为去了解人类的深层偏好，就需要转向认知心理学、神经科学等学科，建立一套人类认知模型。</p><p>另外一个有待完成的工作，是这套模型的技术基础。从标准模型转向新模型，我们需要重建或重修每一个AI层面和分支，各种定理、定义、算法、语言处理、强化学习、搜索、策划。</p><p>最后一个值得深耕的领域，是现实应用。这也是最能体现新模型价值的地方，从而带来后续更多的资金与精力投入研究。比如自动驾驶汽车，这项应用就特别要求AI能同时了解乘客的偏好以及其他公路使用者的行为，从而判断自己做出什么反应。其他正酝酿中的应用，还有个人助理、数码机器人等等。</p><h3 label="大标题" class="text-big-title">未解的难题</h3><p>关于人工智能与人类的前景，很多人可能也想到了其他的问题，如：</p><p label="小标题" class="text-sm-title">误用</p><p>比如《邪恶博士》里，歹徒抱着支配世界、操控世界的动机去开发人工智能。在如今已经棘手的网络犯罪之上，这是另一个更棘手的难题。因为它涉及到人工智能的失控，以及人类的未来。</p><p label="小标题" class="text-sm-title">滥用</p><p>比如《机器人总动员》里，AI代替了人类打理社会的运作，人类失去了动力去理解和发展自己的文明，成了一艘永远前进的游轮上的乘客。在机器持续为我们分越来越多忧、解越来越多难的背景下，人类怎样能保持自己文化与智力上的活力？</p><p>这两个问题暂时算是无解，也期待大家集思广益。</p><p><span class="text-remarks" label="备注">本文来自微信公众号：</span><a href="https://mp.weixin.qq.com/s/RFjy2FNt9l_0PLyXGCPHuw" target="_blank" rel="nofollow" style="text-decoration: none;"><span class="text-remarks">高山书院（ID：gasadaxue）</span></a><span class="text-remarks">，作者：Stuart Russell （加州大学伯克利分校计算机科学教授，人工智能领域的重量级学者，他与Peter Norvig 合著的《人工智能：一种现代方法》被全球超千所大学选择为标准教科书，对高速发展的人工智能技术面临的机遇和挑战有着自己独到的见解。）</span></p>  
</div>
            