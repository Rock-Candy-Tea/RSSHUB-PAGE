
---
title: 'Apple already scans iCloud Mail for CSAM, but not iCloud Photos'
categories: 
 - 新媒体
 - 9To5
 - 9To5 分站
headimg: 'https://i0.wp.com/9to5mac.com/wp-content/uploads/sites/6/2021/08/Apple-scans-iCloud-Mail-for-CSAM-1.jpg?resize=1200%2C628&quality=82&strip=all&ssl=1'
author: 9To5
comments: false
date: Mon, 23 Aug 2021 11:43:08 GMT
thumbnail: 'https://i0.wp.com/9to5mac.com/wp-content/uploads/sites/6/2021/08/Apple-scans-iCloud-Mail-for-CSAM-1.jpg?resize=1200%2C628&quality=82&strip=all&ssl=1'
---

<div>   
<img src="https://i0.wp.com/9to5mac.com/wp-content/uploads/sites/6/2021/08/Apple-scans-iCloud-Mail-for-CSAM-1.jpg?resize=1200%2C628&quality=82&strip=all&ssl=1" referrerpolicy="no-referrer">
<p>Apple has confirmed to me that it already scans <a href="https://9to5mac.com/guides/apple-mail/" target="_blank" rel="noreferrer noopener">iCloud Mail</a> for <a href="https://9to5mac.com/guides/csam/" target="_blank" rel="noreferrer noopener">CSAM</a>, and has been doing so since 2019. It has not, however, been scanning <a href="https://9to5mac.com/guides/photos/" target="_blank" rel="noreferrer noopener">iCloud Photos</a> or iCloud backups.</p>
<p>The clarification followed me querying a rather odd statement by the company’s anti-fraud chief: that Apple was “<a href="https://9to5mac.com/2021/08/20/apples-anti-fraud-chief-child-porn-statement/" target="_blank" rel="noreferrer noopener">the greatest platform for distributing child porn</a>.” That immediately raised the question: If the company wasn’t scanning iCloud photos, how could it know this?</p>
<p><span id="more-746344"></span>
</p>
<p>There are also a couple of other clues that Apple had to have been doing <em>some</em> kind of CSAM scanning. <a href="https://web.archive.org/web/20200110193302/https://www.apple.com/legal/child-safety/en-ww/index.html" target="_blank" rel="noreferrer noopener">An archived version</a> of Apple’s child safety page said this (emphasis ours):</p>
<blockquote class="wp-block-quote">
<p>Apple is dedicated to protecting children throughout our ecosystem wherever our products are used, and we continue to support innovation in this space. We have developed robust protections at all levels of our software platform and throughout our supply chain. As part of this commitment, <strong>Apple uses image matching technology to help find and report child exploitation</strong>. Much like spam filters in email, <strong>our systems use electronic signatures</strong> to find suspected child exploitation. We validate each match with individual review. Accounts with child exploitation content violate our terms and conditions of service, and any accounts we find with this material will be disabled.</p>
</blockquote>
<p>Additionally, the company’s chief privacy officer <a href="https://www.telegraph.co.uk/technology/2020/01/08/apple-scans-icloud-photos-check-child-abuse/" target="_blank" rel="noreferrer noopener">said the same thing</a> back in January 2020:</p>
<blockquote class="wp-block-quote">
<p>Jane Horvath, Apple’s chief privacy officer, said at a tech conference that the company uses screening technology to look for the illegal images. The company says it disables accounts if Apple finds evidence of child exploitation material, although it does not specify how it discovers it.</p>
</blockquote>
<p>Apple wouldn’t comment on Friedman’s quote, but they did tell me that the company has never scanned iCloud Photos.</p>
<h2 id="h-apple-scans-icloud-mail">Apple scans iCloud Mail</h2>
<p>However, Apple confirmed to me that it has been scanning outgoing and incoming iCloud Mail for CSAM attachments since 2019. Email is not encrypted, so scanning attachments as mail passes through Apple servers would be a trivial task. </p>
<p>Apple also indicated that it was doing some limited scanning of other data, but would not tell me what that was, except to suggest that it was on a tiny scale. It did tell me that the “other data” does not include iCloud backups.</p>
<p>Although Friedman’s statement sounds definitive – like it’s based on hard data – it’s now looking likely that it wasn’t. It’s our understanding that the total number of reports Apple makes to CSAM each year is measured in the hundreds, meaning that email scanning would not provide any kind of evidence of a large-scale problem on Apple servers.</p>
<p>The explanation probably lays in the fact that other cloud services were scanning photos for CSAM, and Apple wasn’t. If other services were disabling accounts for uploading CSAM, and iCloud Photos wasn’t (because the company wasn’t scanning there), then the logical inference would be that more CSAM exists on Apple’s platform than anywhere else. Friedman was <em>probably</em> doing nothing more than reaching that conclusion.</p>
<p>The <a href="https://9to5mac.com/guides/csam/" target="_blank" rel="noreferrer noopener">controversy</a> over Apple’s CSAM plans continues, with two Princeton academics stating that they prototyped a scanning system based on exactly the same approach as Apple, but abandoned the work <a href="https://9to5mac.com/2021/08/20/apples-csam-system-is-dangerous/" target="_blank" rel="noreferrer noopener">due to the risk of governmental misuse</a>.</p>
<div class="ad-disclaimer-container"><p class="disclaimer-affiliate"><em>FTC: We use income earning auto affiliate links.</em> <a href="https://9to5mac.com/about/#affiliate">More.</a></p><div class="wp-block-image"><figure class="aligncenter size-full is-resized"><a href="https://bit.ly/3D6NF4W"><img src="https://9to5google.com/wp-content/uploads/sites/4/2021/08/Ivacy-native-banner.png" alt class="wp-image-447416" width="750" height="150" referrerpolicy="no-referrer"></a></figure></div></div><div id="after_disclaimer_placement"></div>
<!-- youtube embed -->
  
</div>
            