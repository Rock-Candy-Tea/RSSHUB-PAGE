
---
title: 'Apple says any expansion of CSAM detection outside of the US will occur on a per-country basis'
categories: 
 - 新媒体
 - 9To5
 - 9To5 分站
headimg: 'https://i2.wp.com/9to5mac.com/wp-content/uploads/sites/6/2020/11/apple-park-header.jpeg?resize=1200%2C628&quality=82&strip=all&ssl=1'
author: 9To5
comments: false
date: Fri, 06 Aug 2021 17:33:01 GMT
thumbnail: 'https://i2.wp.com/9to5mac.com/wp-content/uploads/sites/6/2020/11/apple-park-header.jpeg?resize=1200%2C628&quality=82&strip=all&ssl=1'
---

<div>   
<img src="https://i2.wp.com/9to5mac.com/wp-content/uploads/sites/6/2020/11/apple-park-header.jpeg?resize=1200%2C628&quality=82&strip=all&ssl=1" referrerpolicy="no-referrer">
<p>Apple’s new feature for detection of Child Sexual Abuse Material (CSAM) content in iCloud Photos will launch first in the United States, as <a href="https://9to5mac.com/2021/08/05/apple-announces-new-protections-for-child-safety-imessage-safety-icloud-photo-scanning-more/"><em>9to5Mac </em>reported yesterday</a>. Apple confirmed today, however, that any expansion outside of the United States will occur on a country-by-country basis depending on local laws and regulations. </p>
<p><span id="more-742919"></span>
</p>
<p>As <em>9to5Mac </em>reported yesterday, the new feature will allow Apple to detect known CSAM images when they are stored in iCloud Photos. The feature has faced considerable pushback from certain sources, but Apple promises that this is necessary to protect children and that everything is being done with privacy in mind. </p>
<p>While one of the most common concerns has been on what might happen if other governments try and take advantage of this system for other purposes, the feature will only be available in the United States at launch. The concerns have come from a variety of notable sources, such <a href="https://twitter.com/snowden/status/1423469854347169798?s=21">as Edward Snowden</a> and the <a href="https://www.eff.org/deeplinks/2021/08/apples-plan-think-different-about-encryption-opens-backdoor-your-private-life">Electronic Frontier Foundation</a>.</p>
<p>The EFF wrote: </p>
<blockquote class="wp-block-quote">
<p>All it would take to widen the narrow backdoor that Apple is building is an expansion of the machine learning parameters to look for additional types of content, or a tweak of the configuration flags to scan, not just children’s, but anyone’s accounts. That’s not a slippery slope; that’s a fully built system just waiting for external pressure to make the slightest change.</p>
</blockquote>
<p>Apple confirmed to <em>9to5Mac </em>today that any expansion of the CSAM detection feature outside of the United States will take place on a country-by-country basis depending on local laws and regulations. The company did not provide a specific timetable on when, or if, it will expand CSAM detection to additional countries. </p>
<p>Apple traditionally launches features first in the United States because the US is the company’s largest market and the market in which it is most familiar with local laws and regulations. That is again the case with the new CSAM detection system. </p>
<p>Apple’s implementation of this CSAM detection feature is highly technical, and more details can be learned at the links below. </p>
<ul>
<li><a href="http://www.apple.com/child-safety/pdf/CSAM_Detection_Technical_Summary.pdf">Detailed technical summary for CSAM detection</a></li>
<li><a href="http://www.apple.com/child-safety/pdf/Technical_Assessment_of_CSAM_Detection_Benny_Pinkas.pdf">Technical Assessment of CSAM Detection by Professor Benny Pinkas </a></li>
<li><a href="https://www.apple.com/child-safety/">New Child Safety Landing Page on Apple.com </a></li>
</ul>
<div class="ad-disclaimer-container"><p class="disclaimer-affiliate"><em>FTC: We use income earning auto affiliate links.</em> <a href="https://9to5mac.com/about/#affiliate">More.</a></p><figure class="wp-block-image size-full is-resized"><a href="https://bit.ly/3C0YHbr"><img src="https://9to5mac.com/wp-content/uploads/sites/6/2021/08/9to5mac_promo-01.png" alt class="wp-image-742121" width="750" height="150" referrerpolicy="no-referrer"></a></figure></div><div id="after_disclaimer_placement"></div>
<!-- youtube embed -->
  
</div>
            