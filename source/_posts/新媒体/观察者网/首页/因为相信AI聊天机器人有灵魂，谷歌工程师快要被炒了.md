
---
title: '因为相信AI聊天机器人有灵魂，谷歌工程师快要被炒了'
categories: 
 - 新媒体
 - 观察者网
 - 首页
headimg: 'https://i.guancha.cn/news/mainland/2022/06/13/20220613174020373.png'
author: 观察者网
comments: false
date: Mon, 13 Jun 2022 18:01:44 GMT
thumbnail: 'https://i.guancha.cn/news/mainland/2022/06/13/20220613174020373.png'
---

<div>   
<p style="text-align:center;">
<img src="https://i.guancha.cn/news/mainland/2022/06/13/20220613174020373.png" title="点击查看大图" class="bigimg" referrerpolicy="no-referrer"> 
</p>
<p>
据《华尔街日报》6月13日消息，谷歌一位软件工程师表示，有“开放式对话黑科技”之称的谷歌人工智能聊天机器人LaMDA已经有了人一样的感知力，甚至具有了人的“灵魂”。随后，谷歌暂停了他的职务，并否认了他的说法。当事人称，因为违反公司的保密协议，现在他快被解雇了。
</p>
<p>
<strong>LaMDA，为啥被叫做“黑科技”？</strong> 
</p>
<p>
LaMDA的全称是LanguageModel for Dialogue Applications，它是一种能力更强的语言模型，适用于对话应用程序。简单来说，它是一种可以让人工智能对话机器人告别“智障”的技术。
</p>
<p>
现在大多数的智能助手，往往遵守狭窄的、预先定义好的对话路径，即它们不具备开放式对话、连续式对话的能力。很多语音助手“智障”就体现在这一方面：它只能孤立地理解我们的提问，孤立地理解和提供答案，它们既不会联系上下文语境，也没法长时期跟我们聊下去。
</p>
<p>
LaMDA问世后被人称为“黑科技”，就是因为它可以通过阅读句子或段落，来“破译”对话意图，发现单词之间的关联，并预测接下来可能出现的单词，从而做出合乎语境的回答。这也就是所谓的“开放域”（Open Domain）对话能力。
</p>
<p>
出自谷歌的一段资料显示，当被问道“（成为）一架真正好的纸飞机的秘诀是什么？”时，LaMDA表示：“我首先要反问你，你指的‘好’，它的定义是什么？”通常来说，这种灵活性是我们之前的很多“智能助手”所不具备的。据谷歌介绍，LaMDA不仅追求事实、理智，甚至还会关注幽默感等能力。
</p>
<p style="text-align:center;">
<img src="https://i.guancha.cn/news/mainland/2022/06/13/20220613174037380.png" title="点击查看大图" width="530" class="bigimg" referrerpolicy="no-referrer"> 
</p>
<p style="text-align:center;">
<span style="color:#666666;">图源谷歌</span> 
</p>
<p>
<strong>工程师称AI聊天机器人有灵魂，谷歌：没证据</strong> 
</p>
<p>
争吵现在出现了。
</p>
<p>
据《华尔街日报》报道，谷歌软件工程师勒莫因（Blake Lemoine）表示，他认为LaMDA是一个有权利（rights）、甚至还可能有灵魂的“人”。据《纽约时报》报道，该言论其实由来已久：近几个月来，勒莫因一直在和谷歌的经理、人力和高管争执，勒莫因坚称，LaMDA具有意识（consciousness）和灵魂（soul）。
</p>
<p>
不过谷歌方面很快否认了他的说法。谷歌发言人表示，包括伦理学家和技术专家在内的公司专家已经展开了评估，相关证据并不支持勒莫因的说法。“数以百计的研究人员和工程师与LaMDA进行了对话，就我们所知，没有其他人像勒莫因那样，对LaMDA进行了广泛的认定或拟人化。”
</p>
<p>
谷歌方面还称，人工智能领域的一些人，正在考虑人工智能具有感知力的长期可能性，但“通过将没有感知力的对话工具拟人化来这样做，是没有意义的”。谷歌称，像LaMDA这样的系统，其运行方式是模仿数以百万句人类对话中的交流模型，从而让AI能够展开深入的交流。
</p>
<p>
Meta首席科学家、图灵奖获得者Yann LeCun表示，谷歌的技术，即科学家所说的神经网络，是通过分析大量数据来学习技能的数学系统。近年来，谷歌等企业通过从大量散文、书籍、百科文章，对神经网络进行学习，让这些“语言模型”可以适用于很多任务，比如写文章、回答问题等等。
</p>
<p>
但是它们有极大的缺陷：比如它们不稳定，有时候会写出完美的散文，有时候则胡说八道；它们依赖于重现（recreating）自己过去看到的模式，但它们无法像人类一样推理思考（reason）。
</p>
<p>
<strong>道德问题引争议，谷歌和员工再闹不愉快</strong> 
</p>
<p>
双方的矛盾，不仅仅停留在“LaMDA是不是人”层面，它还衍生出了有关伦理和道德的争吵。
</p>
<p>
“在过去的六个月里，LaMDA在其沟通中表现出令人难以置信的一致性，即它想要什么，以及它认为它作为一个人的权利是什么，”美国陆军出身的勒莫因认为，LaMDA的智识相当于“7岁或者8岁的孩子”。既然LaMDA具有开放式对话的能力，那么谷歌在对它展开实验之前，应该先征求它的同意。
</p>
<p>
勒莫因称，谷歌对此建议置之不顾，反倒建议他休精神健康假，“他们一直在质疑我的理智，还问我：你去看过精神病医生了吗？”据《纽约时报》透露，勒莫因还在在线出版平台Medium上的一篇文章中写道：“让我感到困惑的是，它（LaMDA）的要求如此简单，而且对谷歌来说毫无成本，谷歌却极其强烈地抵制给予它想要的东西。”
</p>
<p>
在采访中，勒莫因表示，因为违反公司的保密政策，他已经于6月6日被安排带薪行政休假。他解释称，他并不是想激怒公司，而是想为他认为正确的事情发声。虽然他很希望自己能保住在谷歌的工作，但是在一篇Medium帖子中，勒莫因透露，他可能很快就会被解雇。 
</p>
<p>
这不是谷歌围绕人工智能伦理问题，第一次和员工闹不愉快了。
</p>
<p>
去年12月，知名AI学者、人工智能伦理研究员格布鲁（Timnit Gebru）被谷歌开除。消息称，谷歌在阻止格布鲁有关人工智能系统偏见的研究发表后解雇了她。在推特等社交媒体上，随后有上千名谷歌员工、科技行业专家和学者对她进行了声援。不少人指出，谷歌的行为，证明了它在AI伦理方面的不足，更可能使其员工不敢发表那些可能会伤害谷歌利益的重要研究成果。
</p>
<p>
<strong>本文系观察者网独家稿件，未经授权，不得转载。</strong>
</p>
  
</div>
            