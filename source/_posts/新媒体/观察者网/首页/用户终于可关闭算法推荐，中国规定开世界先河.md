
---
title: '用户终于可关闭算法推荐，中国规定开世界先河'
categories: 
 - 新媒体
 - 观察者网
 - 首页
headimg: 'https://i.guancha.cn/news/mainland/2022/02/07/20220207132610173.png'
author: 观察者网
comments: false
date: Thu, 03 Mar 2022 07:57:15 GMT
thumbnail: 'https://i.guancha.cn/news/mainland/2022/02/07/20220207132610173.png'
---

<div>   
<p>
【文/ 观察者网专栏作者 熊节】
</p>
<p>
手机上的App会“读心术”早已不是秘密了。很多人有过这样的体验：刚和朋友聊天说到什么想买什么，转眼就在某App里看见了这件商品；刚浏览了几条某个主题的新闻，突然某App里就全是关于这件事的回答。
</p>
<p>
长期来，许多网民对这种情况感到不满，甚至有点恐惧。
</p>
<p>
如今，一个开关“千呼万唤始出来”，在许多常用App的“设置”页，您应该能找到“关闭个性化推荐”之类的选项。
</p>
<p>
笔者找到了“知乎”和“淘宝”的相关开关并亲测：
</p>
<p align="center">
<img src="https://i.guancha.cn/news/mainland/2022/02/07/20220207132610173.png" title="点击查看大图" class="bigimg" width="530" referrerpolicy="no-referrer"> 
</p>
<p>
为何各个常见的App悄悄给出了关闭个性化推荐的选项？原因是今年1月4日正式公布的《互联网信息服务算法推荐管理规定》（下文简称《规定》）中有这么一条：
</p>
<blockquote class="content-quote">
<p>
第十七条 算法推荐服务提供者应当向用户提供不针对其个人特征的选项，或者向用户提供便捷的关闭算法推荐服务的选项。用户选择关闭算法推荐服务的，算法推荐服务提供者应当立即停止提供相关服务。
</p>
</blockquote>
<p>
这份由国家互联网信息办公室、工业和信息化部、公安部、国家市场监督管理总局联合发布的《规定》将于今年3月1日正式起施行。
</p>
<p>
据笔者所知，这是全世界第一个对算法推荐行为加以具体约束的法案。
</p>
<p>
欧洲的《数字市场法案》（DMA）中也有“不允许利用数据优势向用户投放指向性广告，除非获得用户明确许可”的约定，但DMA去年11月才获得欧洲议会内部市场委员会通过，与欧洲各国政府的谈判还未启动，完成立法仍有一段距离。
</p>
<p>
而中国各大互联网企业已经在以实际行动迎接《规定》的落地。
</p>
<p>
<strong>算法推荐的潜在危害</strong> 
</p>
<p>
2006年，哈佛大学凯斯·桑斯坦教授在《信息乌托邦》一书中提出，人类社会存在一种“信息茧房”现象。他认为，在信息传播中人们自身的信息需求并非全方位的，只会注意选择自己想要的、能使自己愉悦的信息，久而久之接触的信息就越来越局限，就像蚕吐出来的丝一样，细细密密地把自己包裹起来，最终将自己桎梏在“信息茧房”内，失去接触和了解不同观念的机会。
</p>
<p>
算法推荐则有可能强化信息茧房效应：你越是对某种事物感兴趣、倾向于某种观念，算法就会越是给你推荐关于这种事物、支持这种观念的材料，让你不断强化自己的兴趣和倾向。而且，算法推荐还可能被有目的性地引导人群，从而影响公众观念，甚至影响政治决策。因此，美国数学家凯西·奥尼尔在《算法霸权》一书中将推荐算法称作“数学大杀器”（weapons of math destruction）。在过去几年中，这件“大杀器”已经在现实世界中多次产生效果。
</p>
<p>
2016年，在支持特朗普竞选的“阿拉莫项目”（Project Alamo）中，来自脸书、谷歌、推特等几个主要社交网络平台的顾问在圣安东尼奥的同一间办公室并肩工作，在数字广告上投放了大约9千万美元。阿拉莫项目采用了精妙的算法推荐技术来影响选民：当一位互联网用户被识别为“关键选民”（例如摇摆州的摇摆县的摇摆选民），社交网络就会给这样的用户定向投放具有引导性的内容，从而用相对不多的经费影响竞选结果。
</p>
<p>
就在美国大选前几个月，英国的剑桥分析（Cambridge Analytica）公司使用来自脸书的用户数据操纵了英国脱欧（Brexit）公投，令脱欧派意外获胜——与特朗普意外当选如出一辙。
</p>
<p>
在正式用于影响英美政局之前，类似的手段已经在多个发展中国家做过实验。2010年，在特立尼达和多巴哥，一场起源于脸书的“Do So”运动让大批非洲裔选民拒绝投票，从而使印度裔主导的联合民族大会（UNC）在大选中受益。2015年，部分尼日利亚用户在脸书上看到暴力血腥、仇视穆斯林的视频短片，其目的是恐吓选民、操纵选举。算法推荐一旦被滥用，真的可以成为“大杀器”。
</p>
<p>
即使不是被故意滥用，算法推荐也可能暗含社会的偏见和歧视。去年10月，推特的推荐算法被发现“无意中放大了右翼团体内容的传播”：政治右翼当选官员发布的推文在算法上被放大的程度超过政治左翼；右翼媒体比左翼媒体的影响力更大。
</p>
<p>
更早之前，职场社交网站领英的搜索算法（也可以看作一种形式的推荐算法：根据搜索关键词推荐“最匹配”的内容）被发现存在性别歧视，男性求职者会被放在更高的位置。谷歌的广告平台AdSense被发现存在种族偏见，如果搜索关键词看起来像是黑人的名字，AdSense就会有更大概率推荐犯罪记录查询相关的广告。
</p>
<p>
因为算法推荐有这些潜在危害的风险，欧美一些研究者很早就提出对推荐算法加以管制。本次《规定》中要求的算法机制机理审核、科技伦理审查、允许用户关闭算法推荐等措施，在国外都早有人提过建议。然而国际互联网大厂从未将这些建议落地，还经常辩称“基于深度学习的算法无法被审核”。为了帮助读者理解《规定》的重要意义，笔者将简要介绍算法推荐背后的技术原理。
</p>

  
</div>
            