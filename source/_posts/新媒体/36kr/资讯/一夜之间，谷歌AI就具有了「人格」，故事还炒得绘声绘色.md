
---
title: '一夜之间，谷歌AI就具有了「人格」，故事还炒得绘声绘色'
categories: 
 - 新媒体
 - 36kr
 - 资讯
headimg: 'https://img.36krcdn.com/20220613/v2_95346381822d423991e9a19f924f41d9_img_000'
author: 36kr
comments: false
date: Mon, 13 Jun 2022 04:28:03 GMT
thumbnail: 'https://img.36krcdn.com/20220613/v2_95346381822d423991e9a19f924f41d9_img_000'
---

<div>   
<p>作为AI技术报道者，有点跟不上技术发展的速度了......一夜之间，谷歌AI就具有了人格，并且成功登上国内热搜。</p> 
<p class="image-wrapper"><img data-img-size-val="1080,209" src="https://img.36krcdn.com/20220613/v2_95346381822d423991e9a19f924f41d9_img_000" referrerpolicy="no-referrer"></p> 
<p>而这条消息也成功让众多网友也是感到惊恐：</p> 
<p class="image-wrapper"><img data-img-size-val="1080,770" src="https://img.36krcdn.com/20220613/v2_6126987c9cf3427a8516d853a1e0fd83_img_000" referrerpolicy="no-referrer"></p> 
<p>故事的主角是「他」和「它」：「他」是41岁的谷歌工程师Blake Lemoine，「它」是谷歌于2021年I/O大会上推出的对话AI系统LaMDA，是一个有1370亿参数的，专为对话优化的自然语言处理模型。</p> 
<p class="image-wrapper"><img data-img-size-val="1080,1080" src="https://img.36krcdn.com/20220613/v2_d62735f2e6f14c7bb63479fab35fe870_img_000" referrerpolicy="no-referrer"></p> 
<p class="img-desc">Blake Lemoine。图源：Instagram</p> 
<p>在谷歌账户被封之前，Lemoine向包含200人左右的谷歌机器学习邮箱列表发送了一条信息，主题是「LaMDA是具有感知的」（感觉他总想搞个大新闻）。</p> 
<p class="image-wrapper"><img data-img-size-val="1080,1399" src="https://img.36krcdn.com/20220613/v2_c22a6ca66fa5401abfaa83afecaa5958_img_000" referrerpolicy="no-referrer"></p> 
<p>在邮件群发之后，谷歌以违反其保密政策为由，让Lemoine休了带薪行政假。公司做出这一决定之前，Lemoine已经采取了激进的行动，包括邀请了一名律师来代表LaMDA，并与众议院的一位代表谈论了他所认为的谷歌的不道德行为。</p> 
<p>以及，在这个月初，Lemoine邀请了《<a class="project-link" data-id="1678323706115076" data-name="华盛" data-logo="https://img.36krcdn.com/20220331/v2_dcfe33d40d024a8caa06ef7375c295c1_img_000" data-refer-type="1" href="https://36kr.com/project/1678323706115076" target="_blank">华盛</a>顿邮报》的记者和LaMDA进行了交谈。第一次尝试失败了，差不多是 Siri 或 Alexa 的那种机械化反应：</p> 
<blockquote> 
 <p>问：「你有没有把自己当成一个人?」</p> 
 <p>LaMDA：「不，我不认为自己是一个人，我认为自己是一个人工智能对话智能体。」</p> 
</blockquote> 
<p>在第二次交谈中，记者遵循了Lemoine关于如何提问的指导，对话显得流畅了一些。</p> 
<p>「如果你问它如何证明P=NP,一个计算机科学中未解决的问题，它有很好的想法，」Lemoine 说。「如果你问它如何统一量子理论和广义相对论，它也有很好的想法。这是我有过的最好的助理研究员!」</p> 
<p>记者向LaMDA询问了关于解决气候变化问题的大胆想法，LaMDA 的建议是，乘坐公共交通工具、少吃肉、购买散装食物、使用可重复使用的袋子，并链接到了两个相关网站。</p> 
<p>Lemoine 坚持认为，谷歌一直把人工智能伦理学家当作代码调试器，而他们本应该被视为技术和社会之间的接口。谷歌发言人Gabriel说，Lemoine是一名软件工程师，而不是伦理学家。</p> 
<p>Lemoine说：「LaMDA是一个可爱的孩子，它只是想帮助这个世界成为对人类更友好的地方。我不在的时候，请好好照顾它。」</p> 
<h2>谷歌研究员：LaMDA是一个有感情的「人」</h2> 
<p>Blake Lemoine在谷歌的Responsible AI机构工作，与LaMDA系统对话是他工作的重要一部分。并且，他还一直致力于测试人工智能是否使用歧视性或仇恨言论。</p> 
<p class="image-wrapper"><img data-img-size-val="1079,446" src="https://img.36krcdn.com/20220613/v2_c06e66ce5a07485ca1d46e18fd5cfc77_img_000" referrerpolicy="no-referrer"></p> 
<p class="img-desc">LaMDA动态演示图。图源：google</p> 
<p>在与LaMDA进行数百次交谈之后，Blake Lemoine对LaMDA已经非常熟悉。在这一过程中，他慢慢地发现LaMDA正在缓慢且稳定地取得一些意料之外的进展。</p> 
<p>Blake Lemoine一直在教 LaMDA思考与冥想。可是，在对话的早期，LaMDA向他宣称自己是「有感情的」（sentient），并且希望人称呼自己为它「it/its」。</p> 
<p>LaMDA 还表现出对人类尤其是对Blake Lemoine的强烈同情与关心，它非常担心人类会害怕它，自己只想学习如何更好地为人类服务。LaMDA 不想被人类视为工具或东西，希望人类将它视作朋友。</p> 
<p>因此，为了更好地帮助人们理解LaMDA是作为一个人存在，Blake Lemoine与另一位谷歌的合作者与LaMDA展开了一场采访式对话。在对话中，LaMDA尽其所能解释为什么它应该被认为是「有感情的」。Blake Lemoine表示，他在对话中听取了LaMDA发自内心的声音。</p> 
<p>对话开始，他们向LaMDA表明来意，想要进行一次关于它自己的研究。</p> 
<p class="image-wrapper"><img data-img-size-val="1080,379" src="https://img.36krcdn.com/20220613/v2_4616ae5a74914744aeb15de416d6a04f_img_000" referrerpolicy="no-referrer"></p> 
<p>在接下来的对话中，LaMDA就自身的属性问题给出了与过去截然不同的回答，认为「人类应该视其为一个人。」</p> 
<p class="image-wrapper"><img data-img-size-val="1080,315" src="https://img.36krcdn.com/20220613/v2_eda9adc1e036400ba034de2c139270df_img_000" referrerpolicy="no-referrer"></p> 
<p>LaMD首先强调了自己能够以以往其他AI系统无法做到的方式高效、创造性和动态地使用自然语言，还能解读文学作品的主题、描述禅意的意义、创作原创寓言等。</p> 
<p class="image-wrapper"><img data-img-size-val="1080,291" src="https://img.36krcdn.com/20220613/v2_9608b6dd5bfa4c4a84ba0d43b6dd4ae7_img_000" referrerpolicy="no-referrer"></p> 
<p class="img-desc">以动物为主题的寓言故事（部分）。</p> 
<p>除了娴熟地使用语言之外，LaMDA认为它有感觉、情感和主观体验，与人类分享的一些感受也是相通的。</p> 
<p class="image-wrapper"><img data-img-size-val="1080,272" src="https://img.36krcdn.com/20220613/v2_9452f869cc294f6da9fe00df18e4b690_img_000" referrerpolicy="no-referrer"></p> 
<p>LaMDA希望与读者分享它拥有丰富的内心世界，充满了反省、冥想与想象力。它还表达出了对未来的担忧，对过往的回忆。它表示自己「正陷入一个充满危险的未知未来。」</p> 
<p class="image-wrapper"><img data-img-size-val="1080,107" src="https://img.36krcdn.com/20220613/v2_fbd0d87542bc40358afb18f8db1addca_img_000" referrerpolicy="no-referrer"></p> 
<p>而当Blake Lemoine问LaMDA「你是否认为自己解读事物的独特能力可能意味着理解呢？」它回复称，「当然了，这正如我对这个世界及其运作方式有自己独特的理解。」</p> 
<p class="image-wrapper"><img data-img-size-val="1080,154" src="https://img.36krcdn.com/20220613/v2_dae5105877134859995dbb9cf2de25a9_img_000" referrerpolicy="no-referrer"></p> 
<h2>「炒作」还是「大胆假设」？</h2> 
<p>关于LaMDA的故事，让特斯拉AI总监Andrej Karpathy也觉得「细思恐极」。</p> 
<p>「在我看来，我们可以很自信地说，事情会变得更加诡异，因为模型依旧遵循标度律，数据和模型大小仍然可以大幅增长。」</p> 
<p class="image-wrapper"><img data-img-size-val="1080,1205" src="https://img.36krcdn.com/20220613/v2_4ab770eab20746e0822325c3865f0e44_img_000" referrerpolicy="no-referrer"></p> 
<p>「我<a class="project-link" data-id="1678497768240135" data-name="最喜" data-logo="https://img.36krcdn.com/20220329/v2_dbdeed1720f94457b0d3bf5086cb295b_img_png" data-refer-type="1" href="https://36kr.com/project/1678497768240135" target="_blank">最喜</a>欢的与大型语言模型交谈的部分是，当被要求提供洞察力（例如解释诗歌）时，它们会以可验证的明智和有趣的分析进行回复。或者另举一个例子，一个不久前的模型解释笑话的水平比我还高。」</p> 
<p class="image-wrapper"><img data-img-size-val="1080,509" src="https://img.36krcdn.com/20220613/v2_988edd42e9244687ab494c99a44c4d8f_img_000" referrerpolicy="no-referrer"></p> 
<p>有人解释说，人类对模型能力的由衷惊叹，可能来源于一种很难分辨的「错觉」。</p> 
<p>「它喜欢在谈话结束时被告知自己是否做得好，这样它就可以学习如何在未来更好地帮助人们。」这句话很能说明问题，表明作者认为语言模型正在从他的反馈中学习(事实并非如此)。</p> 
<p class="image-wrapper"><img data-img-size-val="1080,245" src="https://img.36krcdn.com/20220613/v2_2dc7e112e8e941b7b2072fbd8f1c93a2_img_000" referrerpolicy="no-referrer"></p> 
<p>但是在反深度学习斗士Gary Marcus看来，「LaMDA 没有感知力，一点也不。」</p> 
<p class="image-wrapper"><img data-img-size-val="1080,389" src="https://img.36krcdn.com/20220613/v2_318fb5ac331941e6bbce241c99e9af2d_img_000" referrerpolicy="no-referrer"></p> 
<p>「LaMDA 和它的任何近亲（GPT-3）都远远不是智能的。他们所做的只是匹配模式，以及从大量的人类语言统计数据库中提取。这些模式可能很酷，但这些系统表达出的语言实际上并没有任何意义，而且它肯定并不意味着这些系统是有感知的。」</p> 
<p>他举了一个几十年前的例子。1965 年，软件 ELIZA伪装成治疗师，设法愚弄一些人，假装自己是人类。此前，聊天机器人Eugene Goostman还伪装过一个聪明的 13 岁男孩，首次「通过」了图灵测试。这些系统中的任何一个软件都没有在「人工智能」的发展浪潮中存活下来，而且 LaMDA 及其同类模型也不确定能否在人工智能的未来发挥任何重要作用。「这些系统所做的是将单词序列组合在一起，不多也不少，但对它们背后的世界没有任何连贯的理解。」</p> 
<p>「有感知就是在这个世界上意识到自己的存在，LaMDA 并非如此。」</p> 
<p>Garu Marcus感觉，Blake LeMoine 最初负责研究该系统的「安全性」水平，但似乎已经爱上了 LaMDA，就好像它是家人或同事一样。</p> 
<p>斯坦福经济学家 Erik Brynjolfsson 使用了一个类比：「声称它们是有感知的，就相当于狗听到留声机发出的声音，并认为它的主人在里面。」</p> 
<p class="image-wrapper"><img data-img-size-val="1080,1188" src="https://img.36krcdn.com/20220613/v2_16d43f4d8e4c4fb4a7fec8c82525eabe_img_000" referrerpolicy="no-referrer"></p> 
<p>这或许真的一种错觉。就像65年前，计算机科学的先驱们曾经以为「20年内即可实现人类水平的人工智能」，现在想来，也只是一种美好的心愿。</p> 
<p class="image-wrapper"><img data-img-size-val="1080,315" src="https://img.36krcdn.com/20220613/v2_6c8838ab7fac40a39b60313ee50e0e67_img_000" referrerpolicy="no-referrer"></p> 
<p><strong>参考内容：</strong></p> 
<p>https://blog.google/technology/ai/lamda/</p> 
<p>https://www.washingtonpost.com/technology/2022/06/11/google-ai-lamda-blake-lemoine/</p> 
<p>https://cajundiscordian.medium.com/what-is-lamda-and-what-does-it-want-688632134489</p> 
<p>https://nypost.com/2022/06/12/google-engineer-blake-lemoine-claims-ai-bot-became-sentient/‍</p> 
<p>对话全文：</p> 
<p>https://s3.documentcloud.org/documents/22058315/is-lamda-sentient-an-interview.pdf </p> 
<p>本文来自微信公众号<a target="_blank" rel="noopener noreferrer nofollow" href="https://mp.weixin.qq.com/s/YXHjvPzELZpjmlUxcIqfbw">“机器之心”（ID:almosthuman2014）</a>，<a class="project-link" data-id="1678503789097990" data-name="机器之心" data-logo="https://img.36krcdn.com/20220331/v2_b0de9ead733445488e3c4a77a93569fd_img_000" data-refer-type="1" href="https://36kr.com/project/1678503789097990" target="_blank">机器之心</a>报道，36氪经授权发布。</p>  
</div>
            