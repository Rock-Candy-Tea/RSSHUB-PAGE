
---
title: '如何让人工智能_负起责任_？盯住隐私合规'
categories: 
 - 新媒体
 - 36kr
 - 资讯
headimg: 'https://img.36krcdn.com/20220128/v2_19d1fc369d79401fa36adadeafb5b0bf_img_000'
author: 36kr
comments: false
date: Fri, 28 Jan 2022 12:49:41 GMT
thumbnail: 'https://img.36krcdn.com/20220128/v2_19d1fc369d79401fa36adadeafb5b0bf_img_000'
---

<div>   
<p>2021年9月，中国发布《新一代人工智能伦理规范》，“强化责任担当”是6项基本伦理要求之一。与此同时，联合国教科文组织、经合组织以及欧盟、美国等也先后发布或即将发布相应的指南或法规，规范AI治理。</p> 
<p>如何才能实现负责任的AI？本文作者、国际隐私专业协会（IAPP）研究员Katharina Koerner认为，必须处理好AI治理原则与隐私保护之间的关系。</p> 
<p class="image-wrapper"><img data-img-size-val="906,459" src="https://img.36krcdn.com/20220128/v2_19d1fc369d79401fa36adadeafb5b0bf_img_000" referrerpolicy="no-referrer"></p> 
<p>人工智能（AI）与机器学习（ML）正以前所未有的速度向前发展。由此也带来了一个问题：如何以负责任的、符合伦理要求的方式使用AI/ML系统，而且这种方式还要<a class="project-link" data-id="95377" data-name="得到" data-logo="https://img.36krcdn.com/20210807/v2_966db147ab4646ef82349f069ce61219_img_000" data-refer-type="2" href="https://36kr.com/projectDetails/95377" target="_blank">得到</a>用户和社会的信任？</p> 
<p>监管者、组织机构、研究人员，以及各行各业的从业者都在寻找问题的答案。越来越多隐私领域的专家也参与到AI的治理中来。他们面临的挑战是：一方面要遵循隐私规则对AI有所限制，另一方面还要谋求进一步发展，如何深刻理解上述二者关系并“负责任”地使用AI。</p> 
<p>随着政府相关机构加大在这个复杂领域的执法力度，并且强化规则制定与立法，有一个情况变得至关重要：组织机构必须清楚了解目前适用于AI领域的隐私要求，即将生效的隐私要求，以及可用资源，才能为AI和ML应用建立一个合规的数据保护计划。</p> 
<p class="image-wrapper"><img data-img-size-val="475,244" src="https://img.36krcdn.com/20220128/v2_98f0c726d49140df810bdf602b856c7f_img_000" referrerpolicy="no-referrer"></p> 
<h2><strong>01  AI治理的全球性共识</strong></h2> 
<p>近年来，许多有关可信AI的治理指南先后发布，这些指南取得了良好的效果。大部分AI治理框架在基本原则的定义上包括如下要素：隐私与数据治理、问责与审核、稳健与安全、透明度与可解释性、公平与非歧视、人工监管，以及人类价值的促进。</p> 
<p>有些公共机构发布的负责任的AI框架颇具代表性，例如，联合国教科文组织（UNESCO）发布的《AI伦理问题建议书》（Recommendation on the Ethics of AI），中国的《新一代人工智能伦理规范》，欧洲理事会的报告《AI系统的监管》，经济合作与发展组织（OECD）的《AI原则》，以及欧盟委员会AI高级别专家组制定的《可信AI伦理指南》。</p> 
<p class="image-wrapper"><img data-img-size-val="347,465" src="https://img.36krcdn.com/20220128/v2_372bad401014475e9a360641e2f7ee28_img_000" referrerpolicy="no-referrer"></p> 
<p>除此之外，还有数不清的由公司发布的自律倡议。不仅如此，业界还与学界和非营利组织携手，推动各界负责任地使用AI，例如，AI合作伙伴关系，或者全球AI合作伙伴关系。标准化组织，如国际标准化组织/国际电工委员会（ISO/IEC）、电气与电子工程师协会（IEEE），以及美国国家标准与技术研究院（NIST）也推出了指南。</p> 
<p>当前的治理倡议主要采取宣言的形式，并不具有约束性。与此同时，各种现有的隐私保护法律已经要求在一定程度上，必须负责任地使用AI系统。</p> 
<p>隐私监管机构在AI的治理中担负着重要作用，新加坡个人数据保护委员会（Personal Data Protection Commission）发布的《AI治理框架范例》（Model AI Governance Framework），英国信息专员办公室（U.K. Information Commissioner’s Office）为制定AI审计框架而付出的巨大努力，中国香港特别行政区的隐私专员公署（Office of The Privacy Commissioner for Personal Data of Hong Kong）发布的《AI应用与伦理发展指南》都是很好的例证。</p> 
<p class="image-wrapper"><img data-img-size-val="1080,506" src="https://img.36krcdn.com/20220128/v2_8f022f2937804eb7bb3e8e42f2d7de7b_img_000" referrerpolicy="no-referrer"></p> 
<h2><strong>02  隐私监管与负责任的AI</strong></h2> 
<p>“隐私”就是人们时常提到的<a class="project-link" data-id="3969340" data-name="一条" data-logo="https://img.36krcdn.com/20200916/v2_811751a081924fa9af8741ce120bd7bf_img_png" data-refer-type="2" href="https://36kr.com/projectDetails/3969340" target="_blank">一条</a>负责任的AI原则。这让人<a class="project-link" data-id="195613" data-name="联想" data-logo="https://img.36krcdn.com/20200924/v2_e165f4830deb4b83866bb3a5bb92599a_img_000" data-refer-type="2" href="https://36kr.com/projectDetails/195613" target="_blank">联想</a>到，把通用隐私原则（这也是全球隐私与数据保护的基石）应用到处理个人数据的AI/ML体系中的义务。其中包括，确保收集行为的界限、数据质量、用<a class="project-link" data-id="305033" data-name="途说" data-logo="https://img.36krcdn.com/20201106/v2_01ceedf1e096489dbdd6ca558a5d8c1e_img_000" data-refer-type="2" href="https://36kr.com/projectDetails/305033" target="_blank">途说</a>明、使用范围、问责及个体参与。</p> 
<p>可信的AI原则，如透明度与可解释性、公平与非歧视性、人工监管、数据处理的稳健性与安全性，通常与个人的具体权利，以及相应的隐私法律的条款相关。</p> 
<p>就欧盟的《通用数据保护条例》（GDPR）而言，就是解释的权利[第1（1）条、第12条、第13条、第14条、第15（1）（h）条、第22（3）条，以及引述71]、公平原则[第5（1）（a）条，以及引述75]、人工监管（第22条）、处理的稳健性[第5（1）d条]和安全性。其他的隐私法律，如中国的《个人信息保护法》或英国的《通用数据保护条例》，也包括类似的与这些负责任的AI原则有关的条款。</p> 
<p>美国的联邦贸易委员会（FTC）要求AI开发人员及使用算法的公司必须承担《联邦贸易委员会法》（FTC Act）第5节中规定的责任，以及美国《公<a class="project-link" data-id="572139" data-name="平信" data-logo="https://img.36krcdn.com/20210813/v2_eb065aa0f13644149406563cdef8e28d_img_000" data-refer-type="2" href="https://36kr.com/projectDetails/572139" target="_blank">平信</a>用报告法》（US Fair Credit Reporting Act）和《平等信贷机会法》（Equal Credit Opportunity Act）中规定的责任。FTC在2016年的报告，以及2020年和2021年的指南中明确指出，AI的使用必须透明，其中包括向消费者解释决策算法，并且确保决定是公平的，符合常理的。</p> 
<p>如果不清楚AI系统基于隐私规则的合规要求，面临风险的将不仅仅是受影响的个人。公司会面临巨额罚款，甚至不得不删除数据，并且清除模型和算法。</p> 
<p class="image-wrapper"><img data-img-size-val="915,455" src="https://img.36krcdn.com/20220128/v2_32bceb0e42fa4a2796985705094c9b87_img_000" referrerpolicy="no-referrer"></p> 
<h2><strong>03  最新案例</strong></h2> 
<p><strong>1.澳洲</strong></p> 
<p>2021年底，澳大利<a class="project-link" data-id="3969406" data-name="亚信" data-logo="https://img.36krcdn.com/20200916/v2_811751a081924fa9af8741ce120bd7bf_img_png" data-refer-type="2" href="https://36kr.com/projectDetails/3969406" target="_blank">亚信</a>息专员办公室（Office of Australian Information Commissioner）发现，应用程序Clearview AI违反《澳大利亚隐私法》（Australian Privacy Act），在没有征得许可的情况下，收集人脸生物数据。不久之后，英国信息专员办公室与澳大利亚信息专员办公室开展联合调查，英方根据调查结果宣布，准备以同样的理由对Clearview AI公司至少处以1700万英镑的罚款。不仅如此，加拿大隐私保护当局以及法国的隐私监管机构数据保护局（CNIL）均要求Clearview AI停止数据处理，并且删除所有收集到的数据。</p> 
<p><strong>2.欧洲</strong></p> 
<p>2021年，欧洲数据保护当局追查<a class="project-link" data-id="4260438" data-name="了数" data-logo="https://img.36krcdn.com/20210422/v2_8e636ec7be434dd5bf7deebc8bed2b62_img_png" data-refer-type="2" href="https://36kr.com/projectDetails/4260438" target="_blank">了数</a>起AI/ML系统侵犯隐私的案件。</p> 
<p>2021年12月，荷兰数据保护当局宣布，荷兰税务及海关总署（Dutch Tax and Customs Administration）违反GDPR的规定，以ML算法歧视的方式处理国籍申请者的信息，为此对荷兰税务及海关总署处以275万欧元罚款。这种算法自动把双重国籍的申请人定义为高风险人群，导致这些人很可能被贴上“欺诈”的标签。</p> 
<p>2021年8月还发生了<a class="project-link" data-id="81906" data-name="一起" data-logo="https://img.36krcdn.com/20210709/v2_647b9860d6f7437caf1be2501d37698a_img_000" data-refer-type="1" href="https://www.36dianping.com/space/4772100123" target="_blank">一起</a>具有里程碑意义的案件，意大利数据保护机构Garante以违反GDPR规定为由，对餐食快递公司Foodinho和Deliveroo分别处以近300万美元的罚款。Garante认为，两家公司用于管理外卖骑手的算法缺乏透明度和公平性，而且缺少准确信息。该监管机构还发现，两家公司缺乏数据最小化、安全性、隐私保护设计以及默认的保护措施，而且也没有进行数据保护影响评估。</p> 
<p>在2021年初的类似案件中，阿姆斯特丹地方法院发现，拼车公司Uber和Ola出租车没有满足GDPR的透明度要求，侵犯了要求人工干预的权利。荷兰DPA的调查正在进行中。</p> 
<p><strong>3.美国</strong></p> 
<p>在美国，FTC最近明确表明，在模型或算法的开发过程中，如果不坚持隐私要求，风险会很高。</p> 
<p>在Everalbum事件中，FTC不仅强调向使用者公开收集生物特<a class="project-link" data-id="582231" data-name="征信" data-logo="https://img.36krcdn.com/20200916/v2_811751a081924fa9af8741ce120bd7bf_img_png" data-refer-type="2" href="https://36kr.com/projectDetails/582231" target="_blank">征信</a>息的义务，而且还要求删除或销毁非法获取的数据，以及利用这些数据开发的模型和算法。</p> 
<p class="image-wrapper"><img data-img-size-val="892,441" src="https://img.36krcdn.com/20220128/v2_efc56c384ad64d328a54197b6ed921fb_img_000" referrerpolicy="no-referrer"></p> 
<h2><strong>04  定义与实践的挑战：可解释性与公平性</strong></h2> 
<p>毫无疑问，不按照法规要求执行负责任的AI原则将承担相应的法律责任，但是目前还有许多悬而未决的问题。围绕“同意及以适当方式通知用户”，各国都颁布了许多法律指南，但是部分相关规定，如AI的公平性及可解释性等法律解释和执行的工作还处在起步阶段。其中面临的一个共同问题是，无法使用统一的方法评估各种使用场景中的可信AI原则。</p> 
<p>AI的可解释性或透明度原则旨在打开所谓的ML模型“黑箱”。大家围绕可解释的AI展开了各种AI研究。解释ML的模型意味着什么，大家<a class="project-link" data-id="53908" data-name="众说" data-logo="https://img.36krcdn.com/20210807/v2_6eb77fbc91074f208ea607dc05615c65_img_000" data-refer-type="2" href="https://36kr.com/projectDetails/53908" target="_blank">众说</a>纷纭。为了向监管者或用户解释AI如何“预测”，大家通常采用基于结果的事后模型。替代模型（或初始模型）可以在包含样本和黑箱模型输出结果的数据集上做测试，以便得到近似的预测。任何解释都应该让接收方可以理解，并且包含与这个系统相关的设计选择，以及部署该系统的基本原理。</p> 
<p>AI的公平性原则是另一个不断发展的领域，它涵盖的问题比较复杂。偏见、歧视和公平问题与背景息息相关。关于公平有许多定义，它们之间存在巨大差异。一些隐私监管机构发布了明确的指南。在英国信息专员办公室（ICO）看来，公平意味着个人数据必须以人们认为合理的方式来处理，如果处理的方式会产生不合理的负面效果，就是不公平的。同样，美国FTC的解释是，根据《联邦贸易委员会法》，如果行为导致的损害大于收益，这种行为就是不公平的。另一方面，在GDPR框架下，对公平原则的定义仍然比较少。与此同时，许多组织机构还不确定如何在实践中避免偏见。通常，偏见可能在处理前（在训练算法之前）、处理中（模型训练中）和处理后（在预测中纠正偏见）得以解决。</p> 
<p>AI的可解释性和公平性原则只是负责任的AI领域中快速发展的各种原则中的两项。在其他领域，如安保领域的AI/ML算法也要求增强安全意识，正如欧盟网络安全局（ENISA）在最近的报告中所强调的那样。</p> 
<p>另一个挑战是不同原则之间如何权衡。一些原本关系稳定的特点可能会产生矛盾，如透明度与隐私，或者隐私与公平等。</p> 
<p class="image-wrapper"><img data-img-size-val="912,465" src="https://img.36krcdn.com/20220128/v2_c743c89ba8ba483a8f3e62aeb31683c6_img_000" referrerpolicy="no-referrer"></p> 
<h2><strong>05  实践评估与记录归档</strong></h2> 
<p>法律定义并非“负责任AI原则”的唯一组成部分，还需要进一步清晰化。公司在试图把可信AI原则转化为实际行动时，面临着许多挑战，我们通常把它描述为“负责任的AI缺口”。</p> 
<p>与“负责任的AI”有关的其他问题可以用来扩充数据保护影响评估或隐私影响评估。按照这种方式，使用AI给个人的权利和自由带来的风险是可以识别和控制的。偏见或算法和数据集的不准确给个人带来的任何伤害都应予以评估，恰当使用AI/ML的算法应该予以记录。可以用隐私影响评估（PIA）来描述权衡的过程，例如，在统计上的准确性与数据最小化之间，以及在记录方法与决策的合理化之间。</p> 
<p>此外，组织机构还可以考虑隐私保护ML解决方案，或者使用合成数据。一方面，它们没有取代负责任的AI和隐私政策，没有取代完整的模型风险管理，也没有取代模型的可解释性或探测偏见的方法和工具，另一方面，它们在设计AI架构时，强化了隐私优先的方法。</p> 
<p>挪威数据保护局（DPA）发布了一份报告，阐释在ML算法中如何使用个人数据，报告强调：“使用AI的组织结构尤其要关注两个新要求——隐私保护设计和数据保护影响评估（DPIA）。”</p> 
<p>在这个大背景下，负责任的AI原则面临的关键问题也可以考虑在内。可以从欧盟AI高级专家组（AI-HLEG）推荐的名单，或者AI合作伙伴编制的名单入手。不同领域间的探讨，以及为实现负责任的AI、AI公平性，AI可解释性而开发的工具包，如LIME、SHAP或LORE的部署都可以进一步增进了解，提高用户方的透明度。</p> 
<p>此外，为避免偏见，非技术类的方法可以包括建立AI伦理委员会、内部培训、团队构成的多样化，或者分析数据收集机制。目前，公共机构已经率先罗列出所有使用中的，以透明度为由的算法。其他组织机构开始发布AI的可解释性的声明。无论采用哪种方法，组织结构必须向消费者提供必要的信息，以避免因AI/ML系统，以及评分机制的使用及后果导致的有害行为。</p> 
<p class="image-wrapper"><img data-img-size-val="707,284" src="https://img.36krcdn.com/20220128/v2_23ec522c23ca4d708f3ecebcde764b1a_img_000" referrerpolicy="no-referrer"></p> 
<h2><strong>06  即将出现的新进展</strong></h2> 
<p>未来即将出台的各类法律都会反映出保障可信AI和ML的原则。据OECD统计，从全球范围看，有60个国家出台了700项AI政策倡议。</p> 
<p>《欧盟AI法案》（EU Artificial Intelligence Act）即将出台，高风险的AI系统将直接受到监管。美国的<a class="project-link" data-id="652580" data-name="拜登" data-logo="https://img.36krcdn.com/20201113/v2_92bfb5af14d9459abe7929d26dac143d_img_000" data-refer-type="2" href="https://36kr.com/projectDetails/652580" target="_blank">拜登</a>政府宣布了“AI权利法案”的进展。除了即将为FTC提供5亿美元额外资助外，FTC还请求成为隐私和AI领域的规则制定机构。新成立的加利福尼亚隐私保护局（California Privacy Protection Agency）有可能在2023年之前发布AI管理相关规则，有望产生深远影响。</p> 
<p>随着执法的愈益严格，以及新法规的不断出台，确保AI系统在隐私方面合规将成为负责任地使用AI的最基本要求。协调各方的努力，并且全面深入了解AI/ML生态系统有助于为即将到来的新变化做好充分准备。</p> 
<p><strong>原文链接：</strong></p> 
<p>https://iapp.org/news/a/privacy-and-responsible-ai/</p> 
<p><strong>文中提及的AI部分资料原文链接：</strong></p> 
<p>1.联合国教科文组织（UNESCO）《AI伦理问题建议书》（Recommendation on the Ethics of AI）：</p> 
<p>https://unesdoc.unesco.org/ark:/48223/pf0000377897</p> 
<p>2.《欧盟AI法案》（EU Artificial Intelligence Act）提案：</p> 
<p>https://artificialintelligenceact.eu/the-act/</p> 
<p>3.欧洲理事会报告《AI系统的监管》：</p> 
<p>https://rm.coe.int/prems-107320-gbr-2018-compli-cahai-couv-texte-a4-bat-web/1680a0c17a</p> 
<p>4.经济合作与发展组织（OECD）AI原则：</p> 
<p>https://oecd.ai/en/ai-principles</p> 
<p>5.欧盟委员会AI高级别专家组制定的《可信AI伦理指南》：</p> 
<p>https://op.europa.eu/en/publication-detail/-/publication/d3988569-0434-11ea-8c1f-01aa75ed71a1</p> 
<p>本文来自<a class="project-link" data-id="3968527" data-name="微信" data-logo="https://img.36krcdn.com/20200916/v2_811751a081924fa9af8741ce120bd7bf_img_png" data-refer-type="2" href="https://36kr.com/projectDetails/3968527" target="_blank">微信</a>公众号<a target="_blank" rel="noopener noreferrer" href="https://mp.weixin.qq.com/s?__biz=MzkyOTMxMDg1Mg==&mid=2247487488&idx=1&sn=e2673c192f2d8c9b432dbcd7dc1d1f29&chksm=c20a21ebf57da8fd542d07c6df625f3dc536e94e56872d68e306846dec64a1186bceb03ccb00&token=1218476515&lang=zh_CN#rd">“Internet Law Review”（ID:Internet-law-review）</a>，作者：互联网法律评论，36氪经授权发布。</p>  
</div>
            