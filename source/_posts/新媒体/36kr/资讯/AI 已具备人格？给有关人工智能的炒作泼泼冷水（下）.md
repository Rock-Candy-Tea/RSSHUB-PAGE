
---
title: 'AI 已具备人格？给有关人工智能的炒作泼泼冷水（下）'
categories: 
 - 新媒体
 - 36kr
 - 资讯
headimg: 'https://img.36krcdn.com/20220505/v2_58c32d8b09094585b47968e7d34d9b79_img_jpeg'
author: 36kr
comments: false
date: Mon, 13 Jun 2022 04:37:49 GMT
thumbnail: 'https://img.36krcdn.com/20220505/v2_58c32d8b09094585b47968e7d34d9b79_img_jpeg'
---

<div>   
<blockquote> 
 <p>神译局是36氪旗下编译团队，关注科技、商业、职场、生活等领域，重点介绍国外的新技术、新观点、新风向。</p> 
</blockquote> 
<p>编者按：OpenAI 的 AI 最近被炒得比较火。跟人对话，各种写作，乃至于根据文字描述作画，似乎已经无所不能。《纽约时报》最近一篇《AI 正在掌握语言》的文章更是把这种进展抬到了机器是不是快要具备意识的高度。但一些理性的业界人士也提出了自己的质疑，请求大家擦亮眼睛，不要冲动。文章来自编译，篇幅关系，我们分两部分刊出，此为第二部分。</p> 
<p class="image-wrapper"><img data-img-size-val="1150,480" src="https://img.36krcdn.com/20220505/v2_58c32d8b09094585b47968e7d34d9b79_img_jpeg" referrerpolicy="no-referrer"></p> 
<p><strong>相关阅读：</strong></p> 
<p><a href="https://36kr.com/p/1782968488611206" target="_blank"></a><a href="https://36kr.com/p/1782968488611206">AI 已具备人格？给有关人工智能的炒作泼泼冷水（上）</a></p> 
<h3>Open AI 的圣徒传记</h3> 
<p>Johnson给《纽约时报》杂志写的那篇文章时不时就写成了 OpenAI 及其领导者的圣徒传记，比方说文字是这么描述 Ilya Sutskever 的：</p> 
<blockquote> 
 <p>坐在椅子上的 Sutskever 身体前倾，聚精会神地说：“这就是 GPT-3 的基本思想。”他回答问题的方式很有趣：先挑几个错误的——“我可以给你一个几乎符合你要求的描述”——然后长时间陷入沉思，仿佛他在提前筹划整个回应。</p> 
</blockquote> 
<p>Johnson 还不加批判地把 OpenAI 团队打造成他们正在着手拯救世界的形象：</p> 
<blockquote> 
 <p>再次地，所有证据都表明，这种权力将会被少数硅谷大型企业掌握在手里。</p> 
 <p>7 月的晚上在沙山路举行的那场晚宴，议程极其雄心勃勃：找到驾驭人工智能研究走向最积极道路的最佳办法</p> 
</blockquote> 
<p>作为整体，他们被定位成“解决那个问题的人”，这很自大。这种自大的程度唯有这种天真才能超越：即相信实现这一点主要得靠所谓的“AGI”（通用人工智能）系统。但 Johnson 不加批判、甚至带着钦佩地报道了这一雄心壮志，而且似乎没有注意到 “少数硅谷巨头”的领导人与出席那顿晚餐的人之间几乎没什么区别。</p> 
<blockquote> 
 <p>今天，该组织大约有五分之一的人全职专注于所谓的“安全”与“协调”（也就是让技术与人类的利益保持一致）问题</p> 
</blockquote> 
<p>同样地，绝对没有证据表明 OpenAI 的人能够理解“人类的利益”（而不仅仅是他们自己/像他们这样的人的利益），然而 Johnson 还是不加批判地提出了这一点。并且在写了一万字之后 Johnson 才写道：</p> 
<blockquote> 
 <p>但除了章程本身，以及 OpenAI 的安全团队刻意制定的减速带与禁令之外， OpenAI 并没有具体说明谁来定义人工智能“造福全人类”究竟是什么意思。现在，这些决定将由 OpenAI 的高管和董事会做出——无论他们的意图是多么的令人钦佩，但这群人甚至连旧金山的代表样本都算不上，更不用说全人类了。从近处看，“当利害关系不大时”关注安全和试验值得称赞。但从远处看，很难不把这个组织看作是同样的那一小撮硅谷超级英雄，在没有得到广泛同意的情况就拉动了科技革命的杠杆，就像他们在最近几波创新浪潮所做的事情那样。</p> 
</blockquote> 
<h3>大型语言模型（LLM）并非不可避免，“更广泛的 web”不具备代表性</h3> 
<p>这篇文章还有另一类失误，我觉得也可以归因为与主题（OpenAI）没有保持距离，以及对他们的主张没有保持足够的怀疑。尽管他们往 LLM （以及基于图像数据集训练的类似大型模型）投入了各种资源，但要记住的是，这一切都不是注定的。我们可以想象其他的未来，但要想做到这一点，我们必须保持独立，跟那些认为“AGI”令人向往，并且 LLM 是通往 AGI 之路的人所推崇的叙事拉开距离。</p> 
<p>鉴于此，不妨思考一下《纽约时报》杂志那篇文章里面的这段话：</p> 
<blockquote> 
 <p>LLM 也有更加令人不安的倾向：它们会公开使用种族主义的语言；它们会散布阴谋论的虚假信息；在被问及基本的健康或安全信息时，它们会提供危及生命的建议。所有这些失败都源自一个不可回避的事实：要想获得规模大到能让 LLM 可行的数据集，你得到更广泛的 web 上面找。而可悲的是，更广泛的 web 是我们作为一个物种当下集体心理状态的代表，会持续受到偏见、错误信息体和其他毒素的困扰。</p> 
</blockquote> 
<p>首先，事实上，文中提及的第三点（即 LLM 会提供危及生命的建议）并不<strong>仅仅</strong>源自他们的训练数据未经整理这一事实。相反，它与这样一个事实有关，即 LLM 本来就是瞎编乱造。说得更具体一点，它们是在用训练的语言编写文本字符串，而说这种语言的人可以解释这些字符串。因此，当一个人提出与健康和安全相关的问题时，如果返回的结果是错误的话，也很可能是十分危险的。最重要的是：这些可以理解为不使用 LLM 的原因。可是，整篇文章似乎都在跟着 OpenAI 等的看法走，也就是把这些倾向看作是“目前”的 LLM 暂时存在的缺点，会得到及时解决的。</p> 
<p>其次，更广泛的 web 并不能够代表整个人类。我们曾经详细介绍过各种力量是如何影响着谁可以访问 web 的，比如谁对继续贡献态度温和，被选出来供 LLM 训练数据用的那部分 web 都代表了谁，应用到这些训练数据上面的不成熟的过滤是如何造成进一步的失真的等等。在《压迫性算法》（Algorithms of Oppression）这部杰作里面，Safiya Noble 展示了广告驱动的 web 搜索经济是如何塑造了大家看到的结果的，尽管谷歌（相当误导性的）将这些结果描述为“自然存在的东西”。</p> 
<h3>不仅仅是学术辩论</h3> 
<p>“人工智能”是否真的有效不仅仅是一场学术辩论。如果从事“人工智能”工作的人只是在研究实验室里研究一些深奥的项目，并没有打算货币化的话，那情况可能确实是这样。但这不是我们生活的世界：似乎每天都有新的新闻报道说有人在售卖“人工智能”系统来做一些不恰当的事情，比方说为无法参加考试的学生填写成绩，或诊断心理健康障碍、面试求职者或逮捕移民等。</p> 
<p>因此，重要的是要让广大公众清楚地了解“人工智能”可以做什么，了解哪些自诩是 AI 的 app 不过是骗人的把戏罢了，以及要找出这些系统可能造成的危害（不管它们是否管用）。但 Johnson 错过了用这些方式去教育公众的机会，原因主要有两点。</p> 
<p>首先，他谈到了 LLM 在信息检索方面的可能应用（即替代搜索引擎）：</p> 
<blockquote> 
 <p>如果按照现有的轨迹走下去，像 GPT-3 这样的软件可能会在未来几年彻底改变我们搜索信息的方式。 [...] 如果 GPT-3 的真正信徒是正确的话，那么在不久的将来，你只需要向 LLM 提出问题，对方就会准确而有说服力地反馈答案给你。</p> 
</blockquote> 
<blockquote> 
 <p>这不只是一场只有内行才懂的辩论。如果你可以用下一单词预测去训练机器，去表达复杂的想法或总结繁杂的材料的话，那么我们也许正处在真正的技术革命的风口浪尖，像 GPT-3 这样的系统将取代搜索引擎或维基百科，成为我们发现信息的默认资源。</p> 
</blockquote> 
<p>但正如 Will Douglas Heaven 最近在《MIT 技术评论》发表的文章所说的那样，其实这个想法很糟糕。</p> 
<p>但从更高的角度来看 ，Johnson 把人工智能炒作可能造成的伤亡范围缩小到了人工智能研究本身：</p> 
<blockquote> 
 <p>但是，如果大型语言模型最终只是“随机鹦鹉”的话，那么 AGI 将再次退却到遥远的地平线——作为社会，我们将陷入将太多资源（包括金钱和智力资源）投入到追求虚假神谕的风险。</p> 
</blockquote> 
<p>是的，当这个世界上还有很多其他有价值的（而且很多情况下是紧迫的）问题时，研究资源过于集中在“人工智能”上确实有问题。（就算“AGI”是一个定义明确且可行的研究目标，我也会这么说。）但这并不是唯一的危害：每当有人对数据大规模应用模式识别，去做出会影响到人类的，所谓的“公正”、“客观”的决策系统或内容系统时，就会带来伤害。但所有这些 Johnson 均避而不谈。</p> 
<h3>关于被归类为“质疑者”</h3> 
<p>说句公道话，Johnson 确实给不同的声音留出了一点空间（包括我、Meredith Whittaker 以及 Gary Marcus 的话）。但他把我们说成是“质疑者”，相当于在“双方”的报道中象征性地向另一方点头。但在这两点意义上，情况其实比这还要糟糕：首先，把人说成是质疑者，似乎就把举证的责任，从那些声称自己在做怪异事情（开发“AGI”）的人身上，转移到那些说这毫无根据的人身上。</p> 
<blockquote> 
 <p>一些质疑者认为，这个软件只会盲目模仿——它能模仿人类语言的句法模式，但没法产生自己的想法或做出复杂的决定，这个基本的限制会阻止 LLM 发展成任何与人类智能类似的东西。</p> 
</blockquote> 
<p>在展现了这种质疑之后，他又问道：</p> 
<blockquote> 
 <p>我们如何才能确定 GPT-3 有自己的想法，还是只是在改写它从维基百科、欧柏林学院（Oberlin College）或纽约时报书评（The New York Review of Books）的服务器上扫描而来的语言语法？</p> 
</blockquote> 
<p>但他没有问我们基于什么理由才能相信 GPT-3 自己会思考。相反，文章说的是“其实没人知道”：</p> 
<blockquote> 
 <p>一些人认为，由于神经网络的深度，更高层次的理解正在出现。而有的人则认为，就定义而言，程序光靠整天玩“猜字谜”是无法获得真正的理解的。但其实没人知道。</p> 
</blockquote> 
<p>但对我来说更令人恼火的是，被归入“质疑者”这个框框就把辩论的主动权拱手让给了人工智能的支持者。我说的不只有“不，‘人工智能’不是这么开发的”，Meredith Whittaker 也一样（尽管 Gary Marcus 的观点似乎主要集中在这一点上）。对我来说，相关的问题不是“我们如何开发‘人工智能’？”而是“我们如何转移权力才能少看到一些（理想情况下没有）算法压迫的案例？”，“我们如何设想和部署设计流程，才能将技术定位为工具，从而可由致力于亲社会目的的人们所塑造和使用？”，以及“我们如何才能确保拒绝（AI）的可能性，让关闭有害 app、并确保受到伤害的那些人可以追索成为可能？”</p> 
<p>因此，虽然我认为在这一领域存在非常大的道德问题风险，但我根本就不同意那篇文章所设定的框架。我意见最大的也许是里面提出的道德问题解决方案：</p> 
<blockquote> 
 <p>我们现在正在就什么才是给软件灌输道德和公民价值观的最佳方式进行严肃的辩论，但这个辩论的前提是应该说清楚我们已经迈过了一道重要门槛。</p> 
</blockquote> 
<p>以及</p> 
<blockquote> 
 <p>我们从未向机器传授过价值观。</p> 
</blockquote> 
<p>在这部分 Johnson 引用了我的话：</p> 
<blockquote> 
 <p>Emily Bender 认为：“只要所谓的人工智能系统是由大型科技公司在没有民主监管的情况下开发和部署的，它们体现的就主要是硅谷的价值观。任何试图‘教导'它们的尝试都不过是道德洗礼罢了。”</p> 
</blockquote> 
<p>引用倒是准确的，但其实我说的不止这些：</p> 
<blockquote> 
 <p>说到“教给机器价值观”，这完全是对情况的误判，也是对人工智能的一种炒作。软件系统是人工制品，不是有感觉的实体，因此“教”机器这个比喻不恰当且存在误导性（“机器学习”或“人工智能”也是如此）。相反，与任何其他人工制品或系统一样，开发者正在把价值观设计进系统里面。说他们可以“传授”其他的价值观，其实只是换种方式将系统开发者（已经设计好的）的价值观隐藏在虚假客观性的外表之下。只要所谓的人工智能系统是由大型科技公司在没有民主监管的情况下开发和部署的，它们体现的就主要是硅谷的价值观。任何试图‘教导'它们的尝试都不过是道德洗礼罢了。</p> 
</blockquote> 
<p>但我认为 Johnson 并没有真正理解我的观点，因为这篇文章还是坚持“传授”机器价值观的说法：</p> 
<blockquote> 
 <p>我们要不要开发一个热爱骄傲男孩、垃圾邮件艺术家、俄罗斯巨魔农场、 QAnon 预言家的 AGI ？开发一个能准确解读人类所有的话，带着善意撰写，用体面的意图去表达的人造大脑很容易。但开发一个知道什么时候无视我们的人造大脑很难。</p> 
</blockquote> 
<p>我希望坚持看到这里的人都能看清楚这段话有什么问题，能看出他们是怎么设想这些问题的：他们假设“开发完全自主主体”这一想法既是可能的又是值得的！</p> 
<p>举个例子，Johnson 又一次设想了自主主体的存在：</p> 
<blockquote> 
 <p>如果未来出现大型语言模型，那么最紧迫的问题就变成了：我们如何训练它们当好公民？当人类本身连基本事实都没法达成一致，更不用说核心伦理和公民价值观时，我们又怎么能让它们“造福全人类”？</p> 
</blockquote> 
<p>我一直在（徒劳地）等待这篇文章突破 OpenAI 的框架。为什么要开发这些东西？如果在开发这些东西的话，为什么不先从约束其开发和部署它们的民主治理，以及透明性与记录要求着手，而要一味关注修补算法与相关训练数据？</p> 
<h3>结论</h3> 
<p>我曾经做过几次演讲，演讲题目叫做“人造对话者的意义建构与语言技术的风险”。最后，我提醒听众不要太激动，并记住：</p> 
<ul class=" list-paddingleft-2"> 
 <li><p>仅仅因为文字看起来有条理，未必意味着它背后的模型已经理解任何东西或值得信赖</p></li> 
 <li><p>仅仅因为那个回答是对的，未必意味着下一个回答也是对的</p></li> 
 <li><p>当计算机似乎懂得“说我们的语言”时，其实我们才是幕后功臣</p></li> 
</ul> 
<p>但保持这种怀疑态度并非易事。Johnson 写道：</p> 
<blockquote> 
 <p>最初几次我给 GPT-3 输入类似提示时，都会有一种脊背发凉的感觉。一台机器，完全基于预测下一个词的基本训练，就能如此迅速地生成如此清晰明了的文本，这几乎是不可能的。</p> 
</blockquote> 
<p>以及</p> 
<blockquote> 
 <p>重要的是，我们得强调这不是关于软件是否具有自我意识或感知能力的问题。大型语言模型没有意识——软件里面没有什么“心灵剧场”，没法像人类这种有意识的有机体那样去体验思维。但是当你看到算法在超小说里面创作出来的句子时，很难没有一种机器仿佛在用某种有意义的方式去思考的感觉。</p> 
</blockquote> 
<p>“很难没有感觉”说得很贴切。当我们遇到似乎在说我们的语言的东西时，我们甚至会不假思索地运用起使用该语言与他人交流的相关技能。这些技能主要涉及主体间性和共同关注，因此我们会想象语言背后会有个大脑，即便它并不存在。</p> 
<p>但关键是要提醒我们自己，即所有这些工作都在我们这一边，也就是人类这一边。只有这样我们才可以更清楚地了解现在，才可以更准确地追踪人们利用技术所造成的伤害，并具备更广阔的未来视野，从而可以进行有意义的民主治理与适当的监管。</p> 
<p>译者：boxi。</p> 
<p>相关阅读：</p> 
<p><a href="https://36kr.com/p/1782968488611206" target="_blank">AI 已具备人格？给有关人工智能的炒作泼泼冷水（上）</a></p>  
</div>
            