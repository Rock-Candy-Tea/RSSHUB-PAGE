
---
title: '一块英伟达3090单挑180亿参数大模型，国产开源项目这回杀疯了'
categories: 
 - 新媒体
 - 36kr
 - 资讯
headimg: 'https://img.36krcdn.com/20220517/v2_5d86a3155c5c425d87edca9e0851daa6_img_000'
author: 36kr
comments: false
date: Tue, 17 May 2022 05:40:58 GMT
thumbnail: 'https://img.36krcdn.com/20220517/v2_5d86a3155c5c425d87edca9e0851daa6_img_000'
---

<div>   
<p>什么？<strong>单块GPU</strong>也能训练大模型了？</p> 
<p>还是20系就能拿下的那种？？？</p> 
<p>没开玩笑，事实已经摆在眼前：</p> 
<p>RTX 2060 6GB普通游戏本能训练<strong>15亿</strong>参数模型；</p> 
<p>RTX 3090 24GB主机直接单挑<strong>180亿</strong>参数大模型；</p> 
<p>Tesla V100 32GB连<strong>240亿</strong>参数都能拿下。</p> 
<p class="image-wrapper"><img data-img-size-val src="https://img.36krcdn.com/20220517/v2_5d86a3155c5c425d87edca9e0851daa6_img_000" referrerpolicy="no-referrer"></p> 
<p>相比于PyTorch和业界主流的<a class="project-link" data-id="1713072453200137" data-name="Deep" data-logo="https://img.36krcdn.com/20210203/v2_06f591c995654cb28c3252e17946822e_img_png" data-refer-type="1" href="https://36kr.com/project/1713072453200137" target="_blank">Deep</a>Speed方法，提升参数容量能达到10多倍。</p> 
<p>而且这种方法完全开源，只需要几行代码就能<a class="project-link" data-id="1678461304566791" data-name="搞定" data-logo="https://img.36krcdn.com/20220331/v2_119819c55d6a4c02b99b68d6c9c37b26_img_000" data-refer-type="1" href="https://36kr.com/project/1678461304566791" target="_blank">搞定</a>，修改量也非常少。</p> 
<p class="image-wrapper"><img data-img-size-val src="https://img.36krcdn.com/20220517/v2_8626cb0efa60423fac0bd82f90f9fa3c_img_000" referrerpolicy="no-referrer"></p> 
<p>这波操作真是直接腰斩大模型训练门槛啊，老黄岂不是要血亏。</p> 
<p>那么，搞出如此大<a class="project-link" data-id="1678342390100992" data-name="名堂" data-logo="https://img.36krcdn.com/20220331/v2_803f040fa40545e498e1840c4302f52f_img_000" data-refer-type="1" href="https://36kr.com/project/1678342390100992" target="_blank">名堂</a>的是何方大佬呢？</p> 
<p>它就是<strong>国产开源项目Colossal-AI</strong>。</p> 
<p>自开源以来，曾多次霸榜GitHub热门第一。</p> 
<p class="image-wrapper"><img data-img-size-val src="https://img.36krcdn.com/20220517/v2_1bf911f199974619b887cacb65220974_img_000" referrerpolicy="no-referrer"></p> 
<p><strong>△</strong>开源地址：https://github.com/hpcaitech/ColossalAI</p> 
<p>主要做的事情就是<strong>加速各种大模型训练</strong>，GPT-2、GPT-3、ViT、BERT等模型都能搞定。</p> 
<p>比如能半小时左右预训练一遍ViT-Base/32，2天训完15亿参数GPT模型、5天训完83亿参数GPT模型。</p> 
<p>同时还能省GPU。</p> 
<p>比如训练GPT-3时使用的GPU资源，可以只是英伟达Megatron-LM的一半。</p> 
<p>那么这一回，它又是如何让单块GPU训练百亿参数大模型的呢？</p> 
<p>我们深扒了一下原理~</p> 
<h2><strong>高效利用GPU+CPU异构内存</strong></h2> 
<p>为什么单张消费级显卡很难训练AI大模型？</p> 
<p><strong>显存有限</strong>，是最大的困难。</p> 
<p>当今大模型风头正盛、效果又好，谁不想上手感受一把？</p> 
<p>但动不动就“CUDA out of memory”，着实让人遭不住。</p> 
<p class="image-wrapper"><img data-img-size-val src="https://img.36krcdn.com/20220517/v2_0da9b25a495b4ae888c6712e4fe1d93a_img_000" referrerpolicy="no-referrer"></p> 
<p>目前，业界主流方法是微软DeepSpeed提出的<strong>ZeRO</strong>（Zero Reduency Optimizer）。</p> 
<p>它的主要原理是将模型切分，把模型内存平均分配到单个GPU上。</p> 
<p>数据并行度越高，GPU上的内存消耗越低。</p> 
<p>这种方法在CPU和GPU内存之间仅使用<strong>静态划分模型数据</strong>，而且内存布局针对不同的训练配置也是恒定的。</p> 
<p>由此会导致两方面问题。</p> 
<p><strong>第一</strong>，当GPU或CPU内存不足以满足相应模型数据要求时，即使还有其他设备上有内存可用，系统还是会崩溃。</p> 
<p><strong>第二</strong>，细粒度的张量在不同内存空间传输时，通信效率会很低；当可以将模型数据提前放置到目标计算设备上时，CPU-GPU的通信量又是不必要的。</p> 
<p>目前已经出现了不少DeepSpeed的魔改版本，提出使用<strong>电脑硬盘</strong>来动态存储模型，但是硬盘的读写速度明显<strong>低于</strong>内存和显存，训练速度依旧会被拖慢。</p> 
<p class="image-wrapper"><img data-img-size-val src="https://img.36krcdn.com/20220517/v2_9203634259ef4acb9b78960c2de2a2a2_img_000" referrerpolicy="no-referrer"></p> 
<p>针对这些问题，Colossal-AI采用的解决思路是高效利用<strong>GPU+CPU的异构内存</strong>。</p> 
<p>具体来看，是利用深度学习网络训练过程中<strong>不断迭代</strong>的特性，按照迭代次数将整个训练过程分为<strong>预热</strong>和<strong>正式</strong>两个阶段。</p> 
<p>预热阶段，监测采集到非模型数据内存信息；</p> 
<p>正式阶段，根据采集到的信息，预留出下一个算子在计算设备上所需的峰值内存，移动出一些GPU模型张量到CPU内存。</p> 
<p>大概逻辑如下所示：</p> 
<p class="image-wrapper"><img data-img-size-val src="https://img.36krcdn.com/20220517/v2_3cadea32b0c24e0b82b108e6a909bfdf_img_000" referrerpolicy="no-referrer"></p> 
<p>这里稍微展开说明下，模型数据由参数、梯度和优化器状态组成，它们的足迹和模型结构定义有关。</p> 
<p>非模型数据由operator生成的中间张量组成，会根据训练任务的配置（如批次大小）动态变化。</p> 
<p>它俩常干的事呢，就是抢GPU显存。</p> 
<p class="image-wrapper"><img data-img-size-val src="https://img.36krcdn.com/20220517/v2_91e91355f7a945fc9fe6d73f38170dfe_img_000" referrerpolicy="no-referrer"></p> 
<p>所以，就需要在GPU显存不够时CPU能来帮忙，与此同时还要避免其他情况下内存浪费。</p> 
<p>Colossal-AI高效利用GPU+CPU的异构内存，就是这样的逻辑。</p> 
<p>而以上过程中，获取非模型数据的内存使用量其实<strong>非常难</strong>。</p> 
<p>因为非模型数据的生存周期并不归用户管理，现有的深度学习框架没有暴露非模型数据的追踪接口给用户。其次，CUDA context等非框架开销也需要统计。</p> 
<p>在这里Colossal-AI的解决思路是，在预热阶段用<strong>采样</strong>的方式，获得非模型数据对CPU和GPU的内存的使用情况。</p> 
<p>简单来说，这是道加减法运算：</p> 
<p><strong>非数据模型使用 ＝ 两个统计时刻之间系统最大内存使用 — 模型数据内存使用</strong></p> 
<p>已知，模型数据内存使用可以通过查询管理器得知。</p> 
<p>具体来看就是下面酱婶的：</p> 
<p class="image-wrapper"><img data-img-size-val src="https://img.36krcdn.com/20220517/v2_49c2edaafd3a4eb4bb17c75b689a3003_img_000" referrerpolicy="no-referrer"></p> 
<p>所有模型数据张量交给内存管理器管理，每个张量标记一个状态信息，包括HOLD、COMPUTE、FREE等。</p> 
<p>然后，根据动态查询到的内存使用情况，不断动态转换张量状态、调整张量位置，更高效利用GPU显存和CPU内存。</p> 
<p>在硬件非常有限的情况下，最大化模型容量和平衡训练速度。这对于AI普及化、低成本微调大模型下游任务等，都具有深远意义。</p> 
<p>而且最最最关键的是——加内存条可比买高端显卡<strong>划 算 多 了</strong>。</p> 
<p class="image-wrapper"><img data-img-size-val src="https://img.36krcdn.com/20220517/v2_ca72c48bb2d0418d95f6032ec3b70d30_img_000" referrerpolicy="no-referrer"></p> 
<p>前不久，Colossal-AI还成功复现了谷歌的最新研究成果PaLM (Pathways Language Model)，表现同样非常奈斯，而微软DeepSpeed目前还不支持PaLM模型。</p> 
<p class="image-wrapper"><img data-img-size-val src="https://img.36krcdn.com/20220517/v2_9925088fa4db416fb32004fd4904f566_img_000" referrerpolicy="no-referrer"></p> 
<h2><strong>Colossal-AI还能做什么？</strong></h2> 
<p>前面也提到，Colossal-AI能挑战的任务非常多，比如加速训练、节省GPU资源。</p> 
<p>那么它是如何做到的呢？</p> 
<p>简单来说，Colossal-AI就是一个整合了多种并行方法的系统，提供的功能包括多维并行、大规模优化器、自适应任务调度、消除冗余内存等。</p> 
<p class="image-wrapper"><img data-img-size-val src="https://img.36krcdn.com/20220517/v2_d7f38a95da2d49feacabc5e421ecf8d0_img_000" referrerpolicy="no-referrer"></p> 
<p>目前，基于Colossal-AI的加速方案FastFold，能够将蛋白质结构预测模型AlphaFold的训练时间，从原本的11天，减少到只需<strong>67小时</strong>。</p> 
<p>而且总成本更低，在长序列推理任务中，也能实现9~11.6倍的速度提升。</p> 
<p>这一方案成功超越谷歌和哥伦比亚大学的方法。</p> 
<p class="image-wrapper"><img data-img-size-val src="https://img.36krcdn.com/20220517/v2_e5017b885fdf40439ac9f8f40637e148_img_000" referrerpolicy="no-referrer"></p> 
<p>此外，Colossal-AI还能只用一半GPU数量训练GPT-3。</p> 
<p>相比英伟达方案，Colossal-AI仅需一半的计算资源，即可启动训练；若使用相同计算资源，则能提速11%，可降低GPT-3训练成本超百万美元。</p> 
<p class="image-wrapper"><img data-img-size-val src="https://img.36krcdn.com/20220517/v2_fa5794e621b944ad8844d4d2d48bb6d2_img_000" referrerpolicy="no-referrer"></p> 
<p>与此同时，Colossal-AI也非常注重开源社区建设，提供中文教程、开放用户社群论坛，根据大家的需求反馈不断更新迭代。</p> 
<p>比如之前有读者留言说，Colossal-AI要是能在普通消费级显卡上跑就好了。</p> 
<p class="image-wrapper"><img data-img-size-val src="https://img.36krcdn.com/20220517/v2_d1c0429ccb09481486ab20bbbd060da3_img_000" referrerpolicy="no-referrer"></p> 
<p>这不，几个月后，已经安排好了~</p> 
<h2><strong>背后团队：LAMB优化器作者尤洋领衔</strong></h2> 
<p>看到这里，是不是觉得Colossal-AI确实值得标星关注一发？</p> 
<p>实际上，这一国产项目背后的研发团队来头不小。</p> 
<p>领衔者，正是LAMB优化器的提出者尤洋。</p> 
<p class="image-wrapper"><img data-img-size-val src="https://img.36krcdn.com/20220517/v2_527afc339f7343ca8e4979308a7ede24_img_000" referrerpolicy="no-referrer"></p> 
<p>他曾以第一名的成绩保送清华计算机系硕士研究生，后赴加州大学伯克利分校攻读CS博士学位。</p> 
<p>拿过IPDPS/ICPP最佳论文、ACM/IEEE George Michael HPC Fellowship、福布斯30岁以下精英（亚洲 2021）、IEEE-CS超算杰出新人奖、UC伯克利EECS Lotfi A. Zadeh优秀毕业生奖。</p> 
<p>在谷歌实习期间，凭借LAMB方法，尤洋曾打破BERT预训练世界纪录。</p> 
<p>据英伟达官方GitHub显示，LAMB比Adam优化器快出整整72倍。微软的DeepSpeed也采用了LAMB方法。</p> 
<p>2021年，尤洋回国创办<strong>潞晨科技</strong>——一家主营业务为分布式软件系统、大规模人工智能平台以及企业级云计算解决方案的AI初创公司。</p> 
<p>团队的核心成员均来自美国加州大学伯克利分校、哈佛大学、斯坦福大学、芝加哥大学、清华大学、北京大学、新加坡国立大学、新加坡南洋理工大学等国内外知名高校；拥有Google Brain、IBM、Intel、 Microsoft、NVIDIA等知名厂商工作经历。</p> 
<p>公司成立即获得创新工场、真格基金等多家顶尖VC机构种子轮投资。</p> 
<p class="image-wrapper"><img data-img-size-val src="https://img.36krcdn.com/20220517/v2_2c654da9a3614503885846d4374a4529_img_000" referrerpolicy="no-referrer"></p> 
<p>潞晨CSO Prof. James Demmel为加州大学伯克利分校杰出教授、ACM/IEEE Fellow，同时还是美国科学院、工程院、艺术与科学院三院院士。</p> 
<p>传送门：https://github.com/hpcaitech/ColossalAI</p> 
<p>参考链接：https://medium.com/@hpcaitech/train-18-billion-parameter-gpt-models-with-a-single-gpu-on-your-personal-computer-8793d08332dc</p> 
<p>— <strong>完</strong> —</p> 
<p>本文来自微信公众号 <a target="_blank" rel="noopener noreferrer nofollow" href="http://mp.weixin.qq.com/s?__biz=MzIzNjc1NzUzMw==&mid=2247621950&idx=1&sn=f4756951a89faed4def5e95b6f41fd06&chksm=e8d1aa4cdfa6235a90bcd24bc8ad81007d96811d1833695b02ed52354d4277d1a513b94aa830#rd">“量子位”（ID：QbitAI）</a>，作者：明敏，36氪经授权发布。</p>  
</div>
            