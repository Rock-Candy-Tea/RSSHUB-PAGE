
---
title: '苹果承认从2019年就开始扫描iCloud Mail 寻找CSAM虐童资料'
categories: 
 - 新媒体
 - 36kr
 - 资讯
headimg: 'https://img.36krcdn.com/20210824/v2_77926ed614e84db18369fb75d4a77468_img_000'
author: 36kr
comments: false
date: Tue, 24 Aug 2021 04:05:08 GMT
thumbnail: 'https://img.36krcdn.com/20210824/v2_77926ed614e84db18369fb75d4a77468_img_000'
---

<div>   
<p>8月24日消息，苹果公司计划在今年晚些时候开始在iCloud Photos中扫描CSAM（儿童性虐待照片）内容，许多苹果粉丝对此感到不安。但事实上，苹果早已经在其他服务中使用该功能。</p> 
<p>美国当地时间周一，苹果证实其从2019年就开始在iCloud Mail上扫描CSAM内容，但称没有扫描iCloud Photos或iCloud备份信息。</p> 
<p>关于苹果的CSAM照片扫描计划已经引发了很多争议。隐私倡导者不喜欢该功能，就连许多苹果员工也对此表达了担忧。人权组织敦促苹果首席执行官蒂姆·库克(Tim Cook)，在开始向美国iPhone用户推出这个功能之前将其搁置。不过，CSAM扫描在苹果似乎并不是新功能。虽然该公司过去没有扫描过iCloud Photos，但它始终在悄悄地检查iCloud Mail，寻找虐待儿童的材料。</p> 
<p>苹果证实，该公司从2019年开始iCloud Mail中使用图像匹配技术检测CSAM内容，因为含有CSAM内容的账户违反了其条款，将被禁用。苹果还表示，该公司正在对其他数据进行有限的扫描，但没有透露详情。绝大多数iCloud Mail用户可能都不知道发生了这样的事情，不过苹果并没有对此保密。</p> 
<p><img src="https://img.36krcdn.com/20210824/v2_77926ed614e84db18369fb75d4a77468_img_000" data-img-size-val="641,321" referrerpolicy="no-referrer"></p> 
<p>苹果儿童安全网站的一个存档版本称：“苹果正使用图像匹配技术来帮助发现和报告儿童虐待行为。就像电子邮件中的垃圾邮件过滤器一样，我们的系统使用电子签名来发现可疑的虐待儿童行为。我们会验证每个异常，并进行单独审查。含有儿童虐待内容的账户违反了我们的服务条款，任何此类账户都将被禁用。”</p> 
<p>苹果首席隐私官简·霍瓦特(Jane Hovarth)也在2020年1月的消费电子展上证实了这一做法。她当时参加小组讨论时表示：“我们正在利用一些技术来帮助筛查儿童虐待材料。”但她没有提供有关苹果当时使用的技术的太多细节，也没有具体说明苹果是如何发现证据的。</p> 
<p>苹果决定扩大CSAM扫描的原因尚不完全清楚。但根据苹果公司员工在2020年的一次谈话，该公司反欺诈负责人埃里克·弗里德曼(Eric Friedman)曾称苹果是“传播儿童色情作品的最大平台”。这段谈话是在苹果与游戏开发商Epic Games的法律大战中被曝光的。</p> 
<p>苹果的扩张还可能与包括<a class="project-link" data-id="3967413" data-name="微软" data-logo="https://img.36krcdn.com/20200916/v2_811751a081924fa9af8741ce120bd7bf_img_png" data-refer-type="2" href="https://36kr.com/projectDetails/3967413" target="_blank">微软</a>在内的许多竞争对手也在扫描CSAM内容有关。苹果可能会觉得，如果其他平台都在努力打击CSAM，而苹果却视而不见，这看起来似乎不太好。</p> 
<p>本文来自“<a href="https://page.om.qq.com/page/OjnzqXD2E9tw4ZlGVraOanHA0" target="_blank">腾讯科技</a>”，审校：金鹿，36氪经授权发布。</p>  
</div>
            