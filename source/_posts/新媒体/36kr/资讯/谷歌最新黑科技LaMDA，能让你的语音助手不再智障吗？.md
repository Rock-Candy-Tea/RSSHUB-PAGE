
---
title: '谷歌最新黑科技LaMDA，能让你的语音助手不再智障吗？'
categories: 
 - 新媒体
 - 36kr
 - 资讯
headimg: 'https://img.36krcdn.com/20210521/v2_df9101f8981e4b9da0552076d639ad40_img_000'
author: 36kr
comments: false
date: Fri, 21 May 2021 10:39:14 GMT
thumbnail: 'https://img.36krcdn.com/20210521/v2_df9101f8981e4b9da0552076d639ad40_img_000'
---

<div>   
<p>编者按：本文来自<a class="project-link" data-id="3968527" data-name="微信" data-logo="https://img.36krcdn.com/20200916/v2_811751a081924fa9af8741ce120bd7bf_img_png" data-refer-type="2" href="https://36kr.com/projectDetails/3968527" target="_blank">微信</a>公众号<a href="https://mp.weixin.qq.com/s/FteWCcQ2c052bj2Bhl9H0w">“腾讯研究院”（ID:cyberlawrc）</a>，作者：王焕超，36氪经授权发布。</p> 
<p>到了今天，已经很少有人会把“智能语音助手”当回事，更多人把它看作是“智障”的同义词。</p> 
<p>自苹果的Siri在2016年发布以来，相关技术一轮又一轮地革新，模仿者一个又一个地出现。但智能助手的智能化程度，并没有我们想象中提升得那么快。</p> 
<p>不断失望之后，我们的要求也越来越低，除了让它帮忙订一个明早8:00的闹钟或打开某个App，已经别无他望。</p> 
<p>最近热播的《爱，死亡和机器人》第2季，在第1集中也告诉了我们一个智障的语音助手会带来多严重的后果：在清洁机器人“发疯”并开始无差别攻击之后，女主人打电话给<a class="project-link" data-id="457111" data-name="智能客" data-logo="https://img.36krcdn.com/20201106/v2_51591c6c738f49ae8b580d5d539ba94d_img_000" data-refer-type="2" href="https://36kr.com/projectDetails/457111" target="_blank">智能客</a>服，不但没能解决任何问题，反而一直在添乱，最终靠人的力量才勉强逃出生天。啊，难道说，我们未来仍然要承受这么智障的语音助手吗？</p> 
<p>好在事情出现了转机。美国时间2021年5月18日，一年一度的<a class="project-link" data-id="3968996" data-name="谷歌" data-logo="https://img.36krcdn.com/20200916/v2_811751a081924fa9af8741ce120bd7bf_img_png" data-refer-type="2" href="https://36kr.com/projectDetails/3968996" target="_blank">谷歌</a>I/O大会如期而至。在一众产品和技术之间，LaMDA并不起眼，但它却可能是智障语音助手的拯救者。</p> 
<h2 label="一级标题" style>LaMDA究竟是什么？</h2> 
<p>LaMDA的全称是LanguageModel for Dialogue Applications，简单而言，它是一种能力更强的语言模型，适用于对话应用程序。</p> 
<p>与前辈BERT、GPT-3一样，LaMDA也基于Transformer架构。后者是谷歌公司于2017年发布并开源的神经网络架构。利用该架构生成的模型，可以被训练阅读一组单词（比如一句话或一个段落），并且关注这些单词之间的联系，然后预测接下来会是什么单词。[1]与其他模型不同的是，LaMDA在对话方面接受了更多训练。</p> 
<p>在展开介绍之前，我们需要仔细想想，现有的语音助手为何如此“智障”？</p> 
<p>智障的根本原因是技术能力不足，具体表现为“文不对题”——不能给我们想要的答案，这一点相对还好解决，只要加大训练量就能逐渐优化。但另一个更难以解决的问题是，语音智能助手只会孤立地理解我们提出的问题，并且孤立地给出答案。换句话说，你不能指望它联系上下文语境，跟我们进行长时间的“连续对话”。</p> 
<p>要知道，我们在现实中的对话场景是完全开放性的，经常是从一个主题出发，延伸到另一个主题，最后在完全不相关的主题结束。比如，我们见到一个朋友，常常以“你吃饭了没？”打头，聊到前几天推出的一款新游戏，最后打算周末约一场电影。</p> 
<p>现实对话的开放性特征，使之成为机器学习领域最难解决的问题之一。它涉及到一项很重要的能力，即自然语言理解（NLU），要求AI能够进行语义语境情感的判断，这是比自然语言处理（NLP）还要复杂的能力。</p> 
<p>而现在大多数智能助手，往往按照狭窄的、预先定义好的对话路径被设计，并不能进行开放对话、连续对话。这就是它们看起来还相当智障的原因。</p> 
<p>而LaMDA就针对这一问题进行了技术突破。LaMDA基于谷歌2020年的一项研究[2]，这项研究显示，基于Transformer架构的语言模型在经过对话训练后，能够谈论几乎所有话题。</p> 
<p>在训练的过程中，LaMDA发现了开放式对话与其他形式语言的细微差别。它最为核心的，就是进行“开放域”（Open Domain）对话的能力。而这项能力的重要依托，就是相比于现有的对话模型，LaMDA更能理解对话的语境。它可以通过阅读句子或段落来“破译”对话意图，发现单词之间的关联，并能预测接下来可能出现的单词，从而做出合乎语境的回答。</p> 
<p>在这样的能力支撑下，LaMDA能够和人在无穷无尽的话题转换中聊下去，进行长时间的开放性对话。用谷歌官方的话来形容，就是“能够用自由流动的方式，谈论无穷无尽的主题”。</p> 
<h2 label="一级标题" style>从“冥王星”到“纸飞机”</h2> 
<p>在本次谷歌I/O大会上，LaMDA充分展示了强悍的对话能力。[3]演示环节中，LaMDA扮演了冥王星的角色，与用户进行对话。</p> 
<p><img src="https://img.36krcdn.com/20210521/v2_df9101f8981e4b9da0552076d639ad40_img_000" data-img-size-val="1080,607" referrerpolicy="no-referrer"></p> 
<p>在示例场景中，LaMDA能够根据用户的提问作出精准回答，而且还能够将一个主题，引向另一个主题，不断推进对话。这种主题的过渡并不突兀，显得自然而合理。</p> 
<p>当被问到：“你希望大家了解你的哪一面？”</p> 
<p>它这样回答：“我希望人们知道我不仅仅是一颗随机的冰球（random ice ball），我实际上是一个美丽的星球。”</p> 
<p>对于“冥王星之前是否有过到访者”的问题，LaMDA也能给出准确的答案。它甚至还贴心地提醒用户，如果要访问冥王星，需要带上大衣，因为它非常冷。</p> 
<p>这种对话给人的感觉，就像是在和一个知识渊博的朋友聊天。虽然话题天花乱坠、不断涌现，但LaMDA总能接住话茬，并且自然而然地展开对话。</p> 
<p>在另一个演示中，LaMDA也展现了高超的对话能力。</p> 
<p><img src="https://img.36krcdn.com/20210521/v2_6fac68dd3276491fa98cf87752f50926_img_000" data-img-size-val="1080,603" referrerpolicy="no-referrer"></p> 
<p>在这个演示中，LaMDA扮演的角色是一架<a class="project-link" data-id="477187" data-name="纸飞机" data-logo="https://img.36krcdn.com/20200729/v2_ff0884df80c04fd2970d6c6de2df3d2a_img_000" data-refer-type="2" href="https://36kr.com/projectDetails/477187" target="_blank">纸飞机</a>。当用户问，你曾经最糟糕的着陆地点是什么？它回答说：“可能是一个小水洼（puddle）。”</p> 
<p>当被用户问到：“一架真正好的纸飞机的秘诀是什么？”</p> 
<p>它主动追问用户：“‘好’是什么意思？”，体现了足够的灵活和机敏。</p> 
<p>用户回答：“我关心距离（distance）。”LaMDA进而围绕“如何优化纸飞机的飞行距离”这一话题，分享了相关知识。</p> 
<p>要知道，LaMDA的这些回复都不是预先设定的，而是自然生成的。这也就意味着，LaMDA不必再经过专门的训练才能进行另一次对话，也不会作出重复的回答。这样的能力确实令人惊奇。</p> 
<p>这两个示例中，仅凭寥寥几语就能看出LaMDA使问题应答更有意义了，而这就是理解对话语境能力带来的结果。在这样的能力辅助下，LaMDA表现得相当理智和机敏。</p> 
<p>谷歌公司也表示，理智和特异性并不是LaMDA所追求的唯一品质。他们还注重洞察力、幽默感等能力。与此同时，谷歌也非常关注事实性问题，也就是LaMDA的回答是否符合事实。[4]毕竟对于一个语音助手来说，有趣很重要，正确更重要。</p> 
<h2 label="一级标题" style>LaMDA的前路仍然遥远</h2> 
<p>无论是更先进的AI还是更智能的聊天机器人，谷歌在过去几年一直在着力促进AI如何更好地与人类沟通。</p> 
<p>皮查伊在演讲中提到，语言的丰富性和灵活性正在使其成为人类最伟大的工具之一，同时，它也成为计算科学的最大挑战之一。虽然现在LaMDA可以根据对话语境提供建议和答案，让对话不违和地进行下去，但它仍在研发初期，想要达到AI助手的功能，还需要时间的磨合。</p> 
<p>问题是，提升AI助手的对话能力，究竟有什么意义？至少对于谷歌而言，这项能力作用重大，因为谷歌的很多重要产品都与信息检索有关，它们都基于对计算语言的解读，无论是翻译能力，还是对用户检索信息的理解。如果谷歌能让AI更好地理解语言，那么它就能改进相关的核心产品，比如Google Search、Assistant和Workspace。“它甚至可以将搜索变成对话，更自然流畅。”皮查伊如是说。</p> 
<p>当然也不单单是对谷歌一家公司，对话能力的进步突破，无疑会给所有涉及到人机对话的领域带来全新想象力。</p> 
<p>但语言的丰富性、灵活性以及随之伴生的复杂性，无疑使这项工作成为极大的挑战。可以说，面对这样一个困难领域，LaMDA的能力还称不上成熟。在现实运行中，它仍可能出错，给出荒谬的回应。</p> 
<p>比如，在扮演冥王星的演示案例中，它就说到自己跳得很高（jump really high），经常练习翻转动作，并且很乐于用它最喜欢的球——月球来玩接球游戏。这些回答显然是违背常识的。</p> 
<p>除此之外，作为语言模型，LaMDA也无可避免地面临一些AI的老问题。比如它可能会被滥用或者传播偏见。算法偏见是个极为复杂的问题，既可能源于算法结构设计，也可能是训练数据集的问题，它的本质是社会偏见在算法层面的延伸。</p> 
<p>如谷歌所言，“语言可能是人类最伟大的工具之一，但像所有工具一样，它可能会被滥用。接受语言训练的模型可能传播这种滥用行为——例如，通过内化偏见、反映仇恨言论或复制误导性信息。即使它所训练的语言经过仔细审查，模型本身仍然可能不被善用。”[5]</p> 
<p>当然，LaMDA还会面临许多意想不到的现实风险。比如被违法犯罪分子用于网络欺诈，类似的新闻已经屡见不鲜。更仿真的对话能力，也就意味着更强大的欺诈能力。</p> 
<p>即便在技术层面，LaMDA也有更大的优化空间。目前，LaMDA主要是围绕文本对话进行构建的，在未来，LaMDA可能会兼容其他的媒介形态，包括图像、音频、视频等等。这可以寄希望于同在本次大会发布的MUM（多任务统一模型），未来人机交互手段或许会因这两项技术而出现革命性的变化。</p> 
<p>LaMDA的具体作用如何，还待进一步观察，毕竟Google之前有过黑历史（2017年，Google发布了餐厅订位服务AI Duplex，后来被发现背后有真人帮忙完成[6]）。不过，人们和AI进行更自然、开放的对话，相信已经不再遥远了。</p> 
<p>参考资料：</p> 
<p>1.https://www.blog.google/technology/ai/lamda</p> 
<p>2.https://ai.googleblog.com/2020/01/towards-conversational-agent-that-can.html</p> 
<p>3.https://www.youtube.com/watch?v=aUSSfo5nCdM</p> 
<p>4.https://www.blog.google/technology/ai/lamda</p> 
<p>5.https://www.blog.google/technology/ai/lamda</p> 
<p>6. https://wallstreetcn.com/articles/3567850</p>  
</div>
            