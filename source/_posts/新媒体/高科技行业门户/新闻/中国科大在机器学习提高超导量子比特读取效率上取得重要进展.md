
---
title: '中国科大在机器学习提高超导量子比特读取效率上取得重要进展'
categories: 
 - 新媒体
 - 高科技行业门户
 - 新闻
headimg: 'https://cors.zfour.workers.dev/?http://mp.ofweek.com/data/images/c114/2021-09-03/4286d2dca8d8f0804d2c5ec772a6058e.jpg'
author: 高科技行业门户
comments: false
date: Mon, 06 Sep 2021 03:35:00 GMT
thumbnail: 'https://cors.zfour.workers.dev/?http://mp.ofweek.com/data/images/c114/2021-09-03/4286d2dca8d8f0804d2c5ec772a6058e.jpg'
---

<div>   
<p style="text-indent: 2em; text-align: left;">C114讯 9月3日消息（余予）来自中国科大的消息显示，中国科大郭光灿院士团队近日在机器学习提高超导量子比特读取效率上取得重要进展。该团队郭国平教授研究组与本源量子计算公司合作，在本源“夸父”6比特超导量子<span class="hrefStyle"><a href="http://fiber.ofweek.com/IND-8320185-xinpian.html" target="_blank" title="芯片">芯片</a></span>上研究了串扰对量子比特状态读取的影响，并创新性地提出使用浅层神经网络来识别和读取量子比特的状态信息，从而大幅度抑制了串扰的影响，进一步提高了多比特读取保真度。</p><p style="text-indent: 2em; text-align: left;">据了解，近些年，国际上分别实现了高保真度的单比特单发读取以及多比特的多路复用式单发读取；然而，由于各种形式的杂散耦合的存在，邻近比特的状态可能会对目标比特的测量结果产生影响，从而降低测量保真度，进而降低量子算法的成功率。随着量子芯片的进一步扩展，为了进一步提高读取保真度，如何解决上述串扰问题将成为研究者们面临的主要挑战。</p><p style="text-align:center"><img width="700" height="321" alt src="https://cors.zfour.workers.dev/?http://mp.ofweek.com/data/images/c114/2021-09-03/4286d2dca8d8f0804d2c5ec772a6058e.jpg" referrerpolicy="no-referrer"></p><p style="text-indent: 2em; text-align: center;">图1 传统量子比特读取方案以及串扰的影响</p><p style="text-indent: 2em; text-align: left;">为了解决读取串扰的问题，在此之前，国际上其他课题组的主要集中在如何从硬件层面抑制串扰，例如为每一个量子比特的读取腔单独配置一个读取滤波器，或者增大读取腔之间的空间和频域距离；这些方案虽然在一定程度上抑制了串扰，但是都对量子芯片的扩展和集成产生了不利的影响。</p><p style="text-align:center"><img width="700" height="307" alt src="https://cors.zfour.workers.dev/?http://mp.ofweek.com/data/images/c114/2021-09-03/3a1e29858340c678f19c887379d85508.png" referrerpolicy="no-referrer"></p><p style="text-indent: 2em; text-align: center;">图2 第一代“夸父”6比特超导量子芯片结构图</p><p style="text-indent: 2em; text-align: left;">基于以上，郭国平教授研究组与本源量子计算公司合作，通过对量子比特信息提取过程的抽象和模拟，提出一种新的量子比特读取方案：通过训练基于数字信号处理流程构建的浅层神经网络，实现对量子比特状态的精确识别与分类。</p><p style="text-indent: 2em; text-align: left;">研究人员将这一方案应用到本源“夸父”6比特超导量子芯片上，实验发现，新的读取方案不仅有效提升了6比特的读取保真度，而且大幅度抑制了读取串扰效应；同时，由于新方案中的数据处理可以进一步简化为单步矩阵运算，未来可以直接转移到FPGA上，从而实现对量子比特状态的0延时判断以及对量子比特的实时反馈控制。</p><p style="text-indent: 2em; text-align: left;">该方案不仅适用于超导量子计算，也同时适用于其他量子计算物理实现方案。</p><p style="text-align:center"><img width="424" height="245" alt src="https://cors.zfour.workers.dev/?http://mp.ofweek.com/data/images/c114/2021-09-03/3ff24dcf29fbc83ed03b4d7306b993c8.png" referrerpolicy="no-referrer"></p><p style="text-indent: 2em; text-align: center;">图3 用于量子比特状态读取与分类的浅层神经网络结构</p><p style="text-indent: 2em; text-align: left;">该成果于近日在国际应用物理知名期刊《Physical Review Applied》上。中科院量子信息重点实验室段鹏博士和陈梓峰硕士为文章共同第一作者，郭国平教授为通讯作者。该工作得到了科技部、国家基金委、中国科学院和安徽省的资助。</p><p style="text-indent: 2em; text-align: left;"><span style="text-indent: 2em;">作者： 余予来源：C114通信网</span></p> 
  
</div>
            