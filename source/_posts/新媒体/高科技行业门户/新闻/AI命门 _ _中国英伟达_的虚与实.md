
---
title: 'AI命门 _ _中国英伟达_的虚与实'
categories: 
 - 新媒体
 - 高科技行业门户
 - 新闻
headimg: 'https://mp.ofweek.com/Upload/News/Img/member36344/202112/wx_article__1f1ee2f412b604e9e078e658708187e9.jpg'
author: 高科技行业门户
comments: false
date: Wed, 22 Dec 2021 02:00:00 GMT
thumbnail: 'https://mp.ofweek.com/Upload/News/Img/member36344/202112/wx_article__1f1ee2f412b604e9e078e658708187e9.jpg'
---

<div>   
<p style="text-align:center"><img src="https://mp.ofweek.com/Upload/News/Img/member36344/202112/wx_article__1f1ee2f412b604e9e078e658708187e9.jpg" referrerpolicy="no-referrer"></p><p style="text-indent: 2em; text-align: left;">本文系基于公开资料撰写，仅作为信息交流之用，不构成任何投资建议。  </p><p style="text-indent: 2em; text-align: left;"><span style="text-indent: 2em;">最近一个周期，我国在前沿硬科技领域投入越来越大，相关论文发表和论文引用量逐渐开始占领世界第一的位置。数量上的进步鼓舞人心，但质量上的全面突围还需要耐心。</span><br></p><p style="text-indent: 2em; text-align: left;">比如GPU（图形处理器）。</p><p style="text-indent: 2em; text-align: left;"><span style="text-indent: 2em;">某种意义上，它当前是通往AI世界那道命门：由于AI算法属于计算密集型算法，海量的数据处理任务和<span class="hrefStyle"><a href="https://www.ofweek.com/ai/" target="_blank" title="人工智能">人工智能</a></span>的训练和推理更适合GPU、FPGA、ASIC等并行计算能力强的芯片。</span><br></p><p style="text-indent: 2em; text-align: left;">众所周知的是，这些先进芯片的生产和制造基本都被把握在海外厂商手上。 </p><p style="text-indent: 2em; text-align: left;"><span style="text-indent: 2em;">故而对于中国科技产业来说，自建GPU等核心AI底层硬件，是一件既刻不容缓又需要极大耐心的工程。</span><br></p><p style="text-indent: 2em; text-align: left;">自2017年以来，多个中国新一代初创型GPU研发公司相继成立，正逐渐走上历史舞台。而作为二级市场中独一份的国产标的，A股GPU 第一股的景嘉微（SZ：300474），也凭借股价的年内翻倍走势而愈加受到关注。</p><p style="text-indent: 2em; text-align: left;"><span style="text-indent: 2em;">本文作为我们聚集中国GPU产业的系列研究开篇，将以景嘉微为线索，在探究其虚实同时，力求延伸出更多的产业维度思考。 </span><br></p><p style="text-indent: 2em; text-align: left;"><strong>01 乌龙研报 </strong></p><p style="text-indent: 2em; text-align: left;"><span style="text-indent: 2em;">2016年，GPU业界诞生了一块被戏称“老黄手抖切多了”的神卡：GTX1080。 这块显卡是英伟达（NASDAQ：NVDA）在新架构（Pascal）上的第一个作品，作为首发先锋，秒掉了上一代架构（Maxwell）的所有卡。该卡在314mm?的核心面积里面塞进去了72亿晶体管。时至今日，GTX1080都能被称之为是一块高性能卡，能够满足绝大部分游戏需求。 不过根据某券商在9月17日的一份报告中提到，景嘉微拥有完全自主知识产权的的JM9系列GPU流片成功了，“产品性能与GTX1080相当”。如果该产品性能真的如研报所示，那么该产品一旦量产成功可直接改写中低端GPU国内市场势力格局。</span><br></p><p style="text-align:center"><img src="https://mp.ofweek.com/Upload/News/Img/member36344/202112/wx_article__fbfd10c720c74d6cab774f48bb80cac7.jpg" referrerpolicy="no-referrer"></p><p style="text-indent: 2em; text-align: center;">图片来源：公司公告 </p><p style="text-indent: 2em; text-align: left;"><span style="text-indent: 2em;">上图是公司目前公布的技术指标。可以看到的是，理想很美好，现实仍骨感：</span></p><p style="text-indent: 2em; text-align: left;">视频解码方面能够达到目前的主流水平，支持4K60支持H265，已经足够日常看视频使用了。3D图形支持OpenGL 4．0，目前最新的OpenGL是4．6，也算是跟上前沿步伐。</p><p style="text-indent: 2em; text-align: left;">最后是支持的平台，支持X86和ARM，这意味着可以支持x86下的linux操作系统。这使中标麒麟、UOS、银河麒麟等国产操作系统的支持就有了可能。<br>但，在算力层面，JM9系列跟GTX1080还不在一个级别。</p><p style="text-align:center"><img src="https://mp.ofweek.com/Upload/News/Img/member36344/202112/wx_article__fb29aeb4d8ab11494cda4c5a8d841477.jpg" referrerpolicy="no-referrer"></p><p style="text-indent: 2em; text-align: left;">像素填充率表示的是GPU每秒能够渲染填充进画面的像素数量。单精度浮点性能是GPU每秒能完成计算的数量。简单来说这两个指标都能表达GPU的算力情况。如果说从数据推算上能打赢GTX1080的话，唯一能比的就是功耗算力比。</p><p style="text-indent: 2em; text-align: left;"><span style="text-indent: 2em;">可问题在于，脱离了绝对性能和架构PK功耗算力比，完全是关公战秦琼了。按这个说法，高通骁龙888功耗10W，但其所集成的GPU算力在1．7TFlops，比景嘉微JM9还要高一截。于是把骁龙888装进PC里就能比GTX1080更强大么？恐怕不是这么比的。</span><br></p><p style="text-indent: 2em; text-align: left;">出现这种说法应该是研究员在数据比对中出现了什么误会。<br></p><p style="text-align:center"><img src="https://mp.ofweek.com/Upload/News/Img/member36344/202112/wx_article__ad12f93d1360bdf56345e399a64b916c.jpg" referrerpolicy="no-referrer"></p><p style="text-indent: 2em; text-align: center;">图：相关券商研报 </p><p style="text-indent: 2em; text-align: left;"><span style="text-indent: 2em;">从使用场景来说JM9 系列图形处理芯片产品可满足地理信息系统、媒体处理、CAD 辅助设计、游戏、虚拟化等高性能显示需求和人工智能计算需求。</span><br></p><p style="text-indent: 2em; text-align: left;">对于前面的专业需求来说，景嘉微作为国产自主设计GPU厂商在政采和军供方面是必须要满足的。在游戏和虚拟化层面，由于JM9支持Linux内核的操作系统，这样的性能还是能够玩一些“老游戏”的。但是支持人工智能计算又是从何说起呢？ </p><p style="text-indent: 2em; text-align: left;"><strong>02 AI命门 </strong></p><p style="text-indent: 2em; text-align: left;"><span style="text-indent: 2em;">CPU（中央处理器）和GPU（图像处理器）在设计目标上都是为了完成计算的用途。两者的区别在于芯片内部的设计结构差异。CPU擅长完成较难和较长的任务，而GPU凭借多运算核心可以同时执行多个较为简单的任务。</span><br></p><p style="text-indent: 2em; text-align: left;">例如在把不同像素填充进图形的渲染任务中，CPU的执行方式是分别计算出每一个像素的具体颜色再填充进图形。而GPU可以凭借核心数量多的优势同时计算全部的像素颜色，再同时填充进图形。虽然CPU可以凭借足够快的计算速度来完成这一工作，但小汽车就算开得再快也难于与大货车的运货能力相抗衡。<br></p><p style="text-align:center"><img src="https://mp.ofweek.com/Upload/News/Img/member36344/202112/wx_article__502d6d23c4d06548371122f6f742f3a0.jpg" referrerpolicy="no-referrer"></p><p style="text-indent: 2em; text-align: center;">图片绿色区域是运算核心，来源：NVIDIA </p><p style="text-indent: 2em; text-align: left;"><span style="text-indent: 2em;">CUDA技术文档 人工智能中使用的<span class="hrefStyle"><a href="http://znyj.ofweek.com/CAT-23013-8300-TechnologyApplication.html" target="_blank" title="机器学习">机器学习</a></span>模型是通过模拟生物神经系统来建立的数学网络模型，需要大量数据来训练模型，对计算机处理器的要求是需要大量并行计算。这就要求芯片能够提供多核并行计算能力，核心数量多，拥有更高的浮点运算能力和访存速度的能力，以此来提升深度神经网络的训练速度。 在当前AI应用中，计算芯片在神经网络训练中需要解决两个限制。第一是芯片内部到外部的带宽以及片上缓存空间对运算的限制。第二是在控制功耗的同时不断提升专用计算能力，解决对卷积、残差网络等各类AI计算模型的大量计算。虽然在今天能够满足上述两种需求的芯片主要是GPU、ASIC、FPGA，但这三者同时也具有不同的优缺点。</span></p><p style="text-align:center"><img src="https://mp.ofweek.com/Upload/News/Img/member36344/202112/wx_article__2db5cd59f5212903ffdb52e1c43be141.jpg" referrerpolicy="no-referrer"></p><p style="text-indent: 2em; text-align: left;">当前AI发展还处在一个相对早期的阶段，应用层面分化仍不明显。例如在挖矿工作中，需要计算芯片连续不断执行同一种简单逻辑操作，ASIC作为最快速的芯片就比较适合。在通信行业中，针对接受不同信号并转码的任务交给单一软件逻辑的FPGA是当前主要的研究方向。</p><p style="text-indent: 2em; text-align: left;"><span style="text-indent: 2em;">但在机器视觉、自动驾驶方面，由于算法更迭快，不能支持ASIC每次特定设计的生产成本。同理，FPGA不能支持神经网络的逻辑递归处理。中长期来看，GPU得益于通用性和制程仍有进步空间的优势，仍然是主流AI研究和开发平台。甚至可以说，AI行不行，得先在GPU上跑通了再说。 在未来，通过AI处理或辅助的方式能够大大优化生产过程和提高生活质量。对AI科技公司来说，拥有更多开发人员和更多前言研究论文固然重要。但在没有或受限的硬件条件下进行AI研究，无疑是空中楼阁。 国产自有GPU补充前沿科技研究的需求，呼之欲出。 </span><br></p><p style="text-indent: 2em; text-align: left;"><strong>03引申思考 </strong></p><p style="text-indent: 2em; text-align: left;"><span style="text-indent: 2em;">高新科技的研究和进步远远不是单一领域突破就能实现的，一味追求追高追新的心态有可能会重蹈2003年的“汉芯事件“。 在自研GPU的研发领域上，我们应该关注不应该是谁的产品最有噱头，更应该看的是谁的核心技术最全面，谁能持续稳健的完成技术飞跃。回到景嘉微的JM9系列，深入研究发现这家公司不简单，可能是未来国产GPU领军龙头。 景嘉微成立于2006年，公司产品主要分为图形图像处理系统、小型雷达系统、GPU芯片，广泛应用于军工行业。公司图形显控领域产品包括图显模块和加固类产品，其中图显模块是核心产品。 景嘉微成立当年，恰逢我国军用飞机图形显控系统由使用DSP与FPGA图形加速器向使用GPU图形处理器升级，公司准确把握机遇，将大量资源投入飞机图形显控领域的研究。</span><br></p><p style="text-indent: 2em; text-align: left;">在当时国内外主要采用的是ATI的M9芯片，景嘉微研发VxWorks嵌入式操作系统下M9芯片驱动程序，并于2007年研发成功。虽然芯片驱动距离芯片制造还差了十万八千里，但有自己的驱动来控制芯片，这仍算得上是打响我国图形显控模块摆脱外商依赖第一枪。 2014年，第一代图形处理芯片（GPU）JM5400研制成功，性能优于军工电子显控领域主流进口芯片。2018年，第二代图形处理芯片（GPU）JM7200研制成功，在第一代基础上有较大进步。目前JM5400系列已应用于多种军用显控系统中，JM7200已获得军工意向订单。</p><p style="text-indent: 2em; text-align: left;"><span style="text-indent: 2em;">从这一代开始，景嘉微基本包揽了全部军用领域的图显和显控订单。顺着这代性能的提升，做了芯片与国产CPU的适配，推出了JM7201希望进入到民用市场。这块JM7201的性能应今天的眼光来看，相当于是以独立显卡达到了彼时搭载在国外主板上的集成显卡的水平。 那么如今发布的JM9系显卡究竟是什么水平？直白来说，以这个公布的数据来看，JM9系列相当于80％参数水平的GTX950。而GTX950是在2015年中高端显卡。</span><br></p><p style="text-indent: 2em; text-align: left;">但是，作为一款自主知识产权显卡，其对功耗的控制和恶劣环境的鲁棒性这点是毋庸置疑值得夸赞的。在目前公开版本后续会不会有“加料“的商用版出现也是极有可能的。从参数来说，JM9已经能够完美的解决办公需求，未来在ToG的增量可以期望。 在最后，想说几句题外话。高科技的研发过程是漫长的，只靠授权开发和承他人之荫是不能突破技术封锁的。对任何一家处于成长中的科技硬件公司，重要的不是现在有什么，而是其产品的必要性和愿意研发并持续进步的决心。对于现实产品，不能专业指出缺点反而无脑吹捧不会有裨益，反而会引起不明情况消费者的疑惑。</p><p style="text-indent: 2em; text-align: left;"><span style="text-indent: 2em;">在未来，科技上的博弈还会持续相当久的时间。前沿技术的发展我们不能停滞，提供这些技术诞生的载体（国产CPU、GPU、内存等）更是不容懈怠。 毕竟，东西是自己的才是硬道理。</span></p> 
  
</div>
            