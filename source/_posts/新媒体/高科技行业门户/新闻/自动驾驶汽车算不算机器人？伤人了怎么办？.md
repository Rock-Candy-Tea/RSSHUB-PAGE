
---
title: '自动驾驶汽车算不算机器人？伤人了怎么办？'
categories: 
 - 新媒体
 - 高科技行业门户
 - 新闻
headimg: 'https://picsum.photos/400/300?random=3455'
author: 高科技行业门户
comments: false
date: Sun, 11 Apr 2021 02:25:00 GMT
thumbnail: 'https://picsum.photos/400/300?random=3455'
---

<div>   
<p style="text-indent: 2em; text-align: left;">／ 导读 ／</p><p style="text-indent: 2em; text-align: left;">涉及<span class="hrefStyle"><a href="https://www.ofweek.com/auto/CAT-70109-automaticdriving.html" target="_blank" title="自动驾驶汽车">自动驾驶汽车</a></span>事故的新闻总是上头条。一辆<span class="hrefStyle"><a href="https://www.ofweek.com/auto/CAT-70109-automaticdriving.html" target="_blank" title="自动驾驶">自动驾驶</a></span><span class="hrefStyle"><a href="https://www.ofweek.com/auto/" target="_blank" title="汽车">汽车</a></span>一百次有99次都能“认出”行人并在撞到他／她之前停车，这是研究和试验的可喜成果，但在现实世界中这辆车却是一台杀人机器。如何为自动驾驶汽车制定强有力的、可验证的安全规则，让这1％的事故成为过去呢？</p><p style="text-indent: 2em; text-align: left;">无限不可预测道路的安全才是自动驾驶</p><p style="text-indent: 2em; text-align: left;">几十年来，机器人（无人）车辆一直在危险环境中使用，包括退役的福岛核电站或北海水下能源基础设施的检查。最近，低速的物流送货车等自动驾驶车辆已经从研究中心走入现实世界，很少出现问题。</p><p style="text-indent: 2em; text-align: left;"><strong>自动驾驶汽车不可能像自动送货机器人一样简单</strong></p><p style="text-indent: 2em; text-align: left;">然而，人们承诺的自动驾驶汽车的到来并没有超越测试阶段。在2018年的一次优步自动驾驶汽车试驾中，一名行人被该车撞死。此后，使用自动驾驶功能事故的报道不绝于耳。尽管这样的事故每天都发生在人类驾驶中，但公众对<span class="hrefStyle"><a href="https://auto.ofweek.com/tag-%E6%97%A0%E4%BA%BA%E9%A9%BE%E9%A9%B6%E6%B1%BD%E8%BD%A6.HTM" target="_blank" title="无人驾驶汽车">无人驾驶汽车</a></span>的安全标准要高得多，会将一次性事故解释为：这些车辆在公共道路上行驶太不安全。</p><p style="text-indent: 2em; text-align: left;">设计一辆完美的自动驾驶汽车，让它总是做出最安全的决定是一项艰巨的技术任务。自动驾驶汽车一般是在严格控制的环境中进行实测的，而只有在无限不可预测的道路网络中运行，快速处理许多复杂的变量并保证安全，才是真正的自动驾驶。</p><p style="text-indent: 2em; text-align: left;"><strong>自动驾驶汽车与机器人定律</strong></p><p style="text-indent: 2em; text-align: left;">1942年，科幻作家艾萨克·阿西莫夫（Isaac Asimov）在他的短篇小说《绕线跑》（Runaround）中为机器人设计了一套规则——三条定律。由9篇科幻短篇小说组成了《机器人》（1950）一书。随着其规则被大众文化所接受，经常被人们称为阿西莫夫定律或三定律。</p><p style="text-indent: 2em; text-align: left;">《机器人手册》（公元2058年第56版）书中的三条定律是：</p><p style="text-indent: 2em; text-align: left;">定律1：机器人不得伤害人，也不得通过不作为而使人受到伤害；</p><p style="text-indent: 2em; text-align: left;">定律2：机器人必须服从人类发出的命令，除非这些命令与第一定律相冲突；</p><p style="text-indent: 2em; text-align: left;">定律3：只要机器人的保护不与第一或第二定律相冲突，机器人必须保护自己的存在。</p><p style="text-indent: 2em; text-align: left;">它们像不像专门为今天的自动驾驶汽车制定的法规？</p><p style="text-indent: 2em; text-align: left;">自动驾驶汽车会遵守机器人三定律吗？</p><p style="text-indent: 2em; text-align: left;">阿西莫夫后来在《机器人与帝国》（1985）一书中引入了“机器人第零条定律”。这项新定律优先于前三条，规则为：</p><p style="text-indent: 2em; text-align: left;">定律0：机器人不可以伤害人类，或者不作为，允许人类受到伤害。</p><p style="text-indent: 2em; text-align: left;">自动驾驶汽车就是机器人汽车（Robotic vehicle）。现实是，当自动驾驶汽车伤害人类时，它们显然违反了第一条定律。</p><p style="text-indent: 2em; text-align: left;"><strong>帮自动驾驶汽车做出最安全决定的规则</strong></p><p style="text-indent: 2em; text-align: left;">早在2013年，英国政府就将机器人和自主系统领域作为支撑英国就业和增长工业战略的8项伟大技术之一。</p><p style="text-indent: 2em; text-align: left;">最近，英国国家机器人博物馆（National Robotarium）正在进行一项领先的研究项目AISEC，旨在保证自动驾驶车辆在做出决定时始终遵守“机器人定律”。没有这样的保证就会出现非常严重的安全问题，阻碍自动驾驶汽车在全球的起飞。</p><p style="text-indent: 2em; text-align: left;">该机器人博物馆是世界领先的机器人和人工智能（AI）中心，不仅研究机器人技术理论和发展，也研究与机器人和自主系统领域相关的伦理问题，为应对全球挑战创造创新解决方案。其开创性的研究不断从实验室走向市场，同时培养有远见的技术人才，为社会带来实质性的效益。</p><p style="text-indent: 2em; text-align: left;">机器人伦理学家Patrick Lin表示：“受高速公路法规的启发，我们正在制定一套规则，帮助自动驾驶汽车在任何可能的情况下做出最安全的决定。验证这些规则是否有效是我们必须克服的最后一个障碍，以使值得信赖的自动驾驶汽车安全上路。”</p><p style="text-indent: 2em; text-align: left;">他认为，人工智能软件实际上非常善于了解它从未面对过的情景。利用从人脑布局中获得灵感的“神经网络”，软件可以发现数据中的模式，比如汽车和行人的运动，然后在新奇的场景中回忆这些模式。</p><p style="text-indent: 2em; text-align: left;">他也承认：“我们仍然需要证明任何教给自动驾驶汽车的安全规则在这些新情况下都会起作用。要做到这一点，我们可以转向形式验证——计算机科学家用来证明规则在所有情况下都有效的方法。”</p><p style="text-indent: 2em; text-align: left;">他举例说，在数学中，规则可以证明x＋y等于y＋x，而无需测试x和y的所有可能值。形式验证也可以做类似的事情：它允许我们证明人工智能软件将如何对不同的场景做出反应，而无需我们对公路上可能发生的每个场景进行详尽的测试。</p><p style="text-indent: 2em; text-align: left;">该领域最近取得的一个比较显著的成功是验证了一个人工智能系统，该系统使用神经网络来避免<span class="hrefStyle"><a href="https://www.ofweek.com/auto/tag-%E6%97%A0%E4%BA%BA%E9%A9%BE%E9%A9%B6.HTM" target="_blank" title="无人驾驶">无人驾驶</a></span>飞机之间的碰撞。研究人员已经成功地证实，无论飞机的水平和垂直运动如何，系统都能正确响应。</p><p style="text-indent: 2em; text-align: left;"><strong>自动驾驶汽车决策的复杂性</strong></p><p style="text-indent: 2em; text-align: left;">Patrick Lin在介绍自动驾驶汽车自主决策的复杂性时表示，人类驾驶者遵循高速公路守则来保护所有道路使用者的安全，这依赖于人脑对这些规则的学习，并在无数的现实场景中明智地应用这些规则。</p><p style="text-indent: 2em; text-align: left;">他说：“我们也可以教自动驾驶汽车学习高速公路守则。这就要求我们解开代码中的每一条规则，让车辆的神经网络理解如何遵守每一条规则，然后验证它们在任何情况下都可以安全地遵守这些规则。”</p><p style="text-indent: 2em; text-align: left;">自动驾驶汽车需要学习各种规则</p><p style="text-indent: 2em; text-align: left;">不过，在审查《公路法》中“绝不能”一词的后果时，验证这些规则是否会被安全遵守的挑战相当复杂。要使自动驾驶汽车在任何特定情况下都像人类驾驶员一样反应灵敏，就必须以这样一种方式来规划这些策略，即考虑细微差别、加权风险和不同规则直接冲突的偶发场景，要求汽车忽略其中一个或多个规则。</p><p style="text-indent: 2em; text-align: left;">这样的任务不能只留给程序员，它需要律师、安全专家、系统工程师和决策者的投入。在新启动的AISEC项目中，研究人员正在设计一种工具，以促进建立自动驾驶汽车道德和法律标准所需的跨学科合作。</p><p style="text-indent: 2em; text-align: left;">Patrick Lin说：“教自动驾驶汽车变得完美是一个动态的过程：取决于法律、文化和技术专家如何随着时间的推移来定义完美。AISEC工具正是基于这一点而构建的，它提供了一个‘任务控制面板’，用于监控、补充和调整最成功的自动驾驶汽车管理规则，这些规则随后将提供给整个行业。”</p><p style="text-indent: 2em; text-align: left;">他最后说：“我们希望在2024年交付AISEC工具的第一个实验原型。但我们仍然需要创建自适应验证方法来解决剩余的安全问题，这些方法可能需要数年的时间才能构建并嵌入到自动驾驶汽车中。”</p> 
  
</div>
            