
---
title: '单GPU每秒76帧，重叠对象也能完美分割，多模态Transformer用于视频分割效果惊艳'
categories: 
 - 新媒体
 - ZAKER
 - channel
headimg: 'https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202203/62259f3c8e9f09359879ee5c_1024.jpg'
author: ZAKER
comments: false
date: Sun, 06 Mar 2022 23:10:35 GMT
thumbnail: 'https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202203/62259f3c8e9f09359879ee5c_1024.jpg'
---

<div>   
<p>视频分割效果优于所有现有方法，这篇入选 CVPR 2022 的论文是用 Transformer 解决 CV 任务的又一典范。</p><p>基于注意力的深度神经网络（DNN）在 NLP 和 CV 等不同领域的各种任务上都表现出了卓越的性能。这些进展使得此类网络（如 Transformer）成为解决多模态问题的有力候选。特别是近一两年，Transformer 模型已经开始在 CV 任务上大展手脚，从目标识别到检测，效果优于通用的 CNN 视觉骨干网络。</p><p>参考视频对象分割（referring video object segmentation, RVOS）任务涉及到给定视频帧中文本参考对象实例的分割。相比之下，在得到更广泛研究的参考图像分割（referring image segmention, RIS）任务中，对象主要通过它们的外观进行参考。在 RVOS 中，对象可以通过它们正在执行或参与的动作进行参考。这使得 RVOS 比 RIS 复杂得多，因为参考动作的文本表达通常无法从单个静态帧中推导出来。</p><p>此外，与基于图像的 RIS 不同，RVOS 方法可能还需要跨多个帧（即跟踪）来建立参考对象的数据关联，以处理遮挡或运动模糊这类的干扰。</p><p>为了解决这些挑战，现有 RVOS 方法往往依赖复杂的 pipeline。在被 CVPR 2022 接收的一篇论文《End-to-End Referring Video Object Segmentation with Multimodal Transformers》中，来自以色列理工学院的研究者提出了一种简单的、基于 Transformer 的端到端 RVOS 方法—— Multimodal Tracking Transformer（MTTR ）。</p><p></p><div class="img_box" id="id_imagebox_0" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_0" data-original="http://zkres1.myzaker.com/202203/62259f3c8e9f09359879ee5c_1024.jpg" data-height="274" data-width="1080" src="https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202203/62259f3c8e9f09359879ee5c_1024.jpg" referrerpolicy="no-referrer"></div></div>论文地址：https://arxiv.org/pdf/2111.14821.pdf<p></p><p>项目地址：https://github.com/mttr2021/MTTR</p><p>Huggingface Spaces Gradio demo：https://huggingface.co/spaces/akhaliq/MTTR</p><p>具体地，他们使用 MTTR 将任务建模成序列预测问题。给定一个视频和文本查询，该模型在确定文本参考的对象之前为视频中所有对象生成预测序列。并且，他们的方法不需要与文本相关的归纳偏置模块，利用简单的交叉熵损失对齐视频和文本。因此，该方法相比以往简单的多。</p><p>研究者提出的 pipeline 示意图如下所示。首先使用标准的 Transformer 文本编码器从文本查询中提取语言特征，使用时空编码器从视频帧中提取视觉特征。接着将这些特征传递给多模态 Transformer 以输出几个对象预测序列。然后为了确定哪个预测序列能够最好地对应参考对象，研究者计算了每个序列的文本参考分数。为此，他们还提出了一种时序分割 voting 方案，使模型在做出决策时专注于最相关的部分。</p><p></p><div class="img_box" id="id_imagebox_1" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_1" data-original="http://zkres1.myzaker.com/202203/62259f3c8e9f09359879ee5d_1024.jpg" data-height="939" data-width="957" src="https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202203/62259f3c8e9f09359879ee5d_1024.jpg" referrerpolicy="no-referrer"></div></div>从实验结果来看，MTTR 在 A2D-Sentences 和 JHMDB-Sentences 数据集上分别实现了 +5.7 和 +5.0 的 mAP 增益，同时每秒能够处理 76 帧。<p></p><p>研究者还展示了一系列不同对象之间的实际分割效果，如下穿白色 T 恤和蓝色短裤的冲浪者（淡黄色冲浪板）。</p><p></p><div class="img_box" id="id_imagebox_2" onclick><div class="content_img_div"><img class="lazy opacity_0 zaker_gif_cache" id="img_2" data-original="http://zkres2.myzaker.com/202203/62259f3c8e9f09359879ee5e_1024.jpg" data-gif-url="http://zkres2.myzaker.com/202203/62259f3c8e9f09359879ee5e_raw.gif" data-height="494" data-width="736" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202203/62259f3c8e9f09359879ee5e_1024.jpg" referrerpolicy="no-referrer"></div></div>又如嬉戏玩闹的大小猩猩。<p></p><p></p><div class="img_box" id="id_imagebox_3" onclick><div class="content_img_div"><img class="lazy opacity_0 zaker_gif_cache" id="img_3" data-original="http://zkres1.myzaker.com/202203/62259f3c8e9f09359879ee5f_1024.jpg" data-gif-url="http://zkres1.myzaker.com/202203/62259f3c8e9f09359879ee5f_raw.gif" data-height="497" data-width="741" src="https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202203/62259f3c8e9f09359879ee5f_1024.jpg" referrerpolicy="no-referrer"></div></div>网友对这项研究展示的视频对象分割效果赞不绝口。有人表示，即使在重叠的对象上，分割效果也很有效。<p></p><p></p><div class="img_box" id="id_imagebox_4" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_4" data-original="http://zkres2.myzaker.com/202203/62259f3c8e9f09359879ee60_1024.jpg" data-height="200" data-width="913" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202203/62259f3c8e9f09359879ee60_1024.jpg" referrerpolicy="no-referrer"></div></div><b></b><b><strong>方法介绍</strong></b><p></p><p>任务定义。RVOS 的输入为帧序列，其中；文本查询为，这里 t_i 是文本中的第 i 个单词；大小为的感兴趣帧的子集为，目标是在每一帧中分割对象。</p><p>特征提取。该研究首先使用深度时空编码器从序列 V 中的每一帧中提取特征。同时使用基于 Transformer 的文本编码器从文本查询 T 中提取语言特征。然后，将空间 - 时间和语言特征线性投影到共享维度 D。</p><p>实例预测。之后，感兴趣的帧特征被平化（flattened）并与文本嵌入分开连接，产生一组 T_I 多模态序列，这些序列被并行馈送到 Transformer。在 Transformer 的编码器层中，文本嵌入和每帧的视觉特征交换信息。然后，解码器层对每个输入帧提供 N_q 对象查询，查询与实体相关的多模态序列，并将其存储在对象查询中。该研究将这些查询（在图 1 和图 2 中由相同的唯一颜色和形状表示）称为属于同一实例序列的查询。这种设计允许自然跟踪视频中的每个对象实例。</p><p>输出生成。Transformer 输出的每个实例序列，将会生成一个对应的掩码序列。为了实现这一点，该研究使用了类似 FPN 的空间解码器和动态生成的条件卷积核。最后，该研究使用文本参考评分函数（text-reference score function），该函数基于掩码和文本关联，以确定哪个对象查询序列与 T 中描述的对象具有最强的关联，并将其分割序列作为模型的预测返回。</p><p>时间编码器。适合 RVOS 任务的时间编码器应该能够为视频中的每个实例提取视觉特征（例如，形状、大小、位置）和动作语义。相比之下，该研究使用端到端方法，不需要任何额外的掩码细化步骤，并使用单个主干就可完成。最近，研究者提出了 Video Swin Transformer [ 27 ] 作为 Swin Transformer 对视频领域的泛化。最初的 Swin 在设计时考虑了密集预测（例如分割）， Video Swin 在动作识别基准上进行了大量测试。</p><p>据了解，该研究是第一个使用 Video Swin （稍作修改）进行视频分割的。与 I3D 不同，Video Swin 仅包含一个时间下采样层，并且研究者可以轻松修改以输出每帧特征图。因此，Video Swin 是处理完整的连续视频帧序列以进行分割的更好选择。</p><p><b><strong>实例分割过程</strong></b></p><p>实例分割过程如图 2 所示。</p><div class="img_box" id="id_imagebox_5" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_5" data-original="http://zkres1.myzaker.com/202203/62259f3c8e9f09359879ee68_1024.jpg" data-height="833" data-width="1080" src="https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202203/62259f3c8e9f09359879ee68_1024.jpg" referrerpolicy="no-referrer"></div></div><p></p><p>首先，给定 F_E，即最后一个 Transformer 编码器层输出的更新后的多模态序列，该研究提取每个序列的视频相关部分（即第一个 H × W token）并重塑为集合。然后，该研究采用时间编码器的前 n − 1 个块的输出，并使用类似 FPN 的 [ 21 ] 空间解码器 G_Seg 将它们与分层融合。这个过程产生了视频帧的语义丰富、高分辨率的特征图，表示为 F_Seg。</p><div class="img_box" id="id_imagebox_6" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_6" data-original="http://zkres2.myzaker.com/202203/62259f3c8e9f09359879ee6c_1024.jpg" data-height="83" data-width="816" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202203/62259f3c8e9f09359879ee6c_1024.jpg" referrerpolicy="no-referrer"></div></div><p></p><p>接下来，对于 Transformer 解码器输出的每个实例序列，该研究使用两层感知器 G_kernel 生成相应的条件分割核序列。</p><div class="img_box" id="id_imagebox_7" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_7" data-original="http://zkres2.myzaker.com/202203/62259f3c8e9f09359879ee6e_1024.jpg" data-height="83" data-width="684" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202203/62259f3c8e9f09359879ee6e_1024.jpg" referrerpolicy="no-referrer"></div></div><p></p><p>最后，通过将每个分割核与其对应的帧特征进行卷积，为生成一系列分割掩码 M，然后进行双线性上采样操作以将掩码大小调整为真实分辨率</p><p></p><div class="img_box" id="id_imagebox_8" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_8" data-original="http://zkres2.myzaker.com/202203/62259f3c8e9f09359879ee70_1024.jpg" data-height="80" data-width="831" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202203/62259f3c8e9f09359879ee70_1024.jpg" referrerpolicy="no-referrer"></div></div><b></b><b><strong>实验</strong></b><p></p><p>该研究在 A2D-Sentences 数据集上将 MTTR 与 SOAT 方法进行比较。结果如表 1 所示，该方法在所有指标上都显着优于所有现有方法。</p><p>例如，该模型比当前 SOTA 模型提高了 4.3 mAP ，这证明了 MTTR 能够生成高质量的掩码。该研究还注意到，与当前 SOTA 技术相比，顶级配置（w = 10）的 MTTR 实现了 5.7 的 mAP 提高和 6.7% 的平均 IoU 和总体 IoU 的绝对改进。值得一提的是，这种配置能够在单个 RTX 3090 GPU 上每秒处理 76 帧的同时做到这一点。</p><p></p><div class="img_box" id="id_imagebox_9" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_9" data-original="http://zkres2.myzaker.com/202203/62259f3c8e9f09359879ee71_1024.jpg" data-height="440" data-width="1080" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202203/62259f3c8e9f09359879ee71_1024.jpg" referrerpolicy="no-referrer"></div></div>按照之前的方法 [ 11, 24 ] ，该研究通过在没有微调的 JHMDBSentences 上评估模型的泛化能力。该研究从每个视频中统一采样三帧，并在这些帧上评估模型。如表 2 所示，MTTR 方法具有很好的泛化性并且优于所有现有方法。<p></p><p></p><div class="img_box" id="id_imagebox_10" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_10" data-original="http://zkres1.myzaker.com/202203/62259f3c8e9f09359879ee72_1024.jpg" data-height="407" data-width="1080" src="https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202203/62259f3c8e9f09359879ee72_1024.jpg" referrerpolicy="no-referrer"></div></div>表 3 报告了在 Refer-YouTube-VOS 公共验证集上的结果。与现有方法 [ 24,37 ] 相比，这些方法是在完整数据集上进行训练和评估的，尽管该研究模型在较少的数据上进行训练，并专门在一个更具挑战性的子集上进行评估，但 MTTR 在所有指标上都表现出了卓越的性能。<p></p><p></p><div class="img_box" id="id_imagebox_11" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_11" data-original="http://zkres2.myzaker.com/202203/62259f3c8e9f09359879ee73_1024.jpg" data-height="553" data-width="1080" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202203/62259f3c8e9f09359879ee73_1024.jpg" referrerpolicy="no-referrer"></div></div>如图 3 所示，MTTR 可以成功地跟踪和分割文本参考对象，即使在具有挑战性的情况下，它们被类似实例包围、被遮挡或在视频的广泛部分中完全超出相机的视野。<p></p><p></p><div class="img_box" id="id_imagebox_12" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_12" data-original="http://zkres1.myzaker.com/202203/62259f3c8e9f09359879ee74_1024.jpg" data-height="632" data-width="1080" src="https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202203/62259f3c8e9f09359879ee74_1024.jpg" referrerpolicy="no-referrer"></div></div><p></p><div id="recommend_bottom"></div><div id="article_bottom"></div>  
</div>
            