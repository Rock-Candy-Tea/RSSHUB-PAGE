
---
title: '阿里安全发现_打码图片_可攻击AI视觉系统'
categories: 
 - 新媒体
 - ZAKER
 - channel
headimg: 'https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202108/612c40198e9f0952631ba717_1024.jpg'
author: ZAKER
comments: false
date: Sun, 29 Aug 2021 20:16:00 GMT
thumbnail: 'https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202108/612c40198e9f0952631ba717_1024.jpg'
---

<div>   
<p></p><div class="img_box" id="id_imagebox_0" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_0" data-original="http://zkres1.myzaker.com/202108/612c40198e9f0952631ba717_1024.jpg" data-height="292" data-width="554" src="https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202108/612c40198e9f0952631ba717_1024.jpg" referrerpolicy="no-referrer"></div></div>AI 科技评论报道<p></p><p>人有很强的抽象能力和联想力，例如一个由几块积木拼成的乐高玩具，小朋友也能轻易认出其中描述的场景。甚至几个像素，玩家也可以轻易认出这是哪个人物。</p><p>但 AI 可不一定会轻易识别出来。</p><p>不久前，某知名品牌汽车被曝其自动驾驶系统无法识别白色货车箱体这样类似于 " 一堵墙 " 的障碍物。在自动驾驶中，行人、车辆被漏检或者未能及时被检测到，都可能导致交通事故的产生。此外，安防漏检危险人物与物品也可能导致安全隐患。这些风险都提示，AI 视觉的安全性值得重视。</p><p>在研究 AI 视觉稳定性的过程中，<strong>阿里安全图灵实验室的研究人员札奇</strong>发现，AI 视觉还有一个盲区：利用算法自动鉴别图片关键信息，并巧妙删除，就像给图片 " 打码 " 一样，AI 视觉系统就会无法识别该图片。最近，这项研究成果被 AI 顶会 ICCV 2021 收录。</p><p></p><div class="img_box" id="id_imagebox_1" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_1" data-original="http://zkres2.myzaker.com/202108/612c40198e9f0952631ba718_1024.jpg" data-height="239" data-width="1080" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202108/612c40198e9f0952631ba718_1024.jpg" referrerpolicy="no-referrer"></div></div>论文地址：https://arxiv.org/pdf/2108.09034.pdf<p></p><p>札奇的研究源于逛商场看到乐高玩具迸发的灵感。当时，她有一个疑问：" 人眼如何识别‘马赛克’式样的玩具？还有早期的超级马里奥，虽然只是由几个简单像素组成，人却可以正确识别这种抽象的表达。AI 模型面对‘马赛克’式的图片，能正确识别吗？"</p><p>尽管我们期望 AI 模型能具有和人相当的能力，但是 " 抽象能力 " 对于现在的 AI 模型来说显然还是相当有挑战性的。但相反的，如果我们从对抗样本的角度来考虑：存不存在一种可能，如果我们去掉图片中一些对 AI 模型来说关键而微小的特征，AI 模型就无法再正确识别这些图片。</p><p><strong>那么什么是对抗样本呢？</strong></p><p><strong>1</strong></p><p><strong>对抗样本</strong></p><p>对抗样本一开始由 Szegedy 等人在 2013 年定义 : 给定一张原始图片 x 及其标签 y，以及模型。对抗样本是指在原图 x 上加一些刻意制造的微小的扰动，从而让结果图像无法被正确识别（如下图所示）。通常来说，对抗扰动被限制在一定阈值内，从而保证结果图对人来说与原图几乎不可区分。后续有很多相关工作在当前设定下进一步探索了更多生成对抗样本的攻击方式，以及其他性质，例如迁移性等。</p><p></p><div class="img_box" id="id_imagebox_2" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_2" data-original="http://zkres2.myzaker.com/202108/612c40198e9f0952631ba719_1024.jpg" data-height="290" data-width="720" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202108/612c40198e9f0952631ba719_1024.jpg" referrerpolicy="no-referrer"></div></div>图 1. 对抗攻击<p></p><p><strong>2</strong></p><p><strong>" 对抗样本可能是特征 "</strong></p><p>在对抗样本提出后，有各种各样的防御工作提出，其中对抗训练是最为有效的防御方式之一，但是对抗训练有非常明显的问题是：在稳健性（robustness）和准确率（accuracy）之间始终有一个平衡，即对抗训练提升模型稳健性的同时也导致的模型的准确率下降。为了解释这一现象，Ilyas 等人给对抗样本的存在提出了一个假设：对抗样本不是 bug，而是一组对人来说不可感知的特征。以人类感知为中心，人类所能察觉的特征就是 robust feature，其他的特征则是 non-robust。例如图 2 的狗狗，人类只会注意到其中的耳朵鼻子等显著特征 ( robust feature ) 。</p><p></p><div class="img_box" id="id_imagebox_3" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_3" data-original="http://zkres1.myzaker.com/202108/612c40198e9f0952631ba71a_1024.jpg" data-height="318" data-width="897" src="https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202108/612c40198e9f0952631ba71a_1024.jpg" referrerpolicy="no-referrer"></div></div>图 2. 鲁棒特征与非鲁棒特征<p></p><p>Ilyas 等人通过一组巧妙的实验说明对抗样本其实是模型从数据中学习到一部分特征，尽管对人来说不可感知，但是对于模型来说是具有预测意义的。受 Ilyas 等人工作启发，札奇研究团队试图从一个相反的角度来讨论一个潜在的攻击机制：<strong>可否去掉一些对人来说微小而不可感知但是对于模型决策又重要的特征，从而形成对抗样本呢？</strong></p><p><strong>3</strong></p><p><strong>AdvDrop, 通过丢信息来制造对抗样本</strong></p><p>他们对此猜想进行了验证，实验过程如下：</p><p></p><div class="img_box" id="id_imagebox_4" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_4" data-original="http://zkres1.myzaker.com/202108/612c40198e9f0952631ba71b_1024.jpg" data-height="151" data-width="1080" src="https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202108/612c40198e9f0952631ba71b_1024.jpg" referrerpolicy="no-referrer"></div></div>图 3. 左侧 AdvDrop，信息丢失越来越多，右侧 PGD, 对抗噪声越来越大<p></p><p>他们在这个工作中提出一个新的机制来生成对抗样本：相反于增加对抗扰动，我们通过扔掉一些不可察觉的图像细节来生成对抗样本。关于两种相反机制的说明如图 3，当 AdvDrop 放宽丢掉的信息量的阈值 epsilon，产生的对抗样本越来越趋近于一张灰色图片，伴随着图像存储量的降低。而相反的，PGD 生成的对抗样本，随着干扰幅度的增大，越来越接近于无序噪音。</p><p>一张更细节的对比图 4 所示， 从局部区域来看，PGD 在图片的局部生成了更多的细节，表现为更丰富的色彩。而相反的，AdvDrop 生成的对抗样本与原图相比失去了一些局部细节，表现在色彩精度的降低。</p><p></p><div class="img_box" id="id_imagebox_5" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_5" data-original="http://zkres2.myzaker.com/202108/612c40198e9f0952631ba71c_1024.jpg" data-height="540" data-width="838" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202108/612c40198e9f0952631ba71c_1024.jpg" referrerpolicy="no-referrer"></div></div>图 4. PGD 与 AdvDrop 局部色彩丰富度<p></p><p><strong>4</strong></p><p><strong>他们是如何确定丢掉哪些区域的呢？</strong></p><p>为了确定丢掉哪些区域的图片信息，并且保证扔掉的细节人们无法感知，他们提出一种通过优化量化表的方式来选择丢掉信息的区域以及丢掉的信息量的方法。此外，为了保证丢掉的细节对于人来说依然不可感知，要先将图像通过离散傅里叶变换从 RGB 转换到频域，再用量化表去量化一些频域的信息。频域操作相比于 RGB 的优点是，能更好的分离图像的细节信息（高频信息）和结构信息（低频信息），因此可以保证扔掉的细节对人来说不可感知。</p><p></p><div class="img_box" id="id_imagebox_6" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_6" data-original="http://zkres1.myzaker.com/202108/612c40198e9f0952631ba71d_1024.jpg" data-height="206" data-width="1080" src="https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202108/612c40198e9f0952631ba71d_1024.jpg" referrerpolicy="no-referrer"></div></div>图 5. AdvDrop 算法流程<p></p><p>整个流程如图 5 所示，从优化上，可以被定义为：</p><p></p><div class="img_box" id="id_imagebox_7" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_7" data-original="http://zkres2.myzaker.com/202108/612c40198e9f0952631ba71e_1024.jpg" data-height="212" data-width="1080" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202108/612c40198e9f0952631ba71e_1024.jpg" referrerpolicy="no-referrer"></div></div>其中 D 和分别表示的是离散余弦变环及反变换，表示的是一个可微分的量化过程。<p></p><p>通常的量化，可以定义为：</p><p></p><div class="img_box" id="id_imagebox_8" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_8" data-original="http://zkres1.myzaker.com/202108/612c40198e9f0952631ba71f_1024.jpg" data-height="104" data-width="1080" src="https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202108/612c40198e9f0952631ba71f_1024.jpg" referrerpolicy="no-referrer"></div></div>但是因为量化函数不可微分，极大影响优化过程。因此，札奇研究团队参考了 Gong 等人的工作，通过引入可控 tanh 函数来渐进的逼近阶梯式的量化函数，所以：<p></p><p></p><div class="img_box" id="id_imagebox_9" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_9" data-original="http://zkres2.myzaker.com/202108/612c40198e9f0952631ba720_1024.jpg" data-height="156" data-width="678" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202108/612c40198e9f0952631ba720_1024.jpg" referrerpolicy="no-referrer"></div></div>其斜度可以由 α 调整，如下图所示，经过量化函数可微处理，可以更准确的反向传播梯度从而更准确的估计出应该丢失信息的位置及量化的大小。<p></p><p></p><div class="img_box" id="id_imagebox_10" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_10" data-original="http://zkres2.myzaker.com/202108/612c40198e9f0952631ba722_1024.jpg" data-height="224" data-width="1080" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202108/612c40198e9f0952631ba722_1024.jpg" referrerpolicy="no-referrer"></div></div>图 6. 不同 alpha 下 tanh 函数对量化函数的逼近程度<p></p><p><strong>5</strong></p><p><strong>结果评估</strong></p><p>用 lpips 比较 AdvDrop 及 PGD 在相同信息量变化下的视觉得分：从对抗样本的不可感知角度来说，在同样的感知得分下，丢信息操作允许操作的信息量要比加干扰允许的更大。从人类视觉上来说，相比于加噪，人眼对于局部平滑其实更为不敏感，从图 7 可见，随着量化表阈值的增大，AdvDrop 生成的对抗样本的局部细节越少，例如蜥蜴鳞片的纹理：</p><p><strong></strong></p><div class="img_box" id="id_imagebox_11" onclick><div class="content_img_div perview_img_div"><strong><img class="lazy opacity_0 " id="img_11" data-original="http://zkres1.myzaker.com/202108/612c40198e9f0952631ba723_1024.jpg" data-height="790" data-width="1080" src="https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202108/612c40198e9f0952631ba723_1024.jpg" referrerpolicy="no-referrer"></strong></div></div>图 7. 不同预知下的攻击结果展示<p></p><p>从成功率上来说，无论是在目标攻击还是无目标攻击的设定下， AdvDrop 有相当高的成功率来生成一个对抗样本。在目标攻击下，最高可以达到一个 99.95% 成功率。但相比于传统加噪的对抗攻击生成方式 ( 例如 PGD，BIM ) 可以轻易达到 100% 的成功率来说，依然是强度较弱的。</p><p>" 我们觉得 AdvDrop 强度方面的局限可能来自于两方面：一方面是由于量化这样的方式，另一方面，" 减信息 " 可以操作的空间相比于 " 加信息 " 的空间来说要小很多。"</p><p>他们也评估了 AdvDrop 在不同防御下的表现。目前主流防御方式主要分为两种，一种是<strong>对抗训练</strong> ，另一种是<strong>基于去噪的防御方式</strong>。研究发现 AdvDrop 生成的对抗样本对于现阶段防御方式来说仍是一个挑战，尤其是基于去噪的防御方式。</p><p></p><div class="img_box" id="id_imagebox_12" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_12" data-original="http://zkres1.myzaker.com/202108/612c40198e9f0952631ba724_1024.jpg" data-height="271" data-width="902" src="https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202108/612c40198e9f0952631ba724_1024.jpg" referrerpolicy="no-referrer"></div></div><div class="img_box" id="id_imagebox_13" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_13" data-original="http://zkres1.myzaker.com/202108/612c40198e9f0952631ba725_1024.jpg" data-height="614" data-width="1080" src="https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202108/612c40198e9f0952631ba725_1024.jpg" referrerpolicy="no-referrer"></div></div>具体来说，在一定扰动阈值下，基于制造对抗扰动的对抗样本生成方式经过去噪后，图片有很大概率恢复成原始图片。但是对于用 AdvDrop 生成的 对抗样本来说，其本身就是由于部分特征丢失而导致的错误识别，而去噪操作甚至会加剧这种由于丢失而无法识别的问题。<p></p><p></p><div class="img_box" id="id_imagebox_14" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_14" data-original="http://zkres1.myzaker.com/202108/612c40198e9f0952631ba726_1024.jpg" data-height="547" data-width="1080" src="https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202108/612c40198e9f0952631ba726_1024.jpg" referrerpolicy="no-referrer"></div></div>图 8. AdvDrop 和 PGD 在 Denoise 操作下的细节展示<p></p><p>除了防御的角度，考虑到很多数据都是从网上收集而来，而网络传输中往往存在数据压缩过程，所以通过 AdvDrop 生成的对抗样本可能 " 更耐传输 "。当然，另一个角度来想，也有可能对于正常图像数据来说，一些正常的数据压缩（例如 jpeg）也许不经意间就引入了对抗样本。</p><p><strong>6</strong></p><p><strong>总结</strong></p><p>因此，传统对图片 " 加工 " 以骗过 AI 的方法是给图片加上 " 噪音 "，相当于在当前图片上针对模型 " 乱涂乱画 "，让 AI 无法识别，但原图片本身的关键信息没有丢失，只要用 " 橡皮擦 " 擦一擦，AI 依然能识别。如果反向操作，删除图片的关键信息，就像打 " 马赛克 "，图片的关键信息已经丢失，那么 AI 无论如何也难以识别。这意味着，针对<strong>" 打码攻击 "</strong>，难以有防御措施。</p><p>该工作也展示了 AI 模型另一个角度的局限性：对重要细节丢失的稳健性。</p><p>在这个工作中，仅仅探索了在频域上丢信息的操作，通过其他丢信息方式来生成对抗样本都是可以值得尝试的未来工作。</p><p>专注对 AI 的对抗样本和模型安全性进行研究的阿里安全高级算法专家越丰提醒，除了 AI 视觉场景，真实场景中也可能存在这种对抗攻击，例如针对某知名 PS 软件，只要提供具备对抗攻击性质的 JPEG 量化表，就能产出有 " 攻击性 " 的图片。</p><p>此外，在实际场景中，图片信息丢失是常见现象，例如用户将图片以 JPEG 形式上传到网络，就有一定的信息丢失，可能不经意间就会制造一个 " 对抗样本 "。越丰认为，这对当前内容安全场景的 AI 识别而言，都是不小的挑战。</p><p>" 比如有人将涉黄赌毒图片以损失部分信息的形式上传到网络，人眼依然能领会含义，但 AI 却没能正确识别，这对构建清朗、健康网络环境而言，就是一种对抗。" 越丰举例道，AI 安全行业应该警惕这种类型的对抗。</p><p>当然，" 致盲 AI" 不是研究人员的目标，研究人员最终还是想发现 AI 模型的脆弱性，进一步提升 AI 安全。" 在 AI 安全前沿技术上进行探索，一是为了让 AI 更安全，二是为了让 AI 助力安全，三是为解决具体社会问题寻找提效的新途径。" 阿里安全图灵实验室负责人薛晖提醒，相比 " 事后弥补 "，安全应前置，从源头守卫安全，对前沿技术进行研究布局，以科技创新造就最好的网络安全。</p><div id="recommend_bottom"></div><div id="article_bottom"></div>  
</div>
            