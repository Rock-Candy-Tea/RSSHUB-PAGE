
---
title: '以性虐待之名扫描手机，苹果提醒员工为 iPhone 后门问题做好准备'
categories: 
 - 新媒体
 - ZAKER
 - channel
headimg: 'https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202108/6116a3908e9f0961bb77a920_1024.jpg'
author: ZAKER
comments: false
date: Sat, 14 Aug 2021 17:29:00 GMT
thumbnail: 'https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202108/6116a3908e9f0961bb77a920_1024.jpg'
---

<div>   
<p>IT 之家 8 月 15 日消息 众所周知，苹果此前推出了一项新政策，可在不经过用户许可的情况下扫描存储在 iCloud 照片库中的 CSAM（儿童性虐待照片）内容，一时间在海外激起千层浪，舆论浩荡纷纷反攻苹果。</p><p>但目前看来苹果依然是执意推行新政策，例如苹果本周已下发备忘录，以向其零售和在线销售人员提醒，要求他们准备好如何回答消费者就有关 iPhone 的 CSAM 后门的问题统一口径。</p><p>苹果 :" 让我们明确一点，这项技术检测范围仅限于存储在 iCloud 中的 CSAM 内容，我们不会同意任何政府扩大它的范围。"</p><p>但现在部分用户和外媒又抛出了一个问题 :</p><p>长期以来，苹果确实一直在遵守当地的法律，如果某政府通过立法来要求苹果帮助搜寻用户手机中的其他类型图像 ( 犯罪、恐怖主义、政治、信仰、性取向等 ) ，苹果会怎么做 ? 就算按他们所说，苹果不会在没有相关法律的情况下接受任何政府的要求，但是，基于历史来看，难道苹果也会拒绝法律的要求吗？通往地狱的道路往往是由善意铺就的。</p><p>据悉，苹果还表示，将通过让独立审计师审查系统来解决隐私问题。</p><p>值得一提的是，苹果此前曾表示，将拒绝政府利用其技术监视用户的任何要求。</p><p></p><div class="img_box" id="id_imagebox_0" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_0" data-original="http://zkres2.myzaker.com/202108/6116a3908e9f0961bb77a920_1024.jpg" data-height="400" data-width="776" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202108/6116a3908e9f0961bb77a920_1024.jpg" referrerpolicy="no-referrer"></div></div>IT 之家了解到，CSAM 功能受到了苹果广大用户、安全研究人员、电子前沿基金会 ( EFF ) 和 Facebook 前安全主管爱德华斯诺登，甚至苹果自家员工的大量批评。<p></p><p>面对这些批评，苹果软件工程高级副总裁 Federighi 解答了大家主要关心的问题，强调苹果的系统将受到保护，不会被政府或其他具有 " 多层次可审计性 " 的第三方利用。</p><p>Federighi 还透露了一些关于系统保护措施的新细节，例如在苹果收到警报之前，手机会在照片库中匹配大约 30 个 CSAM 内容，然后它会确认这些图像是否是真实的 CSAM 照片。</p><p>只有当该照片与 30 个已知的 CSAM 照片匹配的阈值时，苹果才会知道用户的帐户和这些照片的信息，而不涉及用户的任何其他照片。此外，Federighi 表示检测过程是全程在手机本地的。</p><div id="recommend_bottom"></div><div id="article_bottom"></div>  
</div>
            