
---
title: '为什么搞AI的更容易彼此diss？因为连研究成果都难以复现啊'
categories: 
 - 新媒体
 - ZAKER
 - channel
headimg: 'https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202104/60862fffb15ec03ef02defc7_1024.jpg'
author: ZAKER
comments: false
date: Sun, 25 Apr 2021 19:13:01 GMT
thumbnail: 'https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202104/60862fffb15ec03ef02defc7_1024.jpg'
---

<div>   
<p></p><div class="img_box" id="id_imagebox_0" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_0" data-original="http://zkres2.myzaker.com/202104/60862fffb15ec03ef02defc7_1024.jpg" data-height="720" data-width="1080" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202104/60862fffb15ec03ef02defc7_1024.jpg" referrerpolicy="no-referrer"></div></div>全世界最聪明的人们正在前赴后继地投入对 AI 的研究，但伴随着一个又一个惊人的成果发布，最基本的事情却出问题了——不同研究者之间没法 " 对话 " 了。<p></p><p><b>论文复现难</b></p><p>在成都电子科技大学做活体检测方向研究的韦仕才依然记得自己的一次 AI 项目的糟糕经历。当时他在一个人脸识别的项目里负责活体检测方向的研究。</p><p>人脸识别的目的是 " 认人 "，活体检测则是为了 " 识真 "，即识别在摄像头前接受测试的脸是不是真正的人脸，以规避照片、视频等欺骗手法。</p><p>像大部分科学研究一样，他首先找了几篇顶级会议收录的最新论文来学习。在学术界，顶级会议和期刊收录的往往是最新的方法和 达到 SOTA（state of the art，最领先的）效果的研究。</p><p>比如，在活体检测领域，最流行的技术趋势是 " 静默活体检测 "，即不需要人做动作。相对应的，是 " 动作活体检测 "，需要人配合做动作，比如眨眼、张嘴或转头，用户体验相较前者差一点。随着研究深入，韦仕才选择了一篇实现了 SOTA 效果、并且公开代码和预训练模型的一篇 " 静默活体检测 " 论文，希望把它的理论用到实践中。</p><p>活体检测讲究跨数据集测试的准确性，他申请了论文提到的那个数据集，进行跨数据集测试。最初，他发现算法效果和论文描述的差不多，看起来自己找到了一个好模型。</p><p>但后面发生的事情让他感到非常困惑。</p><p>" 我做的是实际落地项目，所以把论文的算法部署到了服务器上，想尝试用一下。结果发现实际效果很差，算法很难识别出真人人脸。" 他对品玩说。</p><p>他不得不一层一层地往下挖，找出问题所在：</p><p>" 挖到数据那一层，我发现，这篇论文的测试数据分布不太正常。正常应该是每个类别均等，即真人数据占一半，欺骗数据占一半，但论文的数据分布是欺骗照片更多一点。" 准确来说，欺骗照片是真人数据的两倍多。</p><p>这导致的结果是，这个方法即使更倾向于将输入图片识别为欺骗人脸，试验测试的结果很好。但是在实际部署的时候，算法准确识别真人人脸的概率偏低，甚至出现完全不能识别真人人脸的问题，导致模型失效。</p><p>这是论文的复现出了问题。而这个问题在 AI 研究领域，已经不是一个个别的问题。有评论把这种复现困境，称之为 " 徘徊在 AI 上空的幽灵 "。</p><p><b>看不到的代码</b></p><p>随着 AI 在学术界和业界的复兴和走红，相关论文越来越多。2019 年 Google 大脑负责人 Jeff Dean 曾做过统计，全世界每一天平均会产生 100 篇 AI 论文。</p><p>数量增多却没有带来质量的飞跃，AI 论文复现难的问题，让不少研究者感到困扰。" 在 AI 研究中，论文复现很难避免。除非你的想法跟之前论文完全没有关系，而且效果很好，但这种情况非常罕见。" 韦仕才告诉品玩。</p><p>所谓论文复现，是指研究人员重现某篇论文的结果。如果想把一篇论文的算法落地， 肯定要去复现；如果一个新点子是在某篇论文基础上做改进，也需要先复现论文；如果想把一个新算法与某论文的算法做对比，也要把论文复现之。</p><p>按理说，能复现的论文，才经得起考验，才是靠谱的论文。但由于种种原因，论文复现成了一个老大难问题。即便是大公司如 Google，其语音模型 Transformer-XL 的结果，也在 2020 年被很多研究人员质疑无法复现。</p><p>复现难得根本原因出现在代码和相关训练方法是否公开上。</p><p></p><div class="img_box" id="id_imagebox_1" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_1" data-original="http://zkres1.myzaker.com/202104/60862fffb15ec03ef02defc8_1024.jpg" data-height="277" data-width="406" src="https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202104/60862fffb15ec03ef02defc8_1024.jpg" referrerpolicy="no-referrer"></div></div>" 论文复现难，很大程度上是因为研究人员不公开论文的代码。" 北京一高校的 AI 研究者文聪告诉品玩。<p></p><p>2018 年 AAAI 会议上，挪威科技大学计算机科学家 Odd Erik Gundersen 发布了一项调查结果。他研究了过去几年里，两个 AI 顶会上发表的 400 篇论文，发现只有 6% 的研究者公开了算法的代码，约三分之一的研究中公开了测试数据集。</p><p>不公开代码的原因有很多。比如，代码所有权归属公司，代码可能依赖于某些未发布的代码，又或者研究者纯粹不想透露。此外，如果 AI 研究基于一些敏感行业，比如医疗和法律，出于保密考虑，公开代码也是不现实的。</p><p>在复现失败后，韦仕才又去找了几篇论文的算法做测试，发现效果都不怎么好。" 我当时困惑了一段时间，觉得这些最新的方法怎么都没有用。" 他告诉品玩。</p><p>" 虽然越来越多研究者公开代码和预训练模型，但一般不会公开训练的细节，而这里面有很多可以操作的空间，可以用一些 trick，比如学习率衰减和数据增强。这些 trick 被归类为调参的一种，是被允许的。" 他对品玩表示，" 但部分文章可能直接作弊，把部分测试数据当做训练数据使用。因此即使公开了源码和预训练模型，研究者自己可能也很难训练出和预训练模型一样效果的模型。"</p><p>后来，他直接换了个思路，不再追求最新的方法和 SOTA，而是去尝试一些原理看起来比较可行的方法。" 就算是最后真的不行，我也能知道是哪里不行，也是我思维的完善。"</p><p>于是，研究的目标逐渐从 2018、 2019 年的最新论文，转向了 2012、2013 年的文章，发现复现效果好了很多。虽然效果不是 SOTA，但有理有据，而且 " 很多那时候的传统模型，结合现在深度学习往往能爆发出强大的力量 "。</p><p>除了使用一些 trick，还有很多因素会导致论文难以复现。比如，论文使用了不开源的私有数据集，而一般研究者没法访问这些数据。又或者，大公司筑起了算力 " 围墙 "。一个最近的例子是 GPT-3，这个大规模模型花费了千万级别的算力费用，普通研究者对此遥不可及。还有一些比较恶劣的情况，就是论文故意造假，根本不可能复现出来。</p><p>代码、训练细节、私有数据、算力门槛、故意造假 …… 总而言之，AI 论文复现难这个现象，是综合因素作用下的结果。</p><p>" 论文复现难基本是不可避免的问题。" 文聪告诉品玩，" 做 AI 研究要有心理准备，能复现出来的论文是少数，复现不出来才是正常的。"</p><p>这种无法复现，正在给 AI 研究带来很大的困扰。它使得 AI 研究者们彼此之间无法 " 对话 "。</p><p>一名在某互联网大厂负责机器视觉研究的 AI 科学家对品玩表示，研究者非常需要一个公开透明的环境，让他们不用相互鄙视。" 其实你会发现整个视觉领域有一个很大的特点，就是大家都说自己牛逼，但实际上很有可能拿着香蕉跟苹果比，也说不清楚，其实整个业界都是说不清楚的。"</p><p>他表示，理想状态应该是，" 你敢 claim，就要敢放出来，让大家看看能不能可复现。"</p><p><b>事情正在发生改变</b></p><p>好在，随着论文复现难的问题越来越普遍，AI 学术界正在改变这一现状，尝试推动研究人员在有条件的情况下，尽可能共享源代码。</p><p>2018 年 7 月，Reddit 用户 rstoj 做了一个网站 "Papers with Code"，将最新 AI 论文和 Github 上的代码关联起来，研究者能很方便地发现，哪些论文公布了源代码。2020 年 10 月，论文预印本平台 arXiv 与 Papers with Code 达成合作。研究者在 arXiv 上传论文时，可以同步上传代码。</p><p></p><div class="img_box" id="id_imagebox_2" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_2" data-original="http://zkres2.myzaker.com/202104/60862fffb15ec03ef02defc9_1024.jpg" data-height="250" data-width="750" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202104/60862fffb15ec03ef02defc9_1024.jpg" referrerpolicy="no-referrer"></div></div>一些顶级学术会议如 ICML、ICLR 和 NeurIPS，开始建议研究人员将代码和数据作为评审材料的一部分，与论文一起提交。这其中，NeurIPS 从 2019 年起开始 " 鼓励 " 论文作者提交代码。后来，该顶会又 " 强烈建议 " 提交代码，还提供了准则和模版。不过，NeurIPS 始终没有强制论文作者公开代码。<p></p><p>除此之外，最近还出现了一个名为 "Papers without Code" 的新平台，某种程度上也是这种情绪的集中反映。顾名思义，"Papers without Code" 主要呈现无法复现的 AI 论文，与上文提到的 "Paper with Code" 相对应。</p><p>在这个平台上，研究者可以提交自己复现某论文的细节。平台会与论文作者取得联系，要求其进行相关解释。如果作者没有及时答复，论文会被添加到 " 不可复现的机器学习论文列表 " 中。</p><p>这相当于一种 " 公开处刑 "。</p><p>"Papers without Code" 平台的创建者直言：" 如果其他人不能以该论文为基础或基准，则公开发表的有实证结果的论文毫无意义。"</p><p>这些努力都在逐渐起效。韦仕才明显感觉到，最近几年论文复现难的现象，正朝着好的方向发展，新论文作者开源代码的比例在慢慢增高。他告诉品玩：" 虽然说代码质量也不一定很高，但相较之前没有开源代码的情况，复现一篇论文的投入成本已经没有此前那么高了 "。</p><p>他现在去复现一篇论文前，一定会先看其是否开源代码，如果没有的话，会去看一下文章的模型是否复杂，试验细节是否充分（如数据划分，数据增强方式，重要训练参数）。如果有一个方法效果很好，但没有公开代码而且模型复杂或文章不清晰，也不会考虑去复现。</p><p>韦仕才对品玩表示，" 如果论文的新方法真的如它描述得这么好，大可开放代码和预训练模型，让大家都去用它的方法。或许，此后很多新方法，都是不可避免地会借鉴该论文，论文的影响力和引用次数会上升——这其实是研究者都追求的东西。"</p><p>同时，对研究者个体来说，学会与不完美相处，合理调整预期才是聪明的做法。</p><p>南开大学计算机科学与技术系系主任兼教授程明明在知乎中写道：" 我自己的做法就是，有那么多很好的开源项目都看不过来，我轻易不去 follow 那些很难复现的项目。另外，不同团队的风格不一样，有些团队的工作就非常容易复现。建议多用正能量去看问题。同时学会在不完美的世界中生存的很好的技巧，避开坑，多关注可复现性好的工作。"</p><p>韦仕才告诉品玩，" 我现在复现论文，只是想看一下论文提出的方法，是否真的有效果，已经不指望能把结果百分百复现出来。复现结果比论文结果低一个点，我都可以接受的。" 他还表示：" 重要的是看文章的核心思想，你觉得这个思想是不是合理，你接不接受。如果你自己也觉得他这个思想很不错，那么就可以尝试去复现，就算是最后失败了，你也不会很难受，你知道什地方有问题，而这会是你后面科研的创新来源之一。"</p><div id="recommend_bottom"></div><div id="article_bottom"></div>  
</div>
            