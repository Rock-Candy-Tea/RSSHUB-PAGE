
---
title: 'PyTorch宣布支持苹果M1芯片GPU加速：训练快6倍，推理提升21倍'
categories: 
 - 新媒体
 - ZAKER
 - channel
headimg: 'https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202205/6285f37d8e9f091f45030503_1024.jpg'
author: ZAKER
comments: false
date: Thu, 19 May 2022 00:16:00 GMT
thumbnail: 'https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202205/6285f37d8e9f091f45030503_1024.jpg'
---

<div>   
<p>机器之心报道</p><p><strong>编辑：泽南、蛋酱</strong></p><p>对于 Mac 用户来说，这是令人激动的一天。</p><p>今年 3 月，苹果发布了其自研 M1 芯片的最终型号 M1 Ultra，它由 1140 亿个晶体管组成，是有史以来个人计算机中最大的数字。苹果宣称只需 1/3 的功耗，M1 Ultra 就可以实现比桌面级 GPU RTX 3090 更高的性能。</p><p></p><div class="img_box" id="id_imagebox_0" onclick><div class="content_img_div"><img class="lazy opacity_0 zaker_gif_cache" id="img_0" data-original="http://zkres1.myzaker.com/202205/6285f37d8e9f091f45030503_1024.jpg" data-gif-url="http://zkres1.myzaker.com/202205/6285f37d8e9f091f45030503_raw.gif" data-height="396" data-width="718" src="https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202205/6285f37d8e9f091f45030503_1024.jpg" referrerpolicy="no-referrer"></div></div>随着用户数量的增长，人们已经逐渐接受使用 M1 芯片的计算机，但作为一款 Arm 架构芯片，还有人在担心部分任务的兼容性问题。<p></p><p>昨天，通过与苹果 Metal 团队工程师合作，PyTorch 官方宣布已正式支持在 M1 版本的 Mac 上进行 GPU 加速的 PyTorch 机器学习模型训练。</p><p>此前，Mac 上的 PyTorch 训练仅能利用 CPU，但随着即将发布的 PyTorch v1.12 版本，开发和研究人员可以利用苹果 GPU 大幅度加快模型训练。现在，人们可以在 Mac 上相对高效地执行机器学习工作，例如在本地进行原型设计和微调。</p><p></p><div class="img_box" id="id_imagebox_1" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_1" data-original="http://zkres2.myzaker.com/202205/6285f37d8e9f091f45030504_1024.jpg" data-height="321" data-width="1080" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202205/6285f37d8e9f091f45030504_1024.jpg" referrerpolicy="no-referrer"></div></div><strong>苹果芯片的 AI 训练优势</strong><p></p><p>PyTorch GPU 训练加速是使用苹果 Metal Performance Shaders ( MPS ) 作为后端来实现的。MPS 后端扩展了 PyTorch 框架，提供了在 Mac 上设置和运行操作的脚本和功能。MPS 使用针对每个 Metal GPU 系列的独特特性进行微调的内核能力来优化计算性能。新设备将机器学习计算图和原语映射到 MPS Graph 框架和 MPS 提供的调整内核上。</p><p>每台搭载苹果自研芯片的 Mac 都有着统一的内存架构，让 GPU 可以直接访问完整的内存存储。PyTorch 官方表示，这使得 Mac 成为机器学习的绝佳平台，让用户能够在本地训练更大的网络或批大小。</p><p>这降低了与基于云算力的开发相关的成本或对额外的本地 GPU 算力需求。统一内存架构还减少了数据检索延迟，提高了端到端性能。</p><p>可以看到，与 CPU 基线相比，GPU 加速实现了成倍的训练性能提升：</p><p></p><div class="img_box" id="id_imagebox_2" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_2" data-original="http://zkres2.myzaker.com/202205/6285f37d8e9f091f45030505_1024.jpg" data-height="663" data-width="1080" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202205/6285f37d8e9f091f45030505_1024.jpg" referrerpolicy="no-referrer"></div></div>上图是苹果于 2022 年 4 月使用配备 Apple M1 Ultra（20 核 CPU、64 核 GPU）128GB 内存，2TB SSD 的 Mac Studio 系统进行测试的结果。系统为 macOS Monterey 12.3、预发布版 PyTorch 1.12，测试模型为 ResNet50（batch size = 128）、HuggingFace BERT（batch size = 64）和 VGG16（batch size = 64）。性能测试是使用特定的计算机系统进行的，反映了 Mac Studio 的大致性能。<p></p><p>有开发者推测，鉴于谷歌云服务中使用的英伟达 T4 在 FP32 任务上的浮点性能为 8 TFLOPS，而 M1 Ultra 的图形计算能力大概在 20 TFLOPS 左右。在最有利情况下，可以期望的 M1 Ultra 速度提升或可达到 2.5 倍。</p><p></p><div class="img_box" id="id_imagebox_3" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_3" data-original="http://zkres2.myzaker.com/202205/6285f37d8e9f091f45030506_1024.jpg" data-height="142" data-width="1080" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202205/6285f37d8e9f091f45030506_1024.jpg" referrerpolicy="no-referrer"></div></div>若想使用最新的加速能力，你需要在使用 M1 系列芯片的 Mac 电脑上安装原生版本（arm64）的 Python，并将系统升级至 macOS 12.3 预览版或更新的版本。<p></p><p><strong>开发者亲测：加速效果显著</strong></p><p>虽然官方已宣布提供支持，但目前还不是所有在 PyTorch 上的模型都能用 M1 芯片集成的 GPU 加速，你也可以花几分钟进行一下测试。</p><p></p><div class="img_box" id="id_imagebox_4" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_4" data-original="http://zkres1.myzaker.com/202205/6285f37d8e9f091f45030507_1024.jpg" data-height="884" data-width="1080" src="https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202205/6285f37d8e9f091f45030507_1024.jpg" referrerpolicy="no-referrer"></div></div>机器学习研究者，捷克理工大学博士 Dmytro Mishkin 对多个模型的推理进行了测试，结果显示，大多数图像分类架构都提供了很好的加速。对于一些自定义代码（比如 kornia），可能无法正常工作。<p></p><p>各个测试结果如下：</p><p>首先是经典的卷积神经网络 VGG16，从 2.23 秒提升到 0.5 秒：</p><p></p><div class="img_box" id="id_imagebox_5" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_5" data-original="http://zkres2.myzaker.com/202205/6285f37d8e9f091f45030508_1024.jpg" data-height="354" data-width="1080" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202205/6285f37d8e9f091f45030508_1024.jpg" referrerpolicy="no-referrer"></div></div>接下来是大部分芯片发布会上都会跑的 Resnet50，它在 M1 GPU 上的速度较慢，不升反降，从 0.549 秒到 0.592 秒：<p></p><p></p><div class="img_box" id="id_imagebox_6" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_6" data-original="http://zkres1.myzaker.com/202205/6285f37d8e9f091f45030509_1024.jpg" data-height="349" data-width="1080" src="https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202205/6285f37d8e9f091f45030509_1024.jpg" referrerpolicy="no-referrer"></div></div>但 ResNet18 的提速惊人，从 0.243 秒到 0.024 秒：<p></p><p>AlexNet 的速度对比为 0.126 秒 vs0.005 秒，速度提升了几十倍：</p><p></p><div class="img_box" id="id_imagebox_7" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_7" data-original="http://zkres1.myzaker.com/202205/6285f37d8e9f091f4503050b_1024.jpg" data-height="343" data-width="1080" src="https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202205/6285f37d8e9f091f4503050b_1024.jpg" referrerpolicy="no-referrer"></div></div>尝试一下视觉 transformer 模型，在 M1 CPU 上的速度是 1.855 秒，在 M1 GPU 上则运行崩溃了 ……<p></p><p></p><div class="img_box" id="id_imagebox_8" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_8" data-original="http://zkres1.myzaker.com/202205/6285f37d8e9f091f4503050c_1024.jpg" data-height="360" data-width="1080" src="https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202205/6285f37d8e9f091f4503050c_1024.jpg" referrerpolicy="no-referrer"></div></div>EfficientNetB0 实现了 2.5 倍的加速：<p></p><p></p><div class="img_box" id="id_imagebox_9" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_9" data-original="http://zkres1.myzaker.com/202205/6285f37d8e9f091f4503050d_1024.jpg" data-height="377" data-width="1080" src="https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202205/6285f37d8e9f091f4503050d_1024.jpg" referrerpolicy="no-referrer"></div></div>EfficientNetB4 实现了 3.5 倍加速：<p></p><p></p><div class="img_box" id="id_imagebox_10" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_10" data-original="http://zkres2.myzaker.com/202205/6285f37d8e9f091f4503050e_1024.jpg" data-height="333" data-width="1080" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202205/6285f37d8e9f091f4503050e_1024.jpg" referrerpolicy="no-referrer"></div></div>ConvMixer 运行良好，从 11 秒提速到 2.8 秒：<p></p><p></p><div class="img_box" id="id_imagebox_11" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_11" data-original="http://zkres2.myzaker.com/202205/6285f37d8e9f091f4503050f_1024.jpg" data-height="473" data-width="1080" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202205/6285f37d8e9f091f4503050f_1024.jpg" referrerpolicy="no-referrer"></div></div>Dmytro Mishkin 也表示，使用 M1 芯片集成的 GPU 加速只需要预热一下模型，没有同步命令。和 CUDA 不同，无需异步执行。<p></p><p></p><div class="img_box" id="id_imagebox_12" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_12" data-original="http://zkres2.myzaker.com/202205/6285f37d8e9f091f45030510_1024.jpg" data-height="417" data-width="1080" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202205/6285f37d8e9f091f45030510_1024.jpg" referrerpolicy="no-referrer"></div></div>威斯康星大学麦迪逊分校助理教授 Sebastian Raschka 也对 M1 芯片的 GPU 机器学习能力进行了一番测试，他使用的芯片是 M1 和 M1 Pro。<p></p><p></p><div class="img_box" id="id_imagebox_13" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_13" data-original="http://zkres2.myzaker.com/202205/6285f37d8e9f091f45030511_1024.jpg" data-height="475" data-width="1030" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202205/6285f37d8e9f091f45030511_1024.jpg" referrerpolicy="no-referrer"></div></div>看上去，M1 CPU 似乎比 M1 GPU 更快。但 LeNet-5 是一个非常小的网络，而 MNIST 是一个非常小的数据集。如果用 rescaled CIFAR-10 图像再试一次，结果如下：<p></p><p></p><div class="img_box" id="id_imagebox_14" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_14" data-original="http://zkres2.myzaker.com/202205/6285f37d8e9f091f45030512_1024.jpg" data-height="469" data-width="1035" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202205/6285f37d8e9f091f45030512_1024.jpg" referrerpolicy="no-referrer"></div></div>与 M1 Pro CPU（正数第二行）和 M1 Pro GPU（倒数第二行）相比，M1 Pro GPU 训练网络的速度提高了一倍。<p></p><p>可见，M1 系列芯片的 GPU 加速结果非常可观，在部分情况下已能满足开发者的需求。不过我们知道在 M1 Ultra 这样的芯片中也有 32 核的神经网络引擎，目前却只有苹果自己的 Core ML 框架支持使用该部分获得加速。</p><p>不知启用了 Neural Engine 之后，M1 芯片的 AI 推理速度还能提升多少？</p><div id="recommend_bottom"></div><div id="article_bottom"></div>  
</div>
            