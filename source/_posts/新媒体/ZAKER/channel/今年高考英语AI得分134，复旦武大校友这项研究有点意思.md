
---
title: '今年高考英语AI得分134，复旦武大校友这项研究有点意思'
categories: 
 - 新媒体
 - ZAKER
 - channel
headimg: 'https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202206/62b72c658e9f096af96ea286_1024.jpg'
author: ZAKER
comments: false
date: Sat, 25 Jun 2022 07:52:13 GMT
thumbnail: 'https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202206/62b72c658e9f096af96ea286_1024.jpg'
---

<div>   
<p>在挑战写语文作文后，AI 现在又盯上了高考英语。</p><p>结果好家伙，今年高考英语卷（全国甲卷）一上手，就拿了<strong>134 分</strong>。</p><p></p><div class="img_box" id="id_imagebox_0" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_0" data-original="http://zkres2.myzaker.com/202206/62b72c658e9f096af96ea286_1024.jpg" data-height="381" data-width="382" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202206/62b72c658e9f096af96ea286_1024.jpg" referrerpolicy="no-referrer"></div></div>而且不是偶然的超常发挥。<p></p><p>在 2018-2021 年的 10 套真题测试中，AI 的分数都在 125 分以上，最高纪录为 138.5 分，听力和阅读理解还拿过<strong>满分</strong>。</p><p>这就是由 CMU 学者提出的，高考英语测试 AI 系统<strong>Qin</strong>。</p><p>它的参数量只有 GPT-3 的<strong>16 分之一</strong>，平均成绩却比 GPT-3 高出 15 分。</p><p></p><div class="img_box" id="id_imagebox_1" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_1" data-original="http://zkres1.myzaker.com/202206/62b72c658e9f096af96ea287_1024.jpg" data-height="932" data-width="1080" src="https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202206/62b72c658e9f096af96ea287_1024.jpg" referrerpolicy="no-referrer"></div></div>其背后的秘诀名叫<strong>重构预训练</strong>（reStructured Pre-training），是作者提出的一种新学习范式。<p></p><p>具体来看，就是把维基百科、YouTube 等平台的信息重新提取重构，再喂给 AI 进行训练，由此让 AI 具有更强的泛化能力。</p><p>两位学者用足足<strong>100 多页</strong>的论文，深入解释了这一新范式。</p><p></p><div class="img_box" id="id_imagebox_2" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_2" data-original="http://zkres1.myzaker.com/202206/62b72c658e9f096af96ea288_1024.jpg" data-height="758" data-width="872" src="https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202206/62b72c658e9f096af96ea288_1024.jpg" referrerpolicy="no-referrer"></div></div>那么，这一范式到底讲了什么？<p></p><p>我们来深扒一下 ~</p><p><b>什么是重构预训练？</b></p><p>论文题目很简单，就叫 reStructured Pre-training（重构预训练，RST）。</p><p></p><div class="img_box" id="id_imagebox_3" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_3" data-original="http://zkres1.myzaker.com/202206/62b72c658e9f096af96ea289_1024.jpg" data-height="285" data-width="1080" src="https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202206/62b72c658e9f096af96ea289_1024.jpg" referrerpolicy="no-referrer"></div></div>核心观点凝练来说就是一句话，要<strong>重视数据</strong>啊！<p></p><p>作者认为，这个世界上有价值的信息无处不在，而目前的 AI 系统并没有充分利用数据中的信息。</p><p>比如像维基百科，Github，里面包含了各种可以供模型学习的信号：实体，关系，文本摘要，文本主题等。这些信号之前由于技术瓶颈都没有被考虑。</p><p>所以，作者在本文中提出了一种方法，可以用神经网络统一地<strong>存储和访问</strong>包含各种类型信息的数据。</p><p>他们以信号为单位、结构化地表示数据，这很类似于数据科学里我们常常将数据构造成表或 JSON 格式，然后通过专门的语言（如 SQL）来检索所需的信息。</p><p></p><div class="img_box" id="id_imagebox_4" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_4" data-original="http://zkres2.myzaker.com/202206/62b72c658e9f096af96ea28a_1024.jpg" data-height="369" data-width="1080" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202206/62b72c658e9f096af96ea28a_1024.jpg" referrerpolicy="no-referrer"></div></div>具体来看，这里的信号，其实就是指数据中的有用信息。<p></p><p>比如在 " 莫扎特生于萨尔茨堡 " 这句话中，" 莫扎特 "、" 萨尔茨堡 " 就是信号。</p><p>然后，就需要在各种平台上挖掘数据、提取信号，作者把这个过程比作了从矿山里寻宝。</p><p></p><div class="img_box" id="id_imagebox_5" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_5" data-original="http://zkres1.myzaker.com/202206/62b72c658e9f096af96ea28b_1024.jpg" data-height="822" data-width="1080" src="https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202206/62b72c658e9f096af96ea28b_1024.jpg" referrerpolicy="no-referrer"></div></div>接下来，利用 prompt 方法，就能将这些来自不同地方的信号统一成一种形式。<p></p><p>最后，再将这些重组的数据集成并存储到语言模型中。</p><p>这样一来，该研究就能从 10 个数据源中，统一<strong>26</strong><strong>种</strong>不同类型的信号，让模型获得很强的泛化能力。</p><p>结果表明，在多个数据集中，RST-T、RST-A 零样本学习的表现，都<strong>优于</strong>GPT-3 的少样本学习性能。</p><p></p><div class="img_box" id="id_imagebox_6" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_6" data-original="http://zkres1.myzaker.com/202206/62b72c658e9f096af96ea28c_1024.jpg" data-height="180" data-width="1080" src="https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202206/62b72c658e9f096af96ea28c_1024.jpg" referrerpolicy="no-referrer"></div></div>而为了更进一步测试新方法的表现，作者还想到了让<strong>AI 做高考题</strong>的方法。<p></p><p>他们表示，现在很多工作方法走的都是汉化 GPT-3 的思路，在评估的应用场景上也是跟随 OpenAI、DeepMind。</p><p>比如 GLUE 测评基准、蛋白质折叠评分等。</p><p>基于对当下 AI 模型发展的观察，作者认为可以开辟出一条新的赛道试试，所以就想到了用高考给 AI 练练手。</p><p>他们找来了前后几年共 10 套试卷进行标注，请高中老师来进行打分。</p><p>像听力 / 识图理解这样的题目，还找来机器视觉、语音识别领域的学者帮忙。</p><p>最终，炼出了这套高考英语 AI 模型，也可以叫她为<strong>Qin</strong>。</p><p></p><div class="img_box" id="id_imagebox_7" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_7" data-original="http://zkres2.myzaker.com/202206/62b72c658e9f096af96ea28d_1024.jpg" data-height="418" data-width="1030" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202206/62b72c658e9f096af96ea28d_1024.jpg" referrerpolicy="no-referrer"></div></div>从测试结果可以看到，Qin 绝对是学霸级别了，10 套卷子成绩都高于 T0pp 和 GPT-3。<p></p><p></p><div class="img_box" id="id_imagebox_8" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_8" data-original="http://zkres1.myzaker.com/202206/62b72c658e9f096af96ea28e_1024.jpg" data-height="305" data-width="1080" src="https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202206/62b72c658e9f096af96ea28e_1024.jpg" referrerpolicy="no-referrer"></div></div>此外，作者还提出了高考 benchmark。<p></p><p>他们觉得当下很多评价基准的任务都很单一，大多没有实用价值，和人类情况对比也比较困难。</p><p>而高考题目既涵盖了各种各样的知识点，还直接有人类分数来做比对，可以说是一箭双雕了。</p><p><b>NLP 的第五范式？</b></p><p>如果从更深层次来看，作者认为，重构预训练或许会成为 NLP 的一种新范式，即把<strong>预训练 / 微调</strong>过程视为<strong>数据存储 / 访问</strong>过程。</p><p>此前，作者将 NLP 的发展总结成了 4 种范式：</p><p>P1. 非神经网络时代的完全监督学习 （Fully Supervised Learning, Non-Neural Network）</p><p>P2. 基于神经网络的完全监督学习 ( Fully Supervised Learning, Neural Network ) </p><p>P3. 预训练，精调范式 ( Pre-train, Fine-tune ) </p><p>P4. 预训练，提示，预测范式（Pre-train, Prompt, Predict）</p><p></p><div class="img_box" id="id_imagebox_9" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_9" data-original="http://zkres1.myzaker.com/202206/62b72c658e9f096af96ea28f_1024.jpg" data-height="452" data-width="1080" src="https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202206/62b72c658e9f096af96ea28f_1024.jpg" referrerpolicy="no-referrer"></div></div>但是基于当下对 NLP 发展的观察，他们认为或许之后可以以一种 data-centric 的方式来看待问题。<p></p><p>也就是，预训 / 精调、few-shot/zero-shot 等概念的差异化会更加模糊，核心只关注一个点——</p><p><strong>有价值的信息有多少、能利用多少。</strong></p><p>此外，他们还提出了一个 NLP 进化假说。</p><p>其中的核心思想是，技术发展方向总是顺着这样的——做更少的事实现更好、更通用的系统。</p><p>作者认为，NLP 经历了特征工程、架构工程、目标工程、提示工程，当下正在朝着数据工程方向发展。</p><p></p><div class="img_box" id="id_imagebox_10" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_10" data-original="http://zkres2.myzaker.com/202206/62b72c658e9f096af96ea290_1024.jpg" data-height="348" data-width="614" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202206/62b72c658e9f096af96ea290_1024.jpg" referrerpolicy="no-referrer"></div></div><b>复旦武大校友打造</b><p></p><p>本篇论文的一作为<strong>Weizhe Yuan</strong>。</p><p>她本科毕业于武汉大学，后赴卡内基梅隆大学读研，学习数据科学专业。</p><p>研究方向集中在 NLP 任务的文本生成和评估。</p><p>去年，她被 AAAI 2022、NeurIPS 2021 分别接收了一篇论文，还获得了 ACL 2021 Best Demo Paper Award。</p><p></p><div class="img_box" id="id_imagebox_11" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_11" data-original="http://zkres1.myzaker.com/202206/62b72c658e9f096af96ea291_1024.jpg" data-height="602" data-width="602" src="https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202206/62b72c658e9f096af96ea291_1024.jpg" referrerpolicy="no-referrer"></div></div>论文的通讯作者为卡内基梅隆大学语言技术研究所（LTI）的博士后研究员<strong>刘鹏飞</strong>。<p></p><p>他于 2019 年在复旦大学计算机系获得博士学位，师从邱锡鹏教授、黄萱菁教授。</p><p>研究兴趣包括 NLP 模型可解释性、迁移学习、任务学习等。</p><p>博士期间，他包揽了各种计算机领域的奖学金，包括 IBM 博士奖学金、微软学者奖学金、腾讯人工智能奖学金、百度奖学金。</p><p></p><div class="img_box" id="id_imagebox_12" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_12" data-original="http://zkres1.myzaker.com/202206/62b72c658e9f096af96ea292_1024.jpg" data-height="800" data-width="600" src="https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202206/62b72c658e9f096af96ea292_1024.jpg" referrerpolicy="no-referrer"></div></div><b>One More Thing</b><p></p><p>值得一提的是，刘鹏飞在和我们介绍这项工作时，直言 " 最初我们就没打算拿去投稿 "。</p><p>这是因为他们不想让会议论文的格式限制了构思论文的想象力。</p><p>我们决定把这篇论文当作一个故事来讲，并给 " 读者 " 一种看电影的体验。</p><p>这也是为什么我们在第三页，设置了一个 " 观影模式 " 的全景图。</p><p>就是为了带着大家去了解 NLP 发展的历史，以及我们所展望的未来是怎样的，让每一个研究者都能有一定的代入感，感受到自己去带领着预训练语言模型们 ( PLMs ) 通过矿山寻宝走向更好明天的一个过程。</p><p></p><div class="img_box" id="id_imagebox_13" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_13" data-original="http://zkres2.myzaker.com/202206/62b72c658e9f096af96ea293_1024.jpg" data-height="841" data-width="1080" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202206/62b72c658e9f096af96ea293_1024.jpg" referrerpolicy="no-referrer"></div></div>论文结尾，还藏了一些惊喜彩蛋。<p></p><p>比如 PLMs 主题表情包：</p><p></p><div class="img_box" id="id_imagebox_14" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_14" data-original="http://zkres2.myzaker.com/202206/62b72c658e9f096af96ea294_1024.jpg" data-height="1262" data-width="866" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202206/62b72c658e9f096af96ea294_1024.jpg" referrerpolicy="no-referrer"></div></div>还有结尾的插画：<p></p><p></p><div class="img_box" id="id_imagebox_15" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_15" data-original="http://zkres1.myzaker.com/202206/62b72c658e9f096af96ea295_1024.jpg" data-height="804" data-width="1080" src="https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202206/62b72c658e9f096af96ea295_1024.jpg" referrerpolicy="no-referrer"></div></div>这么看，<strong>100 多页</strong>的论文读起来也不会累 ~<p></p><div id="recommend_bottom"></div><div id="article_bottom"></div>  
</div>
            