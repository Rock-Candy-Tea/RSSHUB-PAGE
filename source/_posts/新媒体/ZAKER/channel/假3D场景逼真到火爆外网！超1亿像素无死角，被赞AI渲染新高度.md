
---
title: '假3D场景逼真到火爆外网！超1亿像素无死角，被赞AI渲染新高度'
categories: 
 - 新媒体
 - ZAKER
 - channel
headimg: 'https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202110/616d26ef8e9f0933cc35de1a_1024.jpg'
author: ZAKER
comments: false
date: Mon, 18 Oct 2021 00:07:55 GMT
thumbnail: 'https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202110/616d26ef8e9f0933cc35de1a_1024.jpg'
---

<div>   
<p>先来看一段 " 视频 "，有没有看出什么不对劲的地方？</p><p></p><div class="img_box" id="id_imagebox_0" onclick><div class="content_img_div"><img class="lazy opacity_0 zaker_gif_cache" id="img_0" data-original="http://zkres2.myzaker.com/202110/616d26ef8e9f0933cc35de1a_1024.jpg" data-gif-url="http://zkres2.myzaker.com/202110/616d26ef8e9f0933cc35de1a_raw.gif" data-height="360" data-width="480" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202110/616d26ef8e9f0933cc35de1a_1024.jpg" referrerpolicy="no-referrer"></div></div>其实，这仅仅是由<strong>一组照片</strong>渲染出来的（右下角为拍摄照片）！<p></p><p></p><div class="img_box" id="id_imagebox_1" onclick><div class="content_img_div"><img class="lazy opacity_0 zaker_gif_cache" id="img_1" data-original="http://zkres2.myzaker.com/202110/616d26ef8e9f0933cc35de1b_1024.jpg" data-gif-url="http://zkres2.myzaker.com/202110/616d26ef8e9f0933cc35de1b_raw.gif" data-height="252" data-width="480" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202110/616d26ef8e9f0933cc35de1b_1024.jpg" referrerpolicy="no-referrer"></div></div>生成的也不仅仅是一段视频，更是一个<strong>3D 场景模型</strong>，不仅能任意角度随意切换、高清无死角，还能调节曝光、白平衡等参数，生成船新的照片：<p></p><p></p><div class="img_box" id="id_imagebox_2" onclick><div class="content_img_div"><img class="lazy opacity_0 zaker_gif_cache" id="img_2" data-original="http://zkres1.myzaker.com/202110/616d26ef8e9f0933cc35de1c_1024.jpg" data-gif-url="http://zkres1.myzaker.com/202110/616d26ef8e9f0933cc35de1c_raw.gif" data-height="267" data-width="480" src="https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202110/616d26ef8e9f0933cc35de1c_1024.jpg" referrerpolicy="no-referrer"></div></div>在完全不同的场景下，例如一个坦克厂中，同样能用一组照片渲染出逼真 3D 场景，相同角度与真实拍摄图像几乎 "<strong>完全一致</strong>"：<p></p><p></p><div class="img_box" id="id_imagebox_3" onclick><div class="content_img_div"><img class="lazy opacity_0 zaker_gif_cache" id="img_3" data-original="http://zkres1.myzaker.com/202110/616d26ef8e9f0933cc35de1d_1024.jpg" data-gif-url="http://zkres1.myzaker.com/202110/616d26ef8e9f0933cc35de1d_raw.gif" data-height="257" data-width="480" src="https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202110/616d26ef8e9f0933cc35de1d_1024.jpg" referrerpolicy="no-referrer"></div></div>要知道，之前<strong>苹果</strong>虽然也做过一组照片生成目标物体 3D 模型的功能，但最多就是一件物体，例如一只箱子：<p></p><p></p><div class="img_box" id="id_imagebox_4" onclick><div class="content_img_div"><img class="lazy opacity_0 zaker_gif_cache" id="img_4" data-original="http://zkres1.myzaker.com/202110/616d26ef8e9f0933cc35de1e_1024.jpg" data-gif-url="http://zkres1.myzaker.com/202110/616d26ef8e9f0933cc35de1e_raw.gif" data-height="248" data-width="440" src="https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202110/616d26ef8e9f0933cc35de1e_1024.jpg" referrerpolicy="no-referrer"></div></div>这次可是整个 3D 场景！<p></p><p></p><div class="img_box" id="id_imagebox_5" onclick><div class="content_img_div"><img class="lazy opacity_0 zaker_gif_cache" id="img_5" data-original="http://zkres1.myzaker.com/202110/616d26ef8e9f0933cc35de1f_1024.jpg" data-gif-url="http://zkres1.myzaker.com/202110/616d26ef8e9f0933cc35de1f_raw.gif" data-height="248" data-width="440" src="https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202110/616d26ef8e9f0933cc35de1f_1024.jpg" referrerpolicy="no-referrer"></div></div>这是德国埃尔朗根 - 纽伦堡大学的几位研究人员做的项目，效果一出就火得不行，在国外社交媒体上赞数超过<strong>5k</strong>，阅读量达到<strong>36w+</strong>。<p></p><p></p><div class="img_box" id="id_imagebox_6" onclick><div class="content_img_div"><img class="lazy opacity_0 zaker_gif_cache" id="img_6" data-original="http://zkres2.myzaker.com/202110/616d26ef8e9f0933cc35de20_1024.jpg" data-gif-url="http://zkres2.myzaker.com/202110/616d26ef8e9f0933cc35de20_raw.gif" data-height="331" data-width="480" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202110/616d26ef8e9f0933cc35de20_1024.jpg" referrerpolicy="no-referrer"></div></div>那么，这样神奇的效果，究竟是怎么生成的呢？<p></p><p><b>用照片还原整个 3D 场景图</b></p><p>整体来说，这篇论文提出了一种基于点的可微神经渲染流水线<strong>ADOP</strong>（Approximate Differentiable One-Pixel Point Rendering），用 AI 分析输入图像，并输出新角度的新图像。</p><p></p><div class="img_box" id="id_imagebox_7" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_7" data-original="http://zkres2.myzaker.com/202110/616d26ef8e9f0933cc35de21_1024.jpg" data-height="253" data-width="1080" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202110/616d26ef8e9f0933cc35de21_1024.jpg" referrerpolicy="no-referrer"></div></div>在输入时，由于需要建模 3D 场景，因此这里的照片需要经过严格拍摄，来获取整个场景的稀疏点云数据。<p></p><p>具体来说，作者在从照片获取点云数据时，采用了<strong>COLMAP</strong>。</p><p>先从多个不同的角度拍摄场景中的照片，其中每张照片的视角都会经过严格控制。</p><p>然后采用 SfM（Structure From Motion，运动恢复结构）方法，来获取相机内外参数，得到整个场景的 3D 重建数据，也就是表示场景结构的稀疏点云：</p><p></p><div class="img_box" id="id_imagebox_8" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_8" data-original="http://zkres2.myzaker.com/202110/616d26ef8e9f0933cc35de22_1024.jpg" data-height="211" data-width="1080" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202110/616d26ef8e9f0933cc35de22_1024.jpg" referrerpolicy="no-referrer"></div></div>然后，包含点云等信息的场景数据会被输入到流水线中，进行进一步的处理。<p></p><p>流水线（pipeline）主要分为<strong>三个部分</strong>：可微光栅化器、神经渲染器和可微色调映射器。</p><p></p><div class="img_box" id="id_imagebox_9" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_9" data-original="http://zkres2.myzaker.com/202110/616d26ef8e9f0933cc35de23_1024.jpg" data-height="253" data-width="1080" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202110/616d26ef8e9f0933cc35de23_1024.jpg" referrerpolicy="no-referrer"></div></div>首先，利用多分辨率的单像素点栅格化可微渲染器（可微光栅化器），将输入的相机参数、重建的点云数据转换成稀疏神经图像。<p></p><p>其中，模型里关于图像和点云对齐的部分，采用了 NavVis 数据集来训练。</p><p>然后，利用神经渲染器，对稀疏神经图像进行阴影计算和孔洞填充，生成 HDR 图片。<strong></strong></p><p></p><div class="img_box" id="id_imagebox_10" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_10" data-original="http://zkres1.myzaker.com/202110/616d26ef8e9f0933cc35de24_1024.jpg" data-height="339" data-width="1080" src="https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202110/616d26ef8e9f0933cc35de24_1024.jpg" referrerpolicy="no-referrer"></div></div>最后，由于不是每个设备都支持 HDR 画面，因此在显示到 LDR 设备之前，还需要利用基于物理的可微色调映射器改变动态范围，将 HDR 图像变成 LDR 图像。<p></p><p><b>每个场景 300+ 图像训练</b></p><p>这个新模型的优势在哪里？</p><p>由于模型的所有阶段都可微，因此这个模型能够优化场景<strong>所有参数</strong>（相机模型、相机姿势、点位置、点颜色、环境图、渲染网络权重、渐晕、相机响应函数、每张图像的曝光和每张图像的白平衡），并用来生成质量更高的图像。</p><p>具体到训练上，作者先是采用了 688 张图片（包含 73M 个点）来训练这个神经渲染流水线（pipeline）。</p><p></p><div class="img_box" id="id_imagebox_11" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_11" data-original="http://zkres2.myzaker.com/202110/616d26ef8e9f0933cc35de25_1024.jpg" data-height="490" data-width="948" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202110/616d26ef8e9f0933cc35de25_1024.jpg" referrerpolicy="no-referrer"></div></div>针对 demo 中的几个场景（火车、灯塔、游乐园、操场等），作者们分别用高端摄像机拍摄了 300~350 张全高清图像，每个场景生成的像素点数量分别为 10M、8M、12M 和 11M，其中 5% 的图像用作测试。<p></p><p>也就是说，制作这样一个 3D 场景，大约需要几百张图像，同时每张图像的拍摄需要经过严格的角度控制。</p><p>不过仍然有读者表示，拍几百张图像就能用 AI 做个场景出来，这个速度比当前人工渲染是要快多了。</p><p></p><div class="img_box" id="id_imagebox_12" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_12" data-original="http://zkres1.myzaker.com/202110/616d26ef8e9f0933cc35de26_1024.jpg" data-height="313" data-width="1080" src="https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202110/616d26ef8e9f0933cc35de26_1024.jpg" referrerpolicy="no-referrer"></div></div>功能上，模型既能生成可以调节参数的新角度照片，还能自动插值生成全场景的 3D 渲染视频，可以说是挺有潜力的。<p></p><p>那么，这个模型的效果与当前其他模型的渲染效果相比如何呢？</p><p><b>实时显示 1 亿 + 像素点场景</b></p><p>据作者表示，论文中采用的高效单像素点栅格化方法，使得 ADOP 能够使用任意的相机模型，并<strong>实时显示超过 1 亿个像素点</strong>的场景。</p><p>肉眼分辨生成结果来看，采用同行几个最新模型生成的图片，或多或少会出现一些伪影或是不真实的情况，相比之下 ADOP 在细节上处理得都非常不错：</p><p></p><div class="img_box" id="id_imagebox_13" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_13" data-original="http://zkres2.myzaker.com/202110/616d26ef8e9f0933cc35de27_1024.jpg" data-height="868" data-width="1080" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202110/616d26ef8e9f0933cc35de27_1024.jpg" referrerpolicy="no-referrer"></div></div>从数据来看，无论是火车、操场、坦克还是灯塔场景，在 ADOP 模型的渲染下，在 VGG、LPIPS 和 PSNR 上几乎都能取得最优秀的结果（除了坦克的数据）。<p></p><p></p><div class="img_box" id="id_imagebox_14" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_14" data-original="http://zkres1.myzaker.com/202110/616d26ef8e9f0933cc35de28_1024.jpg" data-height="196" data-width="1080" src="https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202110/616d26ef8e9f0933cc35de28_1024.jpg" referrerpolicy="no-referrer"></div></div>不过，研究本身也还具有一些局限性，例如单像素点渲染仍然存在点云稀疏时，渲染出现孔洞等问题。<p></p><p>但整体来看，实时显示 3D 场景的效果还是非常出类拔萃的，不少业内人士表示 " 达到了 AI 渲染新高度 "。</p><p>已经有不少网友开始想象这项研究的用途，例如给电影制片厂省去一大波时间和精力：</p><p></p><div class="img_box" id="id_imagebox_15" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_15" data-original="http://zkres1.myzaker.com/202110/616d26ef8e9f0933cc35de29_1024.jpg" data-height="237" data-width="1080" src="https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202110/616d26ef8e9f0933cc35de29_1024.jpg" referrerpolicy="no-referrer"></div></div>（甚至有电影系的学生想直接用到毕设上）<p></p><p></p><div class="img_box" id="id_imagebox_16" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_16" data-original="http://zkres1.myzaker.com/202110/616d26ef8e9f0933cc35de2a_1024.jpg" data-height="134" data-width="1080" src="https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202110/616d26ef8e9f0933cc35de2a_1024.jpg" referrerpolicy="no-referrer"></div></div>对游戏行业影响也非常不错：<p></p><p>在家就能搞 3A 大作的场景，是不是也要实现了？简直让人迫不及待。</p><p></p><div class="img_box" id="id_imagebox_17" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_17" data-original="http://zkres1.myzaker.com/202110/616d26ef8e9f0933cc35de2b_1024.jpg" data-height="187" data-width="1080" src="https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202110/616d26ef8e9f0933cc35de2b_1024.jpg" referrerpolicy="no-referrer"></div></div>还有人想象，要是能在 iPhone 上实现就好了（甚至已经给 iPhone 15 预定上了）：<p></p><p></p><div class="img_box" id="id_imagebox_18" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_18" data-original="http://zkres2.myzaker.com/202110/616d26ef8e9f0933cc35de2c_1024.jpg" data-height="280" data-width="1080" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202110/616d26ef8e9f0933cc35de2c_1024.jpg" referrerpolicy="no-referrer"></div></div>对于研究本身，有网友从行外人视角看来，感觉更像是插帧模型（也有网友回应说差不多是这样）：<p></p><p></p><div class="img_box" id="id_imagebox_19" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_19" data-original="http://zkres2.myzaker.com/202110/616d26ef8e9f0933cc35de2d_1024.jpg" data-height="386" data-width="1080" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202110/616d26ef8e9f0933cc35de2d_1024.jpg" referrerpolicy="no-referrer"></div></div>也有网友表示，由于需要的图像比较多，效果没有宣传中那么好，对研究潜力持保留态度：<p></p><p></p><div class="img_box" id="id_imagebox_20" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_20" data-original="http://zkres1.myzaker.com/202110/616d26ef8e9f0933cc35de2e_1024.jpg" data-height="210" data-width="1080" src="https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202110/616d26ef8e9f0933cc35de2e_1024.jpg" referrerpolicy="no-referrer"></div></div>虽然目前作者们已经建立了 GitHub 项目，但代码还没有放出来，感兴趣的同学们可以先蹲一波。<p></p><p>至于具体的开源时间，作者们表示 " 会在<strong>中了顶会</strong>后再放出来 "。（祝这篇论文成功被顶会收录 ~）</p><div id="recommend_bottom"></div><div id="article_bottom"></div>  
</div>
            