
---
title: '如果这篇文章为AI所写，那是否意味它已学会思考？'
categories: 
 - 新媒体
 - ZAKER
 - channel
headimg: 'https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202208/630426638e9f092123514b6b_1024.jpg'
author: ZAKER
comments: false
date: Tue, 23 Aug 2022 02:33:00 GMT
thumbnail: 'https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202208/630426638e9f092123514b6b_1024.jpg'
---

<div>   
<p></p><div class="img_box" id="id_imagebox_0" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_0" data-original="http://zkres1.myzaker.com/202208/630426638e9f092123514b6b_1024.jpg" data-height="1920" data-width="1080" src="https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202208/630426638e9f092123514b6b_1024.jpg" referrerpolicy="no-referrer"></div></div>当看到现在这句话的时候，你会根据过去的经验觉得是由一个活生生的人类写出来的。而且，现在的确是人类在打字：" 你好！" 不过，在现在这个时代，很多看起来像人写的句子其实是由人工智能系统经过大量人工文本训练后生成的。<p></p><p>人们不假思索地认为，<strong>流利语言的背后一定是活生生的人类，以至于很容易忽略证伪这种假定的反例。</strong>那么，人们又是如何开始探究这一未经涉猎的领域——一段文本究竟是由人类创造，还是人工智能生成的？人们倾向于将流利的表达和连贯的思想联系在一起。由于有这种想法，人们自然就会认为，如果人工智能模型能够自如地表达 " 自我 "，就说明它能像人类一样可知可感。</p><p>最近，谷歌前工程师声称，" 因为谷歌的人工智能系统 LaMDA 可以巧舌如簧地用文字来表达‘自我情绪’，所以它具有自我意识。" 这种观点并不出人意料。在媒体 1 上发酵后，这一观点引发了一系列 2 持高度批判态度的文章 3 和推文 4。<strong>它们质疑的点在于，（如果）人类语言的处理模型是有情绪的，这就意味着它们有思考、感觉和感受的能力。</strong>（即，人工智能太像人这件事引发了恐慌。）</p><p></p><div class="img_box" id="id_imagebox_1" onclick><div class="content_img_div"><img class="lazy opacity_0 zaker_gif_cache" id="img_1" data-original="http://zkres1.myzaker.com/202208/630426638e9f092123514b6c_1024.jpg" data-gif-url="http://zkres1.myzaker.com/202208/630426638e9f092123514b6c_raw.gif" data-height="214" data-width="382" src="https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202208/630426638e9f092123514b6c_1024.jpg" referrerpolicy="no-referrer"></div></div>- LaMDA -<p></p><p>" 人工智能模型可能有感情这事意味着什么 " 是个十分复杂的问题（请看我们同事的调研 5），不过本篇文章的目的并不是彻底搞明白这个问题。作为语言科学研究者 6，我们能做的是用在认知科学和语言学领域的科研成果来解释<strong>为什么人类如此容易落入认知的陷阱</strong>，即，误认为能流利使用语言的实体就一定是有感情、有意识、智力的。</p><p><strong>用人工智能生成人类语言</strong></p><p>技术发展到现在，我们已经很难区分 LaMDA 语言模型生成的文本和人类写的句子。这一惊人的成就归功于数十年以来研究者们在此领域的投入。在科研项目中，研究者们搭建出了可以生成出语法正确且语意连贯的话语的自然语言模型。</p><p>20 世纪 50 年代的早期版本（被称之为 "n 元模型 " [ n-gram models ] ）只能靠计算某个特定词组出现的次数来推测特定语境下哪个词会出现。比如，我们其实很容易就能猜出来 " 花生酱和果酱 "（一种很常见的三明治夹心组合）肯定比 " 花生酱和菠萝 " 更可能被放在一起组词。如果手头有足量英语文本，你会看到 " 花生酱和果酱 " 这个词组一次又一次地出现，但很有可能永远看不到 " 花生酱和菠萝 " 这个词组出现。</p><p></p><div class="img_box" id="id_imagebox_2" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_2" data-original="http://zkres2.myzaker.com/202208/630426638e9f092123514b6d_1024.jpg" data-height="590" data-width="1080" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202208/630426638e9f092123514b6d_1024.jpg" referrerpolicy="no-referrer"></div></div><strong>第一个让人们进行对话的计算机系统是半个多世纪前建造的名为 Eliza 的心理治疗软件。</strong><p></p><p>—</p><p>Rosenfeld Media/Flickr</p><p><strong>当今的语言模型用数据组和规则来模拟人类语言，这和早期的尝试已经有了很大程度上的不同。</strong>首先，当今的模型是利用整个互联网做语料库来训练的。其次，它们可以学习两个毫无关系的词汇之间的联系，而不仅仅停留在同义词或者关联词语之间。再次，它们是用大量的内在 " 球形节点 " 来校准的——校准点的数量之多，即便是编写算法的程序员们也很难完全理解为什么这些模型会生成这一句话而不是那一句话。</p><p>然而，当今的语言模型和 20 世纪 50 年代的版本在使命上并无不同，仍然是找到下一个应该出现的词汇。不过，当今的语言模型在这个领域上已经做得很好了，几乎所有生成的句子都能保证语言流畅、语法通顺。</p><p></p><div class="img_box" id="id_imagebox_3" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_3" data-original="http://zkres2.myzaker.com/202208/630426638e9f092123514b6e_1024.jpg" data-height="1067" data-width="800" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202208/630426638e9f092123514b6e_1024.jpg" referrerpolicy="no-referrer"></div></div>- Karolis Strautniekas -<p></p><p><strong>花生酱和菠萝？</strong></p><p>我们让大型语言生成模型 GPT-37 补全句子 " 花生酱和菠萝 ______"。它给出如下表达，" 花生酱和菠萝是很不错的搭配。花生酱和菠萝完美互补，甜咸口，很好吃。" 如果这句话是人写的，我们可能会猜笔者把花生酱和菠萝放在一起吃过，于是形成了这样的感受，并把这种感受分享给读者。</p><p>但是，GPT-3 是如何说出这句话的呢？答案是，<strong>根据给定语境，生成一个最符合这句话意思的词，然后再生成下一个词，然后继续生成下一个词。</strong>模型从来没见过、没摸过、也没尝过菠萝——它只处理过互联网上所有提到菠萝的文本。然而，阅读这段文本会误导人类（即便是谷歌工程师）认为 GPT-3 是一个有心智的存在，并且认为它的智力水平足以想象放了放了花生酱的菠萝是什么味道。</p><p><strong>Alif Jakir</strong></p><p>大型人工智能语言模型可以参与流利的对话。然而，他们大体上没什么话要说，因此，他们的措辞总是遵循常见的比喻，而这些比喻则是它们从训练文本中提取出来的。比如，如果给一个主题词 " 爱的本质 "，模型可能会生成出 " 相信爱情可以战胜一切 " 这样的句子。读者看到这句话，由于脑内的预判信息，会将这些文字理解为语言模型关于 " 爱情的本质 " 这个话题的观点，但其实这些话只是一系列能说得通的词组合在一起罢了。</p><p><strong>人脑被预设了 " 揣度文字背后的意思 " 这一功能。</strong>跟别人说话的时候，你的思维会不由自主地为你的说话对象建立一个思维模型。然后，你会把对方说的话输入模型，并解读出对方的目的、感受和看法。</p><p>从话语到思想模型的过程是无缝衔接的，并且，你每接收到一个全须全尾的句子，这种转换过程都会被触发一次。这种认知过程帮你省了很多时间和精力，而且极大程度上促进了你的社交。</p><p>然而，<strong>对于人工智能系统而言，这种思想模型是无效的——如同给废话文学做阅读理解。</strong></p><p>只要再多深究一点，就能发现尝试阅读理解人工智能的话语有多没用。看看以下题目：" 花生酱和羽毛放在一块很好吃，因为 _____。"GPT-3 续写如下：" 花生酱和羽毛放在一块很好吃，因为他们吃起来都有坚果的味道。花生酱用它的顺滑细腻的口感很好地抵消了羽毛的口感。"</p><p>这次的生成文本和前文提到的 " 花生酱和菠萝 " 那个例子在语法上一样流利，只是模型开始说胡话了。如果按照 " 人工智能模型可知可感 " 的理论，那些人就又该猜 GPT-3 从来没吃过花生酱和羽毛了。</p><p></p><div class="img_box" id="id_imagebox_4" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_4" data-original="http://zkres1.myzaker.com/202208/630426638e9f092123514b6f_1024.jpg" data-height="1132" data-width="800" src="https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202208/630426638e9f092123514b6f_1024.jpg" referrerpolicy="no-referrer"></div></div><strong>将人工智能归为机器而非人类</strong><p></p><p>认知偏见使人们把 GPT-3 归为人类，但讽刺的是，正是这种认知偏见使人们以非人的方式对待他人。语言学家在社会和文化的大背景下研究语言。他们的研究表明，<strong>人们主观臆断地认为流利的表达和连贯的思想之间存在紧密连接，这种主观臆断的结果是，人们会对语言表达不流利的人产生偏见。</strong></p><p>比如，人们通常觉得说话带口音的人不太聪明 8。除此之外，说话带口音的人也很难找到和他们业务能力相称的工作。同样的偏见也在歧视那些说土话 9（尤其是 " 乡巴佬方言 "，比如美国南部的方言 10）的人、用手语的人 11、以及有语言障碍的人（比如口吃 12）。</p><p>尽管这些偏见已经反复被证明为空穴来风，但仍有很多人在用它们伤害别人，引起种族歧视或者性别歧视。</p><p><strong>流利的语言并不完全意味着人性</strong></p><p>人工智能有朝一日会变得有感情吗？这个问题仍需深思，毕竟哲学家们已经为此思辨 13 了数十年 14。但是科研工作者们已经认清了一个事实，那便是，<strong>当自然语言生成模型描述它的感受的时候，我们不能听它说什么就信什么。</strong>文字很可能误导人，而且，我们实在是太容易认为流利的语言代表连贯的思想了。</p><div id="recommend_bottom"></div><div id="article_bottom"></div>  
</div>
            