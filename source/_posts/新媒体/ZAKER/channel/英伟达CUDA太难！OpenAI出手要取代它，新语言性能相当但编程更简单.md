
---
title: '英伟达CUDA太难！OpenAI出手要取代它，新语言性能相当但编程更简单'
categories: 
 - 新媒体
 - ZAKER
 - channel
headimg: 'https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202107/61025cb78e9f09408232c6e4_1024.jpg'
author: ZAKER
comments: false
date: Thu, 29 Jul 2021 04:51:00 GMT
thumbnail: 'https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202107/61025cb78e9f09408232c6e4_1024.jpg'
---

<div>   
<p>用 CUDA 为 GPU 编程实在太难了。</p><p>为了让没有 CUDA 编程经验的人写出和专家效率相当的 GPU 代码，现在 OpenAI 推出了一种新的语言和编译器——<strong>Triton</strong>。</p><p>它的难度比 CUDA 低，但是性能却可与之相媲美。</p><p>OpenAI 声称：</p><p>Triton 只要 25 行代码，就能在 FP16 矩阵乘法 shang 上达到与 cuBLAS 相当的性能。</p><p></p><div class="img_box" id="id_imagebox_0" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_0" data-original="http://zkres2.myzaker.com/202107/61025cb78e9f09408232c6e4_1024.jpg" data-height="728" data-width="640" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202107/61025cb78e9f09408232c6e4_1024.jpg" referrerpolicy="no-referrer"></div></div>OpenAI 的研究人员已经使用 Triton，来生成比同等 Torch 效率高出 1 倍的内核。<p></p><p>Triton 项目的负责人 Philippe Tillet 说：" 我们的目标是使 Triton 成为深度学习 CUDA 的可行替代方案。"</p><p>25 行代码实现最佳性能</p><p>Triton 起源于 Tillet 在 2019 年学术会议 MLPF 上的一篇论文，当时他还是哈佛大学的一名研究生。</p><p>Tillet 解决的问题是如何开发一种 cuDNN 更具表现力的语言，既能够处理神经网络中涉及的矩阵的各种操作，同时兼具可移植性且以及和 cuDNN 相媲美的性能。</p><p>现代 GPU 大致分为三个主要组件—— DRAM、SRAM、ALU，对这些资源进行调度管理十分复杂，即便是熟悉 CUDA 的程序员。</p><p></p><div class="img_box" id="id_imagebox_1" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_1" data-original="http://zkres1.myzaker.com/202107/61025cb78e9f09408232c6e5_1024.jpg" data-height="254" data-width="796" src="https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202107/61025cb78e9f09408232c6e5_1024.jpg" referrerpolicy="no-referrer"></div></div>Triton 可以将这些优化过程完全自动化，让开发者可以更好地专注于并行代码的高级逻辑。<p></p><p>以矩阵乘法为例，能够为逐元素运算和归约编写融合内核很重要，但考虑到神经网络中矩阵乘法任务的重要性，这还不够。</p><p>Triton 非常适合这些应用，只需约 25 行 Python 代码即可实现最佳性能。</p><p>@triton.jit</p><p>def matmul ( A, B, C, M, N, K, stride_am, stride_ak, </p><p> stride_bk, stride_bn, stride_cm, stride_cn,</p><p> **META ) :</p><p> # extract metaparameters</p><p> BLOCK_M, GROUP_M = META [ 'BLOCK_M' ] , META [ 'GROUP_M' ] </p><p> BLOCK_N = META [ 'BLOCK_N' ] </p><p> BLOCK_K = META [ 'BLOCK_K' ] </p><p> # programs are grouped together to improve L2 hit rate</p><p> _pid_m = tl.program_id ( 0 ) </p><p> _pid_n = tl.program_id ( 1 ) </p><p> pid_m = _pid_m // GROUP_M</p><p> pid_n = ( _pid_n * GROUP_M ) + ( _pid_m % GROUP_M ) </p><p> # rm ( resp. rn ) denotes a range of indices</p><p> # for rows ( resp. col ) of C</p><p> rm = pid_m * BLOCK_M + tl.arange ( 0, BLOCK_M ) </p><p> rn = pid_n * BLOCK_N + tl.arange ( 0, BLOCK_N ) </p><p> # rk denotes a range of indices for columns </p><p> # ( resp. rows ) of A ( resp. B ) </p><p> rk = tl.arange ( 0, BLOCK_K ) </p><p> # the memory addresses of elements in the first block of</p><p> # A and B can be computed using numpy-style broadcasting</p><p> A = A + ( rm [ :, None ] * stride_am + rk [ None, : ] * stride_ak ) </p><p> B = B + ( rk [ :, None ] * stride_bk + rn [ None, : ] * stride_bn ) </p><p> # initialize and iteratively update accumulator</p><p> acc = tl.zeros ( ( BLOCK_M, BLOCK_N ) , dtype=tl.float32 ) </p><p> for k in range ( K, 0, -BLOCK_K ) :</p><p> a = tl.load ( A ) </p><p> b = tl.load ( B ) </p><p> # block level matrix multiplication</p><p> acc += tl.dot ( a, b ) </p><p> # increment pointers so that the next blocks of A and B</p><p> # are loaded during the next iteration</p><p> A += BLOCK_K * stride_ak</p><p> B += BLOCK_K * stride_bk</p><p> # fuse leaky ReLU if desired</p><p> # acc = tl.where ( acc >= 0, acc, alpha * acc ) </p><p> # write back result</p><p> C = C + ( rm [ :, None ] * stride_cm + rn [ None, : ] * stride_cn ) </p><p> mask = ( rm [ :, None ] < M ) & ( rn [ None, : ] < N ) </p><p> tl.store ( C, acc, mask=mask ) </p><p>而另一方面，在 CUDA 中实现类似的过程需要花费更多的精力，甚至可能会降低性能。</p><p>手写矩阵乘法内核的一个重要优点是它们可以根据需要进行定制，以适应其输入和输出的融合变换。</p><p>如果没有 Triton，对于没有特殊 GPU 编程经验的开发者来说，矩阵乘法内核的修改是非常困难的。</p><p></p><div class="img_box" id="id_imagebox_2" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_2" data-original="http://zkres2.myzaker.com/202107/61025cb78e9f09408232c6e6_1024.jpg" data-height="872" data-width="1430" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202107/61025cb78e9f09408232c6e6_1024.jpg" referrerpolicy="no-referrer"></div></div>Triton 背后的原理<p></p><p>Triton 的良好性能，来自于以 Triton-IR 为中心的模块化系统架构，这是一种基于 LLVM 的中间表示。</p><p>@triton.jit decorator 通过遍历提供 Python 函数的抽象语法树（AST），产生的 Triton-IR 使用通用 SSA 构建算法上的动态。</p><p>生成的 IR 代码随后由编译器后端进行简化、优化和自动并行化，然后转换为高质量的 LLVM-IR（最终转换为 PTX）。</p><p>研究人员发现，数据可以通过查看计算密集型块级操作（例如 tl.dot）的操作数自动存储到共享内存中，并使用标准活性分析技术进行分配 / 同步。</p><p></p><div class="img_box" id="id_imagebox_3" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_3" data-original="http://zkres2.myzaker.com/202107/61025cb78e9f09408232c6e7_1024.jpg" data-height="456" data-width="1322" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202107/61025cb78e9f09408232c6e7_1024.jpg" referrerpolicy="no-referrer"></div></div>另一方面，Triton 程序可以通过同时执行不同的内核实例跨 SM 进行高效和自动并行化，以及通过分析每个块级操作的迭代空间，并在不同的 SIMD 中进行充分分区将 SM 内单元并行化。<p></p><p></p><div class="img_box" id="id_imagebox_4" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_4" data-original="http://zkres1.myzaker.com/202107/61025cb78e9f09408232c6e8_1024.jpg" data-height="784" data-width="1920" src="https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202107/61025cb78e9f09408232c6e8_1024.jpg" referrerpolicy="no-referrer"></div></div>目前 Triton 仅适用于英伟达 GPU，但官方表示 AMD GPU 以及 CPU 的版本正在开发中。<p></p><p>开源地址：</p><p>https://github.com/openai/triton</p><p>论文：</p><p>https://dl.acm.org/doi/abs/10.1145/3315508.3329973</p><div id="recommend_bottom"></div><div id="article_bottom"></div>  
</div>
            