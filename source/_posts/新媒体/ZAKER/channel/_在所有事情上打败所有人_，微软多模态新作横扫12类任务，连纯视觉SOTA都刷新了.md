
---
title: '_在所有事情上打败所有人_，微软多模态新作横扫12类任务，连纯视觉SOTA都刷新了'
categories: 
 - 新媒体
 - ZAKER
 - channel
headimg: 'https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202208/630a38ab8e9f09231f39141e_1024.jpg'
author: ZAKER
comments: false
date: Sat, 27 Aug 2022 19:08:53 GMT
thumbnail: 'https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202208/630a38ab8e9f09231f39141e_1024.jpg'
---

<div>   
<p>仅靠 19 亿参数，只用公共数据集，在 12 个任务上狂刷 SOTA。</p><p>微软这篇多模态论文刚挂上 arXiv 不久，就在业内引发强烈关注。</p><p>有网友将之总结成<strong>" 在所有事情上打败了所有人 "</strong>。</p><p></p><div class="img_box" id="id_imagebox_0" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_0" data-original="http://zkres2.myzaker.com/202208/630a38ab8e9f09231f39141e_1024.jpg" data-height="188" data-width="1080" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202208/630a38ab8e9f09231f39141e_1024.jpg" referrerpolicy="no-referrer"></div></div>怎么回事？先来看这张雷达图：<p></p><p><strong>橙色内圈</strong>，是各大任务之前的 SOTA。</p><p><strong>紫色外圈</strong>，就是这篇 BEiT-3 的结果，不仅超越，而且是全面超越。</p><p></p><div class="img_box" id="id_imagebox_1" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_1" data-original="http://zkres2.myzaker.com/202208/630a38ab8e9f09231f39141f_1024.jpg" data-height="708" data-width="1080" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202208/630a38ab8e9f09231f39141f_1024.jpg" referrerpolicy="no-referrer"></div></div>具体一圈看下来，BEiT-3 这个多模态模型不光刷遍多模态任务，连右上角的<strong>纯视觉</strong>三大经典任务也都刷到 SOTA，简直是<strong>六边形战士</strong>。<p></p><p>知乎上一位同样做多模态研究的选手直呼<strong>" 杀死了比赛 "</strong>。</p><p></p><div class="img_box" id="id_imagebox_2" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_2" data-original="http://zkres1.myzaker.com/202208/630a38ab8e9f09231f391420_1024.jpg" data-height="364" data-width="1080" src="https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202208/630a38ab8e9f09231f391420_1024.jpg" referrerpolicy="no-referrer"></div></div>其实说起来，微软 BEiT 这个系列最开始做的是视觉自监督学习。<p></p><p>其核心思想与<strong>何恺明的 MAE</strong>一致，甚至比 MAE 提出的还早一段时间，不过当时性能惜败于 MAE。</p><p>如今在多模态方向上绕了一圈后，没想到能以方式横扫视觉与多模态榜单。</p><p>取得这种成果的，一般来说还不得是上百亿上千亿参数的大大大模型？</p><p>但 BEiT-3 总参数不过<strong>19 亿</strong>，甚至训练数据上也没什么秘密武器，全都用的<strong>公开资源</strong>。</p><p></p><div class="img_box" id="id_imagebox_3" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_3" data-original="http://zkres1.myzaker.com/202208/630a38ab8e9f09231f391421_1024.jpg" data-height="433" data-width="1080" src="https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202208/630a38ab8e9f09231f391421_1024.jpg" referrerpolicy="no-referrer"></div></div>那么，这一切是如何做到的？<p></p><p><b>把图像当成一种外语</b></p><p>最关键的一点，论文标题和摘要就已经指明：</p><p>把图像当成一种外语。</p><p>这样一来，文本数据是<strong>English</strong>，图像数据作者开了个小玩笑命名为<strong>Imglish</strong>，那么图文对数据就相当于<strong>平行语料</strong>。</p><p>那么多模态也好纯视觉也罢，都能用同一个预训练任务来处理。</p><p>在这个基础上，论文中把所做突破总结成一个词，<strong>大一统</strong>（Big Convergence）。</p><p></p><div class="img_box" id="id_imagebox_4" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_4" data-original="http://zkres2.myzaker.com/202208/630a38ab8e9f09231f391422_1024.jpg" data-height="199" data-width="1080" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202208/630a38ab8e9f09231f391422_1024.jpg" referrerpolicy="no-referrer"></div></div>首先，大一统表现在<strong>网络架构</strong>上。<p></p><p>通过统一多模态表示方式，对于不同任务可以共享一部分参数，采用 Multiway（多路）Transformer 架构作为骨干网络。</p><p>具体来说就是<strong>共享多头自注意力层</strong>，输出时再根据具体任务选择<strong>专用的 FFN 层</strong>。</p><p></p><div class="img_box" id="id_imagebox_5" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_5" data-original="http://zkres2.myzaker.com/202208/630a38ab8e9f09231f391423_1024.jpg" data-height="488" data-width="1080" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202208/630a38ab8e9f09231f391423_1024.jpg" referrerpolicy="no-referrer"></div></div>第二，大一统又表现在<strong>预训练方法</strong>上。<p></p><p>既然所有数据都能当成文本数据，那就可以全都按照 BERT 的方法，用掩码 - 预测来做预训练，称为 Masked Data Modeling。</p><p>与基于对比学习的训练方法相比，新方法可以选用更小的 Batch Size，又能额外降低显存消耗。</p><p></p><div class="img_box" id="id_imagebox_6" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_6" data-original="http://zkres2.myzaker.com/202208/630a38ab8e9f09231f391424_1024.jpg" data-height="125" data-width="1080" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202208/630a38ab8e9f09231f391424_1024.jpg" referrerpolicy="no-referrer"></div></div>第三，大一统还表现在<strong>规模效应</strong>上。<p></p><p>统一的预训练任务让模型参数扩大到 10 亿数量级后，对下游任务的<strong>泛化能力</strong>增强。</p><p></p><div class="img_box" id="id_imagebox_7" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_7" data-original="http://zkres1.myzaker.com/202208/630a38ab8e9f09231f391425_1024.jpg" data-height="444" data-width="1080" src="https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202208/630a38ab8e9f09231f391425_1024.jpg" referrerpolicy="no-referrer"></div></div>另外不同模态的数据集在此方法下也产生规模效应。<p></p><p>团队特意只用公开数据的条件下增加训练数据集规模，结果超越了一些使用高质量私有数据的模型。</p><p>BEiT-v 的训练数据来自 5 个公开数据集中的约<strong>500 万张图像和 2100 万图像 - 文本对</strong>；单模态数据则使用来自 ImageNet-21K 的<strong>1400 万张图像和 160GB 的文本语料库</strong>。</p><p>除此之外，在规模上也远小于其它的多模态预训练模型，例如 ALIGN（18 亿图文对）、CLIP（4 亿图文对）、SimVLM（18 亿图文对，800GB 文本）等。</p><p>所有这些优势叠加在一起，BEiT-3 就以更少的训练数据、更小的模型参数取得更好的性能。</p><p>在纯视觉任务（图像分类、目标检测、语义分割）以及多模态任务（视觉推理、视觉问答、图像描述、微调的跨模态检索、零样本跨模态检索）总共 8 类任务下超越各自之前的 SOTA。</p><p></p><div class="img_box" id="id_imagebox_8" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_8" data-original="http://zkres1.myzaker.com/202208/630a38ab8e9f09231f391426_1024.jpg" data-height="445" data-width="1080" src="https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202208/630a38ab8e9f09231f391426_1024.jpg" referrerpolicy="no-referrer"></div></div>BEiT-3 这篇论文很简短，不算参考文献只有 9 页。<p></p><p>但熟悉微软 BEiT 系列历史的话就会知道，这项研究取得成功的意义不仅在于其自身，也不仅是多模态学习的一项突破——</p><p>还给视觉大规模预训练这个兴起不久的领域，带来新的可能性。</p><p><b>BEiT 与 MAE，视觉自监督的路线之争</b></p><p>关于微软的 BEiT 系列，全称为<strong>B</strong>idirectional <strong>E</strong>ncoder representation from <strong>I</strong>mage <strong>T</strong>ransformers，比大家熟悉的语言模型<strong>BERT</strong>多了个 "Image"。</p><p>其主要思想就是借鉴 BERT，把掩码建模方法用到视觉任务上，做视觉的自监督学习，解决高质量标注数据难以获得的难题。</p><p>初代 BEiT 论文于去年 6 月发表，比同类工作何恺明的 MAE 还要早一些，也是 MAE 论文中的主要比较对象之一。</p><p><b>初代 BEiT，惜败 MAE</b></p><p></p><div class="img_box" id="id_imagebox_9" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_9" data-original="http://zkres1.myzaker.com/202208/630a38ab8e9f09231f391427_1024.jpg" data-height="380" data-width="1080" src="https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202208/630a38ab8e9f09231f391427_1024.jpg" referrerpolicy="no-referrer"></div></div>两项研究都是用 " 先掩码再预测 " 来做预训练任务，最大的区别在于 BEiT 会把视觉 token 离散化、最后模型预测的是<strong>token</strong>，而 MAE 则是直接预测<strong>原始像素</strong>。<p></p><p></p><div class="img_box" id="id_imagebox_10" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_10" data-original="http://zkres1.myzaker.com/202208/630a38ab8e9f09231f391428_1024.jpg" data-height="574" data-width="1080" src="https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202208/630a38ab8e9f09231f391428_1024.jpg" referrerpolicy="no-referrer"></div></div><strong>△</strong>初代 BEiT 的架构<p></p><p>在三大视觉任务上，MAE 比当时的 BEiT 略胜一筹。并且因方法更简单直接，MAE 运行起来也要快上不少（3.5 倍）。</p><p>为了证明在 MAE 中 token 化这一步并无必要，何恺明团队在论文中还特意做了消融试验。</p><p>结果表明，两种方法统计上并无显著差异，对于 MAE 来说预测原始像素就足够了。</p><p></p><div class="img_box" id="id_imagebox_11" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_11" data-original="http://zkres1.myzaker.com/202208/630a38ab8e9f09231f391429_1024.jpg" data-height="504" data-width="846" src="https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202208/630a38ab8e9f09231f391429_1024.jpg" referrerpolicy="no-referrer"></div></div>不过 BEiT 团队并没有放弃离散化 token 这个方法，而是沿着这个思路继续探索下去。<p></p><p><b>VL-BEiT，初探多模态</b></p><p>一年之后，团队发表了多模态模型 VL-BEiT，可以算作是现在这篇 BEiT-3 的雏形。</p><p></p><div class="img_box" id="id_imagebox_12" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_12" data-original="http://zkres2.myzaker.com/202208/630a38ab8e9f09231f39142a_1024.jpg" data-height="346" data-width="1080" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202208/630a38ab8e9f09231f39142a_1024.jpg" referrerpolicy="no-referrer"></div></div>VL-BEiT 已经用上了共享 Attenion 层、再对不同任务连接不同 FFN 层的架构。<p></p><p>这一思想其实来自同一团队更早之前一篇论文<strong>VLMo</strong>，对每个模态设置一个专家层的方法称为 MoME（Mixture-of-Modality-Experts）。</p><p></p><div class="img_box" id="id_imagebox_13" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_13" data-original="http://zkres1.myzaker.com/202208/630a38ab8e9f09231f39142b_1024.jpg" data-height="448" data-width="1078" src="https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202208/630a38ab8e9f09231f39142b_1024.jpg" referrerpolicy="no-referrer"></div></div>不过，VL-BEiT 在预训练任务上还比较复杂，会对文本数据和图像数据分别做掩码建模，至于多模态图文对数据也是分开处理的。<p></p><p></p><div class="img_box" id="id_imagebox_14" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_14" data-original="http://zkres1.myzaker.com/202208/630a38ab8e9f09231f39142c_1024.jpg" data-height="1157" data-width="1080" src="https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202208/630a38ab8e9f09231f39142c_1024.jpg" referrerpolicy="no-referrer"></div></div>最后结果，VL-BEiT 在多模态任务和纯视觉任务上表现都不错，但还不像现在的 BEiT-3 这样大杀四方。<p></p><p>不过别急，突破口很快就被找到。</p><p><b>BEiT v2，把 token 提升到语义级</b></p><p>BEiT-3 发表仅一周之前，微软与国科大团队合作发表了一篇 BEiT v2。</p><p>两者命名方式有细微差别，因为 BEiT v2 确实代表是 BEiT 的升级版。</p><p>而 BEiT-3 的 3 论文中虽未明说，但说的大概不是 " 第三代 "，而是另有所指（稍后揭秘）。</p><p></p><div class="img_box" id="id_imagebox_15" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_15" data-original="http://zkres1.myzaker.com/202208/630a38ab8e9f09231f39142d_1024.jpg" data-height="440" data-width="1080" src="https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202208/630a38ab8e9f09231f39142d_1024.jpg" referrerpolicy="no-referrer"></div>说回到 BEiT v2，这篇论文重新专注于纯视觉，在初代 BEiT 基础上提出了新的语义级 tokenizer。<p></p><p>具体来说，BEiT v2 引入了<strong>矢量量化</strong>（Vector-Quantized）和<strong>知识蒸馏</strong>（Knowledge Distillation）来训练 tokenizer。</p><p></p><div class="img_box" id="id_imagebox_16" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_16" data-original="http://zkres1.myzaker.com/202208/630a38ab8e9f09231f39142e_1024.jpg" data-height="498" data-width="1080" src="https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202208/630a38ab8e9f09231f39142e_1024.jpg" referrerpolicy="no-referrer"></div></div>同样是做离散化 token，新方法能重建知识蒸馏中教师模型的语义特征，大大提高 token 中携带的语义信息，从而提高模型性能。<p></p><p>接下来，教师模型用谁就很关键了。</p><p>在对比了 FAIR 的<strong>DINO</strong>模型和 OpenAI 的<strong>CLIP</strong>模型之后，团队发现还是 CLIP 更香。</p><p></p><div class="img_box" id="id_imagebox_17" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_17" data-original="http://zkres1.myzaker.com/202208/630a38ab8e9f09231f39142f_1024.jpg" data-height="468" data-width="1080" src="https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202208/630a38ab8e9f09231f39142f_1024.jpg" referrerpolicy="no-referrer"></div></div>最终结果上，BEiTv2 性能反超 MAE 和这段时间出现的其他方法，重回 SOTA。<p></p><p></p><div class="img_box" id="id_imagebox_18" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_18" data-original="http://zkres1.myzaker.com/202208/630a38ab8e9f09231f391430_1024.jpg" data-height="570" data-width="1080" src="https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202208/630a38ab8e9f09231f391430_1024.jpg" referrerpolicy="no-referrer"></div></div><b>BEiT-3，集大成者</b><p></p><p>了解了整个 BEiT 系列的发展历程，最后再来看一下 BEiT-3。</p><p>论文共同一作<strong>董力</strong>，点出了模型命名中<strong>"<strong>3"</strong></strong>的含义：</p><p></p><div class="img_box" id="id_imagebox_19" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_19" data-original="http://zkres2.myzaker.com/202208/630a38ab8e9f09231f391431_1024.jpg" data-height="253" data-width="1080" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202208/630a38ab8e9f09231f391431_1024.jpg" referrerpolicy="no-referrer"></div></div>多模态统一的预训练方式 + 共享 Attention 的多路 Transformer+ 扩大规模的大一统（Big Convergence）。<p></p><p>如此一来，BEiT-3 能在多模态任务和视觉任务中都取得 SOTA 也就不奇怪了。</p><p>这样一篇论文，自然吸引了行业内很多目光。</p><p>鲁汶大学一位教授认为，这代表微软在 AI 科研方面赶上谷歌 /DeepMind、Meta 和 OpenAI，" 重新坐上了牌桌 "。</p><p></p><div class="img_box" id="id_imagebox_20" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_20" data-original="http://zkres2.myzaker.com/202208/630a38ab8e9f09231f391432_1024.jpg" data-height="370" data-width="1020" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202208/630a38ab8e9f09231f391432_1024.jpg" referrerpolicy="no-referrer"></div></div>随着讨论热度升级，对论文更严格的审视目光也多了起来。<p></p><p>谷歌一位研究员指出，论文结果看起来简洁又令人印象深刻，就是这雷达图的坐标取值有点不太严谨。</p><p></p><div class="img_box" id="id_imagebox_21" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_21" data-original="http://zkres2.myzaker.com/202208/630a38ab8e9f09231f391433_1024.jpg" data-height="1316" data-width="1080" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202208/630a38ab8e9f09231f391433_1024.jpg" referrerpolicy="no-referrer"></div></div>知乎上也有网友提问，如果用了 CLIP 作为教师模型的话，那么来自 CLIP 高质量配对数据的贡献有多少，直接改改 CLIP 就用又会如何？<p></p><p></p><div class="img_box" id="id_imagebox_22" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_22" data-original="http://zkres2.myzaker.com/202208/630a38ab8e9f09231f391434_1024.jpg" data-height="694" data-width="1080" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202208/630a38ab8e9f09231f391434_1024.jpg" referrerpolicy="no-referrer"></div></div><b>作者团队</b><p></p><p>最后再来介绍一下作者团队，BEiT-3 相关研究论文的作者都来自微软。</p><p></p><div class="img_box" id="id_imagebox_23" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_23" data-original="http://zkres2.myzaker.com/202208/630a38ab8e9f09231f391435_1024.jpg" data-height="449" data-width="1080" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202208/630a38ab8e9f09231f391435_1024.jpg" referrerpolicy="no-referrer"></div></div>三位共同一作分别是 Wenhui Wang，Hangbo Bao（鲍航波）和 Li Dong（董力）。<p></p><p>其中，鲍航波和董力都是从初代 BEiT 就参与了研究，一直贯穿 VL-BEiT 和 BEiT v2 的发展，鲍航波更是 BEiT 和 VL-BEiT 论文的一作。另一位 Wenhui Wang 之前也曾参与过 VL-BEiT 的研究。</p><p>通讯作者是微软亚洲研究院 NLP 小组的 Partner 研究经理 Furu Wei（韦福如）。</p><p></p><div class="img_box" id="id_imagebox_24" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_24" data-original="http://zkres1.myzaker.com/202208/630a38ab8e9f09231f391436_1024.jpg" data-height="635" data-width="1080" src="https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202208/630a38ab8e9f09231f391436_1024.jpg" referrerpolicy="no-referrer"></div></div><p></p><div id="recommend_bottom"></div><div id="article_bottom"></div></div>  
</div>
            