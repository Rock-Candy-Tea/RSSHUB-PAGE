
---
title: '这个GAN没见过猪，却能把狗变成猪'
categories: 
 - 新媒体
 - ZAKER
 - channel
headimg: 'https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202108/610ffe4b8e9f094c92737ec4_1024.jpg'
author: ZAKER
comments: false
date: Sun, 08 Aug 2021 18:07:31 GMT
thumbnail: 'https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202108/610ffe4b8e9f094c92737ec4_1024.jpg'
---

<div>   
<p><strong>不用成千上万张目标图片训练</strong>，就能让 GAN 生成你想要的图片，有可能吗？</p><p></p><div class="img_box" id="id_imagebox_0" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_0" data-original="http://zkres1.myzaker.com/202108/610ffe4b8e9f094c92737ec4_1024.jpg" data-height="289" data-width="361" src="https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202108/610ffe4b8e9f094c92737ec4_1024.jpg" referrerpolicy="no-referrer"></div></div>还真有可能！<p></p><p>来自特拉维夫大学和英伟达的研究人员成功地<strong>盲训</strong>出领域自适应的图像生成模型——<strong>StyleGAN-NADA</strong>。</p><p>也就是只需用简单地一个或几个字描述，一张目标领域的图像也不需要，StyleGAN-NADA 就能在几分钟内训练出你想要的图片：</p><p>比如现在在几张狗狗的基础图片上输入 "Sketch"，<strong>不到 1 分钟</strong>，一张张草图风格狗的图片就出来了。 ( 视频没有声音可放心 " 食用 " ) </p><p>再比如在人像上给出文字 "Pixar"，就能生成<strong>皮克斯风格</strong>的图片：</p><p></p><div class="img_box" id="id_imagebox_1" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_1" data-original="http://zkres1.myzaker.com/202108/610ffe4b8e9f094c92737ec5_1024.jpg" data-height="270" data-width="1080" src="https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202108/610ffe4b8e9f094c92737ec5_1024.jpg" referrerpolicy="no-referrer"></div></div>各种人像风格都可以：<p></p><p></p><div class="img_box" id="id_imagebox_2" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_2" data-original="http://zkres1.myzaker.com/202108/610ffe4b8e9f094c92737ec6_1024.jpg" data-height="960" data-width="902" src="https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202108/610ffe4b8e9f094c92737ec6_1024.jpg" referrerpolicy="no-referrer"></div></div>甚至<strong>把狗变成猪</strong>也行：<p></p><p></p><div class="img_box" id="id_imagebox_3" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_3" data-original="http://zkres1.myzaker.com/202108/610ffe4b8e9f094c92737ec7_1024.jpg" data-height="432" data-width="1080" src="https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202108/610ffe4b8e9f094c92737ec7_1024.jpg" referrerpolicy="no-referrer"></div></div>问题来了，AI 不可能生成它完全没有见过的照片，但是又不给它参考照片，那怎么满足要求呢？<p></p><p><b>基于 CLIP</b></p><p>答案就是借助<strong>CLIP</strong>的语义能力。</p><p>CLIP 是 OpenAI 提出的根据文字生成图片的 DALL 模型的图像分类模块，可以根据文字描述给图片的匹配程度打分。</p><p>今年年初，就有人用 CLIP 做出了一个用 " 大白话 " 检索图片的功能，效果还挺惊艳的。</p><p><strong></strong></p><div class="img_box" id="id_imagebox_4" onclick><div class="content_img_div perview_img_div"><strong><img class="lazy opacity_0 " id="img_4" data-original="http://zkres2.myzaker.com/202108/610ffe4b8e9f094c92737ec8_1024.jpg" data-height="601" data-width="1080" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202108/610ffe4b8e9f094c92737ec8_1024.jpg" referrerpolicy="no-referrer"></strong></div></div><strong>△</strong>输入 "The word love written on the wall" 的搜索结果<p></p><p>总的来说，StyleGAN-NADA 的训练机制包含两个紧密相连的生成器<strong>Gfrozen 和 Gtrain</strong>，它俩都使用了<strong>StyleGAN2</strong>的体系结构，并共享同一个映射网络，因此也具有同一个隐空间（latent space）和隐码（latent code），所以它们在最开始生成的图像是一样的。</p><p></p><div class="img_box" id="id_imagebox_5" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_5" data-original="http://zkres2.myzaker.com/202108/610ffe4b8e9f094c92737ec9_1024.jpg" data-height="272" data-width="641" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202108/610ffe4b8e9f094c92737ec9_1024.jpg" referrerpolicy="no-referrer"></div></div>首先使用在单个源域（例如人脸、狗、教堂或汽车数据集）上预训练的模型权重<strong>初始化</strong>这两个生成器。<p></p><p>由于最终目标是生成一个风格不一样的图像，那就要更改其中一个成对生成器的域，同时保持另一个作为参考域。</p><p>具体的话就是 Gfrozen 的权重保持不变，而 Gtrain 的权重通过优化和迭代层冻结（iterative layer-freezing）方案进行修改。</p><p>而 Gtrain 的域在通过用户提供的文本方向进行更改（shift）的同时，会保持共享隐空间（latent space）。</p><p>具体怎么 " 更改 " 呢？</p><p>这就用到了一组<strong>基于 CLIP 的损失</strong>（loss）和 "<strong>分层冻结</strong><strong>"</strong>（layer-freezing）方案。</p><p>该方案可以自适应地确定在每次迭代训练中最相关的子层、并 " 冻结 " 其余层来提高训练稳定性保证效果。下面就详细介绍一下这两个方法。</p><p>基于 CLIP 的损失（loss）</p><p>StyleGAN-NADA 依靠预先训练的 CLIP 作目标域的唯一监督来源。为了有效地从 CLIP 中提取 " 知识 "，一共用了三种损失算法：</p><p>（1）负责确定在每次迭代中训练哪个子集层的<strong>全局目标损失</strong>（Global CLIP loss）；</p><p>（2）旨在保持多样性的<strong>局部定向损失</strong>（Directional CLIP loss）；</p><p>（3）以及防止图像生成不必要的语义伪影的<strong>嵌入范数损失</strong>（Embedding-norm Loss）。</p><p><strong></strong></p><div class="img_box" id="id_imagebox_6" onclick><div class="content_img_div perview_img_div"><strong><img class="lazy opacity_0 " id="img_6" data-original="http://zkres1.myzaker.com/202108/610ffe4b8e9f094c92737eca_1024.jpg" data-height="633" data-width="640" src="https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202108/610ffe4b8e9f094c92737eca_1024.jpg" referrerpolicy="no-referrer"></strong></div></div><strong>△</strong> 局部定向损失要求源 / 目标图像 / 文字的 CLIP-space 方向一致<p></p><p>" 分层冻结 "（layer-freezing）</p><p>此机制分为两阶段：</p><p>（1）<strong>选层阶段</strong>，保持所有网络权重不变并对一组隐码进行优化，然后选择变化最显著的一层（优化使用目标域文本描述驱动的全局 CLIP 损失进行）；</p><p>（2）<strong>优化阶段</strong>，" 解冻 " 选定层的权重，然后使用定向 CLIP 损失进行优化和更改。</p><p></p><div class="img_box" id="id_imagebox_7" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_7" data-original="http://zkres1.myzaker.com/202108/610ffe4b8e9f094c92737ecb_1024.jpg" data-height="484" data-width="672" src="https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202108/610ffe4b8e9f094c92737ecb_1024.jpg" referrerpolicy="no-referrer"></div></div><b>大多数训练只需几分钟就可完成</b><p></p><p>首先，该模型可以实现<strong>范围广泛</strong>的域外自适应，从纹理变化到大的形状修改，从现实到魔幻风格……甚至包括一些收集高质量数据成本很高的目标域。</p><p>其次，所有的这些图片的生成都只需给一个简单的文字描述，除了极端情况，大多数训练<strong>只需几分钟</strong>就能完成。</p><p>对于基于纹理的修改目标，该模型通常需要 300 次迭代，batch size 为 2，在一个 NVIDIA V100 GPU 上训练大约 3 分钟。在某些情况下（比如从 " 照片 " 到 " 草图 "），训练只需不到一分钟的时间。</p><p>然后，所有的实验用的就是这个完整当然模型，没有添加任何<strong>latent mapper</strong>。研究人员发现，对于纯粹是基于样式的图像生成，模型需要跨所有层进行训练，比如下面这种：</p><p></p><div class="img_box" id="id_imagebox_8" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_8" data-original="http://zkres1.myzaker.com/202108/610ffe4b8e9f094c92737ecc_1024.jpg" data-height="583" data-width="1080" src="https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202108/610ffe4b8e9f094c92737ecc_1024.jpg" referrerpolicy="no-referrer"></div></div>而对于较小的形状修改，则只需训练大约<strong>2/3</strong>数量的层数就能折中保持<strong>训练时间和效果</strong>：<p></p><p></p><div class="img_box" id="id_imagebox_9" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_9" data-original="http://zkres2.myzaker.com/202108/610ffe4b8e9f094c92737ecd_1024.jpg" data-height="611" data-width="1080" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202108/610ffe4b8e9f094c92737ecd_1024.jpg" referrerpolicy="no-referrer"></div></div>最后，将该模型与 StyleCLIP（结合了 StyleGAN 和 CLIP 的域内图像编辑模型）、以及只用了 Gfrozen 生成器的模型对比发现，只有 StyleGAN-NADA 可以实现目标。<p></p><p></p><div class="img_box" id="id_imagebox_10" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_10" data-original="http://zkres1.myzaker.com/202108/610ffe4b8e9f094c92737ece_1024.jpg" data-height="669" data-width="1080" src="https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202108/610ffe4b8e9f094c92737ece_1024.jpg" referrerpolicy="no-referrer"></div></div>再将零样本的 StyleGAN-NADA 与一些<strong>少样本</strong>的图像生成模型对比发现，别的都要么过拟合要么崩溃（MineGAN 更是只记住了训练集图像），只有 StyleGAN-NADA 在保持多样性的情况下成功生成（但它也有伪影出现）。<p></p><p></p><div class="img_box" id="id_imagebox_11" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_11" data-original="http://zkres2.myzaker.com/202108/610ffe4b8e9f094c92737ecf_1024.jpg" data-height="632" data-width="1080" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202108/610ffe4b8e9f094c92737ecf_1024.jpg" referrerpolicy="no-referrer"></div></div>下面是消融实验：<p></p><p><strong></strong></p><div class="img_box" id="id_imagebox_12" onclick><div class="content_img_div perview_img_div"><strong><img class="lazy opacity_0 " id="img_12" data-original="http://zkres2.myzaker.com/202108/610ffe4b8e9f094c92737ed0_1024.jpg" data-height="601" data-width="1080" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202108/610ffe4b8e9f094c92737ed0_1024.jpg" referrerpolicy="no-referrer"></strong></div></div><strong>△</strong> 通过训练 latent mapper 可以进一步提高生成质量<p></p><p>ps. 在论文的最后，研究人员表示：</p><p>由于这项技术，也许在不久的将来，这类图像生成的工作将不再受到训练数据的约束，而只取决于我们的创造力。</p><div id="recommend_bottom"></div><div id="article_bottom"></div>  
</div>
            