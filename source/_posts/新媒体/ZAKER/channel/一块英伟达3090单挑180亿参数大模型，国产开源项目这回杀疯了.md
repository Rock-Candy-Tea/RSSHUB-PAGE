
---
title: '一块英伟达3090单挑180亿参数大模型，国产开源项目这回杀疯了'
categories: 
 - 新媒体
 - ZAKER
 - channel
headimg: 'https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202205/62833fe48e9f09678f4fbf9d_1024.jpg'
author: ZAKER
comments: false
date: Mon, 16 May 2022 23:12:49 GMT
thumbnail: 'https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202205/62833fe48e9f09678f4fbf9d_1024.jpg'
---

<div>   
<p>什么？<strong>单块 GPU</strong>也能训练大模型了？</p><p>还是 20 系就能拿下的那种？？？</p><p>没开玩笑，事实已经摆在眼前：</p><p>RTX 2060 6GB 普通游戏本能训练<strong>15 亿</strong>参数模型；</p><p>RTX 3090 24GB 主机直接单挑<strong>180 亿</strong>参数大模型；</p><p>Tesla V100 32GB 连<strong>240 亿</strong>参数都能拿下。</p><p></p><div class="img_box" id="id_imagebox_0" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_0" data-original="http://zkres2.myzaker.com/202205/62833fe48e9f09678f4fbf9d_1024.jpg" data-height="608" data-width="1080" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202205/62833fe48e9f09678f4fbf9d_1024.jpg" referrerpolicy="no-referrer"></div></div>相比于 PyTorch 和业界主流的 DeepSpeed 方法，提升参数容量能达到 10 多倍。<p></p><p>而且这种方法完全开源，只需要几行代码就能搞定，修改量也非常少。</p><p></p><div class="img_box" id="id_imagebox_1" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_1" data-original="http://zkres1.myzaker.com/202205/62833fe48e9f09678f4fbf9e_1024.jpg" data-height="252" data-width="942" src="https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202205/62833fe48e9f09678f4fbf9e_1024.jpg" referrerpolicy="no-referrer"></div></div>这波操作真是直接腰斩大模型训练门槛啊，老黄岂不是要血亏。<p></p><p>那么，搞出如此大名堂的是何方大佬呢？</p><p>它就是<strong>国产开源项目 Colossal-AI</strong>。</p><p>自开源以来，曾多次霸榜 GitHub 热门第一。</p><p></p><div class="img_box" id="id_imagebox_2" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_2" data-original="http://zkres1.myzaker.com/202205/62833fe48e9f09678f4fbfa0_1024.jpg" data-height="641" data-width="977" src="https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202205/62833fe48e9f09678f4fbfa0_1024.jpg" referrerpolicy="no-referrer"></div></div><strong>△</strong>开源地址：https://github.com/hpcaitech/ColossalAI<p></p><p>主要做的事情就是<strong>加速各种大模型训练</strong>，GPT-2、GPT-3、ViT、BERT 等模型都能搞定。</p><p>比如能半小时左右预训练一遍 ViT-Base/32，2 天训完 15 亿参数 GPT 模型、5 天训完 83 亿参数 GPT 模型。</p><p>同时还能省 GPU。</p><p>比如训练 GPT-3 时使用的 GPU 资源，可以只是英伟达 Megatron-LM 的一半。</p><p>那么这一回，它又是如何让单块 GPU 训练百亿参数大模型的呢？</p><p>我们深扒了一下原理 ~</p><p><b>高效利用 GPU+CPU 异构内存</b></p><p>为什么单张消费级显卡很难训练 AI 大模型？</p><p><strong>显存有限</strong>，是最大的困难。</p><p>当今大模型风头正盛、效果又好，谁不想上手感受一把？</p><p>但动不动就 "CUDA out of memory"，着实让人遭不住。</p><p></p><div class="img_box" id="id_imagebox_3" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_3" data-original="http://zkres1.myzaker.com/202205/62833fe48e9f09678f4fbfa1_1024.jpg" data-height="432" data-width="440" src="https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202205/62833fe48e9f09678f4fbfa1_1024.jpg" referrerpolicy="no-referrer"></div></div>目前，业界主流方法是微软 DeepSpeed 提出的<strong>ZeRO</strong>（Zero Reduency Optimizer）。<p></p><p>它的主要原理是将模型切分，把模型内存平均分配到单个 GPU 上。</p><p>数据并行度越高，GPU 上的内存消耗越低。</p><p>这种方法在 CPU 和 GPU 内存之间仅使用<strong>静态划分模型数据</strong>，而且内存布局针对不同的训练配置也是恒定的。</p><p>由此会导致两方面问题。</p><p><strong>第一</strong>，当 GPU 或 CPU 内存不足以满足相应模型数据要求时，即使还有其他设备上有内存可用，系统还是会崩溃。</p><p><strong>第二</strong>，细粒度的张量在不同内存空间传输时，通信效率会很低；当可以将模型数据提前放置到目标计算设备上时，CPU-GPU 的通信量又是不必要的。</p><p>目前已经出现了不少 DeepSpeed 的魔改版本，提出使用<strong>电脑硬盘</strong>来动态存储模型，但是硬盘的读写速度明显<strong>低于</strong>内存和显存，训练速度依旧会被拖慢。</p><p></p><div class="img_box" id="id_imagebox_4" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_4" data-original="http://zkres2.myzaker.com/202205/62833fe48e9f09678f4fbfa2_1024.jpg" data-height="421" data-width="1080" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202205/62833fe48e9f09678f4fbfa2_1024.jpg" referrerpolicy="no-referrer"></div></div>针对这些问题，Colossal-AI 采用的解决思路是高效利用<strong>GPU+CPU 的异构内存</strong>。<p></p><p>具体来看，是利用深度学习网络训练过程中<strong>不断迭代</strong>的特性，按照迭代次数将整个训练过程分为<strong>预热</strong>和<strong>正式</strong>两个阶段。</p><p>预热阶段，监测采集到非模型数据内存信息；</p><p>正式阶段，根据采集到的信息，预留出下一个算子在计算设备上所需的峰值内存，移动出一些 GPU 模型张量到 CPU 内存。</p><p>大概逻辑如下所示：</p><p></p><div class="img_box" id="id_imagebox_5" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_5" data-original="http://zkres2.myzaker.com/202205/62833fe48e9f09678f4fbfa3_1024.jpg" data-height="394" data-width="1080" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202205/62833fe48e9f09678f4fbfa3_1024.jpg" referrerpolicy="no-referrer"></div></div>这里稍微展开说明下，模型数据由参数、梯度和优化器状态组成，它们的足迹和模型结构定义有关。<p></p><p>非模型数据由 operator 生成的中间张量组成，会根据训练任务的配置（如批次大小）动态变化。</p><p>它俩常干的事呢，就是抢 GPU 显存。</p><p></p><div class="img_box" id="id_imagebox_6" onclick><div class="content_img_div"><img class="lazy opacity_0 zaker_gif_cache" id="img_6" data-original="http://zkres2.myzaker.com/202205/62833fe48e9f09678f4fbfa4_1024.jpg" data-gif-url="http://zkres2.myzaker.com/202205/62833fe48e9f09678f4fbfa4_raw.gif" data-height="172" data-width="224" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202205/62833fe48e9f09678f4fbfa4_1024.jpg" referrerpolicy="no-referrer"></div></div>所以，就需要在 GPU 显存不够时 CPU 能来帮忙，与此同时还要避免其他情况下内存浪费。<p></p><p>Colossal-AI 高效利用 GPU+CPU 的异构内存，就是这样的逻辑。</p><p>而以上过程中，获取非模型数据的内存使用量其实<strong>非常难</strong>。</p><p>因为非模型数据的生存周期并不归用户管理，现有的深度学习框架没有暴露非模型数据的追踪接口给用户。其次，CUDA context 等非框架开销也需要统计。</p><p>在这里 Colossal-AI 的解决思路是，在预热阶段用<strong>采样</strong>的方式，获得非模型数据对 CPU 和 GPU 的内存的使用情况。</p><p>简单来说，这是道加减法运算：</p><p><strong>非数据模型使用 ＝ 两个统计时刻之间系统最大内存使用 — 模型数据内存使用</strong></p><p>已知，模型数据内存使用可以通过查询管理器得知。</p><p>具体来看就是下面酱婶的：</p><p></p><div class="img_box" id="id_imagebox_7" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_7" data-original="http://zkres2.myzaker.com/202205/62833fe48e9f09678f4fbfa5_1024.jpg" data-height="502" data-width="1080" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202205/62833fe48e9f09678f4fbfa5_1024.jpg" referrerpolicy="no-referrer"></div></div>所有模型数据张量交给内存管理器管理，每个张量标记一个状态信息，包括 HOLD、COMPUTE、FREE 等。<p></p><p>然后，根据动态查询到的内存使用情况，不断动态转换张量状态、调整张量位置，更高效利用 GPU 显存和 CPU 内存。</p><p>在硬件非常有限的情况下，最大化模型容量和平衡训练速度。这对于 AI 普及化、低成本微调大模型下游任务等，都具有深远意义。</p><p>而且最最最关键的是——加内存条可比买高端显卡<strong>划 算 多 了</strong>。</p><p></p><div class="img_box" id="id_imagebox_8" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_8" data-original="http://zkres2.myzaker.com/202205/62833fe48e9f09678f4fbfa6_1024.jpg" data-height="245" data-width="267" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202205/62833fe48e9f09678f4fbfa6_1024.jpg" referrerpolicy="no-referrer"></div></div>前不久，Colossal-AI 还成功复现了谷歌的最新研究成果 PaLM ( Pathways Language Model ) ，表现同样非常奈斯，而微软 DeepSpeed 目前还不支持 PaLM 模型。<p></p><p></p><div class="img_box" id="id_imagebox_9" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_9" data-original="http://zkres1.myzaker.com/202205/62833fe48e9f09678f4fbfa7_1024.jpg" data-height="610" data-width="1080" src="https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202205/62833fe48e9f09678f4fbfa7_1024.jpg" referrerpolicy="no-referrer"></div></div>Colossal-AI 还能做什么？<p></p><p><b></b></p><p>前面也提到，Colossal-AI 能挑战的任务非常多，比如加速训练、节省 GPU 资源。</p><p>那么它是如何做到的呢？</p><p>简单来说，Colossal-AI 就是一个整合了多种并行方法的系统，提供的功能包括多维并行、大规模优化器、自适应任务调度、消除冗余内存等。</p><p></p><div class="img_box" id="id_imagebox_10" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_10" data-original="http://zkres2.myzaker.com/202205/62833fe48e9f09678f4fbfa8_1024.jpg" data-height="456" data-width="866" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202205/62833fe48e9f09678f4fbfa8_1024.jpg" referrerpolicy="no-referrer"></div></div>目前，基于 Colossal-AI 的加速方案 FastFold，能够将蛋白质结构预测模型 AlphaFold 的训练时间，从原本的 11 天，减少到只需<strong>67 小时</strong>。<p></p><p>而且总成本更低，在长序列推理任务中，也能实现 9~11.6 倍的速度提升。</p><p>这一方案成功超越谷歌和哥伦比亚大学的方法。</p><p></p><div class="img_box" id="id_imagebox_11" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_11" data-original="http://zkres2.myzaker.com/202205/62833fe48e9f09678f4fbfa9_1024.jpg" data-height="229" data-width="977" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202205/62833fe48e9f09678f4fbfa9_1024.jpg" referrerpolicy="no-referrer"></div></div>此外，Colossal-AI 还能只用一半 GPU 数量训练 GPT-3。<p></p><p>相比英伟达方案，Colossal-AI 仅需一半的计算资源，即可启动训练；若使用相同计算资源，则能提速 11%，可降低 GPT-3 训练成本超百万美元。</p><p></p><div class="img_box" id="id_imagebox_12" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_12" data-original="http://zkres1.myzaker.com/202205/62833fe48e9f09678f4fbfaa_1024.jpg" data-height="402" data-width="977" src="https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202205/62833fe48e9f09678f4fbfaa_1024.jpg" referrerpolicy="no-referrer"></div></div>与此同时，Colossal-AI 也非常注重开源社区建设，提供中文教程、开放用户社群论坛，根据大家的需求反馈不断更新迭代。<p></p><p>比如之前有读者留言说，Colossal-AI 要是能在普通消费级显卡上跑就好了。</p><p></p><div class="img_box" id="id_imagebox_13" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_13" data-original="http://zkres2.myzaker.com/202205/62833fe48e9f09678f4fbfab_1024.jpg" data-height="192" data-width="832" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202205/62833fe48e9f09678f4fbfab_1024.jpg" referrerpolicy="no-referrer"></div></div>这不，几个月后，已经安排好了 ~<p></p><p><b>背后团队：LAMB 优化器作者尤洋领衔</b></p><p>看到这里，是不是觉得 Colossal-AI 确实值得标星关注一发？</p><p>实际上，这一国产项目背后的研发团队来头不小。</p><p>领衔者，正是 LAMB 优化器的提出者尤洋。</p><p></p><div class="img_box" id="id_imagebox_14" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_14" data-original="http://zkres2.myzaker.com/202205/62833fe48e9f09678f4fbfac_1024.jpg" data-height="514" data-width="772" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202205/62833fe48e9f09678f4fbfac_1024.jpg" referrerpolicy="no-referrer"></div></div>他曾以第一名的成绩保送清华计算机系硕士研究生，后赴加州大学伯克利分校攻读 CS 博士学位。<p></p><p>拿过 IPDP/ICPP 最佳论文、ACM/IEEE George Michael HPC Fellowship、福布斯 30 岁以下精英（亚洲 2021）、IEEE-CS 超算杰出新人奖、UC 伯克利 EECS Lotfi A. Zadeh 优秀毕业生奖。</p><p>在谷歌实习期间，凭借 LAMB 方法，尤洋曾打破 BERT 预训练世界纪录。</p><p>据英伟达官方 GitHub 显示，LAMB 比 Adam 优化器快出整整 72 倍。微软的 DeepSpeed 也采用了 LAMB 方法。</p><p>2021 年，尤洋回国创办<strong>潞晨科技</strong>——一家主营业务为分布式软件系统、大规模人工智能平台以及企业级云计算解决方案的 AI 初创公司。</p><p>团队的核心成员均来自美国加州大学伯克利分校、哈佛大学、斯坦福大学、芝加哥大学、清华大学、北京大学、新加坡国立大学、新加坡南洋理工大学等国内外知名高校；拥有 Google Brain、IBM、Intel、 Microsoft、NVIDIA 等知名厂商工作经历。</p><p>公司成立即获得创新工场、真格基金等多家顶尖 VC 机构种子轮投资。</p><p></p><div class="img_box" id="id_imagebox_15" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_15" data-original="http://zkres1.myzaker.com/202205/62833fe48e9f09678f4fbfad_1024.jpg" data-height="650" data-width="977" src="https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202205/62833fe48e9f09678f4fbfad_1024.jpg" referrerpolicy="no-referrer"></div></div>潞晨 CSO Prof. James Demmel 为加州大学伯克利分校杰出教授、ACM/IEEE Fellow，同时还是美国科学院、工程院、艺术与科学院三院院士。<p></p><div id="recommend_bottom"></div><div id="article_bottom"></div>  
</div>
            