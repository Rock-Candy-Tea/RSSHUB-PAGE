
---
title: '买不起手办就用AI渲染一个！用网上随便搜的图就能合成，已有网友开炒游戏NFT'
categories: 
 - 新媒体
 - ZAKER
 - channel
headimg: 'https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202202/62125bd4b15ec078475c6e64_1024.jpg'
author: ZAKER
comments: false
date: Sun, 20 Feb 2022 17:22:32 GMT
thumbnail: 'https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202202/62125bd4b15ec078475c6e64_1024.jpg'
---

<div>   
<p>渲染一个精细到头发和皮肤褶皱的龙珠 3D 手办，有多复杂？</p><p></p><div class="img_box" id="id_imagebox_0" onclick><div class="content_img_div"><img class="lazy opacity_0 zaker_gif_cache" id="img_0" data-original="http://zkres2.myzaker.com/202202/62125bd4b15ec078475c6e64_1024.jpg" data-gif-url="http://zkres2.myzaker.com/202202/62125bd4b15ec078475c6e64_raw.gif" data-height="472" data-width="470" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202202/62125bd4b15ec078475c6e64_1024.jpg" referrerpolicy="no-referrer"></div></div>对于经典模型 NeRF 来说，至少需要同一个相机从特定距离拍摄的<strong>100 张</strong>手办照片。<p></p><p>但现在，一个新 AI 模型只需要<strong>40 张</strong>来源不限的网络图片，就能把整个手办渲染出来！</p><p></p><div class="img_box" id="id_imagebox_1" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_1" data-original="http://zkres2.myzaker.com/202202/62125bd4b15ec078475c6e65_1024.jpg" data-height="242" data-width="1080" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202202/62125bd4b15ec078475c6e65_1024.jpg" referrerpolicy="no-referrer"></div></div>这些照片的拍摄角度、远近和亮暗都没有要求，还原出来的图片却能做到清晰无伪影：<p></p><p></p><div class="img_box" id="id_imagebox_2" onclick><div class="content_img_div"><img class="lazy opacity_0 zaker_gif_cache" id="img_2" data-original="http://zkres2.myzaker.com/202202/62125bd4b15ec078475c6e66_1024.jpg" data-gif-url="http://zkres2.myzaker.com/202202/62125bd4b15ec078475c6e66_raw.gif" data-height="202" data-width="480" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202202/62125bd4b15ec078475c6e66_1024.jpg" referrerpolicy="no-referrer"></div></div>甚至还能预估<strong>材质</strong>，并从任意角度重新打光：<p></p><p></p><div class="img_box" id="id_imagebox_3" onclick><div class="content_img_div"><img class="lazy opacity_0 zaker_gif_cache" id="img_3" data-original="http://zkres1.myzaker.com/202202/62125bd4b15ec078475c6e67_1024.jpg" data-gif-url="http://zkres1.myzaker.com/202202/62125bd4b15ec078475c6e67_raw.gif" data-height="180" data-width="480" src="https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202202/62125bd4b15ec078475c6e67_1024.jpg" referrerpolicy="no-referrer"></div></div>这个 AI 模型名叫<strong>NeROIC</strong>，是南加州大学和 Snap 团队玩出来的新花样。<p></p><p></p><div class="img_box" id="id_imagebox_4" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_4" data-original="http://zkres1.myzaker.com/202202/62125bd4b15ec078475c6e68_1024.jpg" data-height="274" data-width="1080" src="https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202202/62125bd4b15ec078475c6e68_1024.jpg" referrerpolicy="no-referrer"></div></div>有网友见状狂喜：<p></p><p>不同角度的照片就能渲染 3D 模型，快进到只用照片来拍电影……</p><p></p><div class="img_box" id="id_imagebox_5" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_5" data-original="http://zkres1.myzaker.com/202202/62125bd4b15ec078475c6e69_1024.jpg" data-height="460" data-width="1080" src="https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202202/62125bd4b15ec078475c6e69_1024.jpg" referrerpolicy="no-referrer"></div></div>还有网友借机炒了波<strong>NF</strong><strong>T</strong>（手动狗头）<p></p><p></p><div class="img_box" id="id_imagebox_6" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_6" data-original="http://zkres2.myzaker.com/202202/62125bd4b15ec078475c6e6a_1024.jpg" data-height="250" data-width="754" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202202/62125bd4b15ec078475c6e6a_1024.jpg" referrerpolicy="no-referrer"></div></div>所以，NeROIC 究竟是如何仅凭任意 2D 输入，就获取到物体的 3D 形状和性质的呢？<p></p><p><b>基于 NeRF 改进，可预测材料光照</b></p><p>介绍这个模型之前，需要先简单回顾一下 NeRF。</p><p>NeRF 提出了一种名叫神经辐射场（neural radiance field）的方法，利用 5D 向量函数来表示连续场景，其中 5 个参数分别用来表示空间点的坐标位置（x,y,z）和视角方向（θ , φ）。</p><p></p><div class="img_box" id="id_imagebox_7" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_7" data-original="http://zkres1.myzaker.com/202202/62125bd4b15ec078475c6e6b_1024.jpg" data-height="277" data-width="1080" src="https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202202/62125bd4b15ec078475c6e6b_1024.jpg" referrerpolicy="no-referrer"></div></div>然而，NeRF 却存在一些问题：<p></p><p>对输入图片的要求较高，必须是同一场景下拍摄的物体照片；</p><p>无法预测物体的材料属性，因此无法改变渲染的光照条件。</p><p>这次的<strong>NeROIC</strong>，就针对这两方面进行了优化：</p><p><strong>输入图片的场景不限</strong>，可以是物体的任意背景照片，甚至是网络图片；</p><p><strong>可以预测材料属性</strong>，在渲染时可以改变物体表面光照效果（可以打光）。</p><p>它主要由 2 个网络构成，包括深度提取网络（a）和渲染网络（c）。</p><p></p><div class="img_box" id="id_imagebox_8" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_8" data-original="http://zkres2.myzaker.com/202202/62125bd4b15ec078475c6e6c_1024.jpg" data-height="306" data-width="1080" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202202/62125bd4b15ec078475c6e6c_1024.jpg" referrerpolicy="no-referrer"></div></div>首先是<strong>深度提取网络</strong>，用于提取物体的各种参数。<p></p><p>为了做到输入场景不限，需要先让 AI 学会从不同背景中抠图，但由于 AI 对相机的位置估计得不准确，抠出来的图片总是存在下面这样的伪影（图左）：</p><p></p><div class="img_box" id="id_imagebox_9" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_9" data-original="http://zkres1.myzaker.com/202202/62125bd4b15ec078475c6e6d_1024.jpg" data-height="580" data-width="466" src="https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202202/62125bd4b15ec078475c6e6d_1024.jpg" referrerpolicy="no-referrer"></div></div>因此，深度提取网络引入了<strong>相机参数</strong>，让 AI 学习如何估计相机的位置，也就是估算图片中的网友是从哪个角度拍摄、距离有多远，抠出来的图片接近真实效果（GT）：<p></p><p></p><div class="img_box" id="id_imagebox_10" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_10" data-original="http://zkres2.myzaker.com/202202/62125bd4b15ec078475c6e6e_1024.jpg" data-height="480" data-width="650" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202202/62125bd4b15ec078475c6e6e_1024.jpg" referrerpolicy="no-referrer"></div></div>同时，设计了一种估计物体表面<strong>法线</strong>的新算法，在保留关键细节的同时，也消除了几何噪声的影响（法线即模型表面的纹路，随光线条件变化发生变化，从而影响光照渲染效果）：<p></p><p></p><div class="img_box" id="id_imagebox_11" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_11" data-original="http://zkres1.myzaker.com/202202/62125bd4b15ec078475c6e6f_1024.jpg" data-height="710" data-width="756" src="https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202202/62125bd4b15ec078475c6e6f_1024.jpg" referrerpolicy="no-referrer"></div></div>最后是<strong>渲染网络</strong>，用提取的参数来渲染出 3D 物体的效果<strong>。</strong><p></p><p>具体来说，论文提出了一种将颜色预测、神经网络与参数模型结合的方法，用于计算颜色、预测最终法线等。</p><p>其中，NeROIC 的实现框架用<strong>PyTorch</strong>搭建，训练时用了 4 张英伟达的 Tesla V100 显卡。</p><p>训练时，深度提取网络需要跑 6~13 小时，渲染网络则跑 2~4 小时。</p><p><b>用网络图片就能渲染 3D 模型</b></p><p>至于训练 NeROIC 采用的<strong>数据集</strong>，则主要有三部分：</p><p>来源于互联网（部分商品来源于网购平台，即亚马逊和淘宝）、NeRD、以及作者自己拍摄的（牛奶、电视、模型）图像，平均每个物体收集<strong>40 张</strong>照片。</p><p>那么，这样的模型效果究竟如何呢？</p><p>论文先是将 NeROIC 与 NeRF 进行了对比。</p><p>从直观效果来看，无论是物体<strong>渲染细节</strong>还是<strong>清晰度</strong>，NeROIC 都要比 NeRF 更好。</p><p></p><div class="img_box" id="id_imagebox_12" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_12" data-original="http://zkres2.myzaker.com/202202/62125bd4b15ec078475c6e70_1024.jpg" data-height="594" data-width="614" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202202/62125bd4b15ec078475c6e70_1024.jpg" referrerpolicy="no-referrer"></div></div>具体到峰值信噪比（PSNR）和结构相似性（SSIM）来看，深度提取网络的 " 抠图 " 技术都挺不错，相较 NeRF 做得更好：<p></p><p></p><div class="img_box" id="id_imagebox_13" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_13" data-original="http://zkres1.myzaker.com/202202/62125bd4b15ec078475c6e71_1024.jpg" data-height="284" data-width="768" src="https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202202/62125bd4b15ec078475c6e71_1024.jpg" referrerpolicy="no-referrer"></div></div>同时，论文也在更多场景中测试了渲染模型的效果，事实证明不会出现伪影等情况：<p></p><p></p><div class="img_box" id="id_imagebox_14" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_14" data-original="http://zkres1.myzaker.com/202202/62125bd4b15ec078475c6e72_1024.jpg" data-height="664" data-width="1080" src="https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202202/62125bd4b15ec078475c6e72_1024.jpg" referrerpolicy="no-referrer"></div></div>还能产生新角度，而且重新打光的效果也不错，例如这是在<strong>室外场景</strong>：<p></p><p></p><div class="img_box" id="id_imagebox_15" onclick><div class="content_img_div"><img class="lazy opacity_0 zaker_gif_cache" id="img_15" data-original="http://zkres2.myzaker.com/202202/62125bd4b15ec078475c6e73_1024.jpg" data-gif-url="http://zkres2.myzaker.com/202202/62125bd4b15ec078475c6e73_raw.gif" data-height="125" data-width="480" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202202/62125bd4b15ec078475c6e73_1024.jpg" referrerpolicy="no-referrer"></div></div><strong>室内场景</strong>的打光又是另一种效果：<p></p><p></p><div class="img_box" id="id_imagebox_16" onclick><div class="content_img_div"><img class="lazy opacity_0 zaker_gif_cache" id="img_16" data-original="http://zkres1.myzaker.com/202202/62125bd4b15ec078475c6e74_1024.jpg" data-gif-url="http://zkres1.myzaker.com/202202/62125bd4b15ec078475c6e74_raw.gif" data-height="125" data-width="480" src="https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202202/62125bd4b15ec078475c6e74_1024.jpg" referrerpolicy="no-referrer"></div></div>作者们还尝试将照片数量<strong>减少到 20 张甚至 10 张</strong>，对 NeRF 和 NeROIC 进行训练。<p></p><p>结果显示，即使是数据集不足的情况下，NeROIC 的效果依旧比 NeRF 更好。</p><p></p><div class="img_box" id="id_imagebox_17" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_17" data-original="http://zkres2.myzaker.com/202202/62125bd4b15ec078475c6e75_1024.jpg" data-height="169" data-width="1080" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202202/62125bd4b15ec078475c6e75_1024.jpg" referrerpolicy="no-referrer"></div></div>不过也有网友表示，作者没给出玻璃或是半透明材质的渲染效果：<p></p><p></p><div class="img_box" id="id_imagebox_18" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_18" data-original="http://zkres2.myzaker.com/202202/62125bd4b15ec078475c6e76_1024.jpg" data-height="190" data-width="1080" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202202/62125bd4b15ec078475c6e76_1024.jpg" referrerpolicy="no-referrer"></div></div>对 AI 来说，重建透明或半透明物体确实也确实是比较复杂的任务，可以等代码出来后尝试一下效果。<p></p><p>据作者表示，代码目前还在准备中。网友调侃：" 可能中顶会、或者在演讲之后就会放出 "。</p><p></p><div class="img_box" id="id_imagebox_19" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_19" data-original="http://zkres2.myzaker.com/202202/62125bd4b15ec078475c6e77_1024.jpg" data-height="321" data-width="1080" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202202/62125bd4b15ec078475c6e77_1024.jpg" referrerpolicy="no-referrer"></div></div><b>一作清华校友</b><p></p><p></p><div class="img_box" id="id_imagebox_20" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_20" data-original="http://zkres2.myzaker.com/202202/62125bd4b15ec078475c6e78_1024.jpg" data-height="532" data-width="532" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202202/62125bd4b15ec078475c6e78_1024.jpg" referrerpolicy="no-referrer"></div></div>论文一作<strong>匡正非</strong>，目前在南加州大学（University of Southern California）读博，导师是计算机图形学领域知名华人教授<strong>黎颢</strong>。<p></p><p>他本科毕业于清华计算机系，曾经在胡事民教授的计图团队中担任助理研究员。</p><p>这篇文章是他在 Snap 公司实习期间做出来的，其余作者全部来自 Snap 团队。</p><p></p><div class="img_box" id="id_imagebox_21" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_21" data-original="http://zkres2.myzaker.com/202202/62125bd4b15ec078475c6e79_1024.jpg" data-height="192" data-width="1080" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202202/62125bd4b15ec078475c6e79_1024.jpg" referrerpolicy="no-referrer"></div></div>以后或许只需要几张网友 " 卖家秀 "，就真能在家搞 VR 云试用了。<p></p><p></p><div class="img_box" id="id_imagebox_22" onclick><div class="content_img_div"><img class="lazy opacity_0 zaker_gif_cache" id="img_22" data-original="http://zkres1.myzaker.com/202202/62125bd4b15ec078475c6e7a_1024.jpg" data-gif-url="http://zkres1.myzaker.com/202202/62125bd4b15ec078475c6e7a_raw.gif" data-height="350" data-width="480" src="https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202202/62125bd4b15ec078475c6e7a_1024.jpg" referrerpolicy="no-referrer"></div></div><p></p><div id="recommend_bottom"></div><div id="article_bottom"></div>  
</div>
            