
---
title: 'Diffusion Model一发力，GAN就过时了？？？'
categories: 
 - 新媒体
 - ZAKER
 - channel
headimg: 'https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202208/6300efc88e9f0949472b275b_1024.jpg'
author: ZAKER
comments: false
date: Sat, 20 Aug 2022 21:10:07 GMT
thumbnail: 'https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202208/6300efc88e9f0949472b275b_1024.jpg'
---

<div>   
<p>曾经大红大紫的<strong>GAN 已过时</strong>。</p><p>马里兰大学副教授 Tom Goldstein 最近发表的一个推文，可谓是一石激起千层浪。</p><p>就连科技圈的大佬们也纷纷前来关注：</p><p></p><div class="img_box" id="id_imagebox_0" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_0" data-original="http://zkres1.myzaker.com/202208/6300efc88e9f0949472b275b_1024.jpg" data-height="1132" data-width="826" src="https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202208/6300efc88e9f0949472b275b_1024.jpg" referrerpolicy="no-referrer"></div></div>话题里 " 剑指 " 的关键词则是 Diffusion Model，用 Tom 的话来说就是：<p></p><p>在 2021 年，它甚至可以说是<strong>闻所未闻</strong>。</p><p>但其实这个算法并不陌生，因为它正是 AI 作画神器<strong>DALL · E</strong>的核心。</p><p>而且 DALL · E 的作者打一开始就 " 没看上 "GAN，直接将其放弃。</p><p>无独有偶，同样的话题在国内也引发了不小的讨论：</p><p></p><div class="img_box" id="id_imagebox_1" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_1" data-original="http://zkres2.myzaker.com/202208/6300efc88e9f0949472b275c_1024.jpg" data-height="646" data-width="1080" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202208/6300efc88e9f0949472b275c_1024.jpg" referrerpolicy="no-referrer"></div></div>那么图像生成领域的这波 " 后浪推前浪 "，究竟是为何？<p></p><p>咱们这就来盘一盘。</p><p><b>什么是 Diffusion Model？</b></p><p>Diffusion Model 这次被拉进聚光灯之下，不得不归功于各类 "AI 一句话作图 " 神器的火爆。</p><p>例如 OpenAI 家的 DALL · E 2：</p><p></p><div class="img_box" id="id_imagebox_2" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_2" data-original="http://zkres2.myzaker.com/202208/6300efc88e9f0949472b275d_1024.jpg" data-height="720" data-width="1080" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202208/6300efc88e9f0949472b275d_1024.jpg" referrerpolicy="no-referrer"></div></div>谷歌家的 Imagen：<p></p><p></p><div class="img_box" id="id_imagebox_3" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_3" data-original="http://zkres2.myzaker.com/202208/6300efc88e9f0949472b275e_1024.jpg" data-height="750" data-width="750" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202208/6300efc88e9f0949472b275e_1024.jpg" referrerpolicy="no-referrer"></div></div>不难看出，这些近期大流行的图像生成神器，不论是真实程度亦或是想象、理解能力，都是比较符合人类的预期。<p></p><p>因此它们也成为了这届网友们把玩的 " 新宠 "（当年 GAN 出道的时候也是被玩坏了）。</p><p>而如此能力背后的关键，便是 Diffusion Model。</p><p>它的研究最早可以追溯到 2015 年，当时，斯坦福和伯克利的研究人员发布了一篇名为 Deep Unsupervised Learning using Nonequilibrium Thermodynamics 的论文：</p><p></p><div class="img_box" id="id_imagebox_4" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_4" data-original="http://zkres1.myzaker.com/202208/6300efc88e9f0949472b275f_1024.jpg" data-height="478" data-width="1080" src="https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202208/6300efc88e9f0949472b275f_1024.jpg" referrerpolicy="no-referrer"></div></div>但这篇研究和目前的 Diffusion Model 非常不一样；而真正使其发挥作用的研究是 2020 年，一项名为 Denoising Diffusion Probabilistic Models 的研究：<p></p><p></p><div class="img_box" id="id_imagebox_5" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_5" data-original="http://zkres2.myzaker.com/202208/6300efc88e9f0949472b2760_1024.jpg" data-height="327" data-width="1080" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202208/6300efc88e9f0949472b2760_1024.jpg" referrerpolicy="no-referrer"></div></div>我们可以先来看一下各类生成模型之间的对比：<p></p><p></p><div class="img_box" id="id_imagebox_6" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_6" data-original="http://zkres1.myzaker.com/202208/6300efc88e9f0949472b2761_1024.jpg" data-height="747" data-width="1080" src="https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202208/6300efc88e9f0949472b2761_1024.jpg" referrerpolicy="no-referrer"></div></div>不难看出，Diffusion Model 和其它模型的不同点在于，它的 latent code ( z ) 和原图是同尺寸大小的。<p></p><p>若是简单来概括 Diffusion Model，就是存在一系列高斯噪声（T 轮），将输入图片 x0 变为纯高斯噪声 xT。</p><p>再细分来看，Diffusion Model 首先包含一个前向过程（Forward diffusion process）。</p><p>这个过程的目的，就是往图片上添加噪声；但在这一步中还无法实现图片生成。</p><p></p><div class="img_box" id="id_imagebox_7" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_7" data-original="http://zkres1.myzaker.com/202208/6300efc88e9f0949472b2762_1024.jpg" data-height="380" data-width="1080" src="https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202208/6300efc88e9f0949472b2762_1024.jpg" referrerpolicy="no-referrer"></div></div>其次是一个逆向过程（Reverse diffusion process），这个过程可以理解为 Diffusion 的去噪推断过程。<p></p><p></p><div class="img_box" id="id_imagebox_8" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_8" data-original="http://zkres1.myzaker.com/202208/6300efc88e9f0949472b2763_1024.jpg" data-height="798" data-width="1080" src="https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202208/6300efc88e9f0949472b2763_1024.jpg" referrerpolicy="no-referrer"></div></div>最后在训练阶段，则是通过对真实数据分布下，最大化模型预测分布的对数似然。<p></p><p>上述的过程是基于 DDPM 这项研究展开。</p><p>不过知乎用户<strong>" 我想唱 high C"</strong>（TSAIL 博士）认为：</p><p>DDPM 提出的时候，领域里的研究者其实并不完全清楚这个模型背后的数学原理，所以文章里的描述没有探寻到更本质的数学原理。</p><p>在他看来，直到斯坦福大学 Yang Song 等在 Score-Based Generative Modeling through Stochastic Differential Equations 中，才首次揭示了 diffusion model 的连续版本对应的数学背景。</p><p>并且将统计机器学习中的 denoising score matching 方法与 DDPM 中的去噪训练统一起来。</p><p></p><div class="img_box" id="id_imagebox_9" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_9" data-original="http://zkres1.myzaker.com/202208/6300efc88e9f0949472b2764_1024.jpg" data-height="393" data-width="1080" src="https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202208/6300efc88e9f0949472b2764_1024.jpg" referrerpolicy="no-referrer"></div></div>更多细节过程可以参考文末链接中的论文详情。<p></p><p>那么接下来需要探讨的一个问题是：</p><p><b>为什么 GAN 这么快会被取代？</b></p><p>用 OpenAI 的一篇论文内容来讲，用 Diffusion Model 生成的<strong>图像质量明显优于 GAN 模型</strong>。</p><p>DALL · E 是个多模态预训练大模型，" 多模态 " 和 " 大 " 字都说明，训练这个模型的数据集十分庞大冗杂。</p><p>发表这篇推特的 Tom Goldstein 教授提到，GAN 模型训练过程有个难点，就是众多损失函数的鞍点（saddle-point）的最优权重如何确定，这其实是个蛮复杂的数学问题。</p><p></p><div class="img_box" id="id_imagebox_10" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_10" data-original="http://zkres2.myzaker.com/202208/6300efc88e9f0949472b2765_1024.jpg" data-height="572" data-width="672" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202208/6300efc88e9f0949472b2765_1024.jpg" referrerpolicy="no-referrer"></div></div>在多层深度学习模型的训练过程中，需通过多次反馈，直至模型收敛。<p></p><p>但在实际操作中发现，损失函数往往不能可靠地收敛到鞍点，导致模型稳定性较差。即使有研究人员提出一些技巧来加强鞍点的稳定性，但还是不足以解决这个问题。</p><p>尤其面对更加复杂、多样化的数据，鞍点的处理就变得愈加困难了。</p><p>与 GAN 不同，DALL · E 使用 Diffusion Model，不用在鞍点问题上纠结，只需要去最小化一个标准的凸交叉熵损失（convex cross-entropy loss），而且人已经知道如何使其稳定。</p><p>这样就大大简化了模型训练过程中，数据处理的难度。说白了，就是用一个新的数学范式，从新颖的角度克服了一道障碍。</p><p>此外，GAN 模型在训练过程中，除了需要 " 生成器 "，将采样的高斯噪声映射到数据分布；还需要额外训练判别器，这就导致训练变得很麻烦了。</p><p>和 GAN 相比，Diffusion Model 只需要训练 " 生成器 "，训练目标函数简单，而且不需要训练别的网络（判别器、后验分布等），瞬间简化了一堆东西。</p><p>目前的训练技术让<strong>Diffusion Model 直接跨越了 GAN 领域调模型的阶段</strong>，而是直接可以用来做下游任务。</p><p></p><div class="img_box" id="id_imagebox_11" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_11" data-original="http://zkres2.myzaker.com/202208/6300efc88e9f0949472b2766_1024.jpg" data-height="460" data-width="1080" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202208/6300efc88e9f0949472b2766_1024.jpg" referrerpolicy="no-referrer"></div></div><strong>△</strong>Diffusion Model 直观图<p></p><p>从理论角度来看，Diffusion Model 的成功在于训练的模型只需要 " 模仿 " 一个简单的前向过程对应的逆向过程，而不需要像其它模型那样 " 黑盒 " 地搜索模型。</p><p>并且，这个逆向过程的每一小步都非常简单，只需要用一个简单的高斯分布（q ( x ( t-1 ) | xt ) ）来拟合。</p><p>这为 Diffusion Model 的优化带来了诸多便利，这也是它经验表现非常好的原因之一。</p><p><b>Diffushion Model 是否就是完美？</b></p><p>不见得。</p><p>从趋势上来看，Diffushion Model 领域确实正处于百花齐放的状态，但正如 " 我想唱 high C" 所述：</p><p>这个领域有一些核心的理论问题还需要研究，这给我们这些做理论的人提供了个很有价值的研究内容。></p><p>并且，哪怕对理论研究不感兴趣，由于这个模型已经很 work 了，它和下游任务的结合也才刚刚起步，有很多地方都可以赶紧占坑。</p><p>我相信 Diffusion Model 的加速采样肯定会在不久的将来彻底被解决，从而让 Diffusion Model 占据深度生成模型的主导。</p><p>而对于 Diffusion Model 的有效性以及很快取代 GAN 这件事，<strong>马毅</strong>教授认为充分地说明了一个道理：</p><p>几行简单正确的数学推导，可以比近十年的大规模调试超参调试网络结构有效得多。</p><p>不过对于这种 " 前浪推后浪 " 的火热，马毅教授也有不一样的观点：</p><p>希望年轻的研究员端正研究的目的和态度，千万不要被目前热的东西忽悠。</p><p>包括 Diffusion Process，这其实也是好几百年 old 的想法，只是老树发新芽，找到新的应用。</p><div id="recommend_bottom"></div><div id="article_bottom"></div>  
</div>
            