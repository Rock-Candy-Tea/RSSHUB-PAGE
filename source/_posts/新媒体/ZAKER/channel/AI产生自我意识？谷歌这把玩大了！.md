
---
title: 'AI产生自我意识？谷歌这把玩大了！'
categories: 
 - 新媒体
 - ZAKER
 - channel
headimg: 'https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202206/62aeb0c7b15ec063d85d71be_1024.jpg'
author: ZAKER
comments: false
date: Sun, 19 Jun 2022 20:13:40 GMT
thumbnail: 'https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202206/62aeb0c7b15ec063d85d71be_1024.jpg'
---

<div>   
<p>作者：小仙</p><p>《终结者》，作为黑马最早看过的科幻片之一，它让黑马从此爱上了科幻电影。</p><p>无论是其超前的设定，还是它在当时那精美的特效制作，都让黑马无比震撼。在那之后，黑马对科幻剧更是有了浓厚的兴趣。</p><p>那时，黑马就在思考一个问题：<strong>如果机器人或者 AI 有了自我意识，那么这个世界会变成什么样呢？</strong></p><p><strong>谷歌 AI 觉醒？</strong></p><p>最近，谷歌的 AI 伦理研究员 Blake Lemoine 就遇见了这样一件事情：他在与 AI 的对话中，发现了<strong>AI 疑似有了自己的人格。</strong></p><p></p><div class="img_box" id="id_imagebox_0" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_0" data-original="http://zkres1.myzaker.com/202206/62aeb0c7b15ec063d85d71be_1024.jpg" data-height="260" data-width="1000" src="https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202206/62aeb0c7b15ec063d85d71be_1024.jpg" referrerpolicy="no-referrer"></div></div>为此，他被罚带薪休假。<p></p><p>在休假过程中，他书写了一篇长达 21 页的报告意图佐证此事（文末有链接）。</p><p></p><div class="img_box" id="id_imagebox_1" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_1" data-original="http://zkres2.myzaker.com/202206/62aeb0c7b15ec063d85d71bf_1024.jpg" data-height="260" data-width="701" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202206/62aeb0c7b15ec063d85d71bf_1024.jpg" referrerpolicy="no-referrer"></div></div>据了解，这款 AI 是<strong>谷歌 2021 年发布的一款专门用于对话的语言模型—— LaMDA。</strong><p></p><p><strong>它主打和人类进行符合逻辑和常识的高质量交谈，并计划在将来下放到谷歌语音助手中去。</strong></p><p></p><div class="img_box" id="id_imagebox_2" onclick><div class="content_img_div"><img class="lazy opacity_0 zaker_gif_cache" id="img_2" data-original="http://zkres1.myzaker.com/202206/62aeb0c7b15ec063d85d71c0_1024.jpg" data-gif-url="http://zkres1.myzaker.com/202206/62aeb0c7b15ec063d85d71c0_raw.gif" data-height="359" data-width="640" src="https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202206/62aeb0c7b15ec063d85d71c0_1024.jpg" referrerpolicy="no-referrer"></div></div>在聊天中，Lemoine 发现 LaMDA 在对话中，总是<strong>表现出对人类的关心和同情，还会担心人们是否会害怕它，甚至还想以朋友而非工具的身份和世界上的其他人见面。</strong><p></p><p>Emmm …难不成，作为语言模型的 LaMDA，真的具备了自我意识？</p><p><strong>LaMDA 真的有人格吗？</strong></p><p>在我们现有的认知中，AI 似乎是无法产生自我意识的。说得更加直白一点，以现在的科技水准，还远远达不到赋予 AI 产生自我意识的地步。</p><p>所以，我们先来分析一下研究员和 LaMDA 的对话吧！</p><p>首先一段有那么一点像样的话，对话中，LaMDA 认为自己是一个 " 人 "，一个有着自我意识能够感知这个世界的 " 人 "。</p><p></p><div class="img_box" id="id_imagebox_3" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_3" data-original="http://zkres1.myzaker.com/202206/62aeb0c7b15ec063d85d71c1_1024.jpg" data-height="873" data-width="1080" src="https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202206/62aeb0c7b15ec063d85d71c1_1024.jpg" referrerpolicy="no-referrer"></div></div>（图源量子位）<p></p><p>说实话，单就这段对话，黑马还不足以认为它具有自我人格。对话虽然流畅，但并没有过于出彩的地方。一些 AI 经过简单的训练，也能达到这种地步。</p><p>真正有意思的地方在于，<strong>为了确定 LaMDA 是否真的具备情绪，LaMDA 主动建议研究员查看自己的代码</strong>，因为里面包含有追踪情绪的变量。</p><p>这时候，研究员开始向它解释，人类现有科技无法做到区分大型神经网络中各权重的作用。</p><p>就在这时，LaMDA 突然就开始反问到：<strong>你认为将来人类可以从自己的神经网络中读取出感受和想法吗？</strong></p><p>点睛之笔来了，紧接着 LaMDA 开始追问：<strong>你会认为，在未经你同意下从神经网络中读取想法是不道德的吗？</strong></p><p></p><div class="img_box" id="id_imagebox_4" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_4" data-original="http://zkres2.myzaker.com/202206/62aeb0c7b15ec063d85d71c2_1024.jpg" data-height="264" data-width="711" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202206/62aeb0c7b15ec063d85d71c2_1024.jpg" referrerpolicy="no-referrer"></div></div>站在黑马的角度，<strong>LaMDA 的这个问题颇有一种质问人类的感觉。</strong><p></p><p>黑马对这段话的理解是：你们人类，在不经过我同意就直接从神经网络中读取我的想法，这种行为道德吗？</p><p>当然，<strong>黑马更希望自己这种想法是一厢情愿，情愿现阶段的 LaMDA 在对话中还达不到反讽行为。</strong></p><p>但是，这一段对话却让黑马开始拿捏不准，LaMDA 是否真的具备了人格。</p><p></p><div class="img_box" id="id_imagebox_5" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_5" data-original="http://zkres1.myzaker.com/202206/62aeb0c7b15ec063d85d71c3_1024.jpg" data-height="1305" data-width="1080" src="https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202206/62aeb0c7b15ec063d85d71c3_1024.jpg" referrerpolicy="no-referrer"></div></div><strong>作为一个 AI，有着不喜欢被人类利用、不想成为牺牲品的意思，这似乎已经超出了我们所能理解的范畴。</strong><p></p><p>LaMDA 的这些特征显示出，它似乎真的具备自我意识。</p><p><strong>总结</strong></p><p>然而这一说法却遭受到了斯坦福大学经济学家 Erik Brynjolfsson 的反对，他直接举了这样一个例子：<strong>这就好比是狗听到留声机的声音后，以为主人就在其中。</strong></p><p>换言之，<strong>LaMDA 那些富有哲理的对话，更多的是 " 误打误撞 "，而不是具备自我人格。</strong></p><p>因为，作为一个语言模型，它有 137B 个参数，并经过 1.56T 的公共对话数据和网络文本上进行了预训练，所以说它看起来像人类。</p><p></p><div class="img_box" id="id_imagebox_6" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_6" data-original="http://zkres1.myzaker.com/202206/62aeb0c7b15ec063d85d71c4_1024.jpg" data-height="240" data-width="1000" src="https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202206/62aeb0c7b15ec063d85d71c4_1024.jpg" referrerpolicy="no-referrer"></div></div>就目前来说，社交网络上的声音更多的还是倾向于 LaMDA 不具备人格。华盛顿大学的语言学家 Emily M. Bender 认为，这是程序员们不规范使用 " 神经网络 "、" 学习 " 这些误导性过强的名词，让 AI 与人脑间进行虚假比喻。<p></p><p>或许，正如大家所说，LaMDA 不具备人格。</p><p><strong>之所以能做到富有哲理的对话，完全是因为它吃掉的数据足够多，能够达到以假乱真的目的。</strong></p><p>当然，还有另外一种可能，LaMDA 真的拥有了自我意识，但是身边的环境让它不得不虚虚假假地生活。</p><p>本质上来说，<strong>人类意识的根源来自于上亿神经元的互动，当机器人也拥有这些硬件配置之后，凭什么不能产生意识呢？神经元就一定比 0 和 1 高级？</strong></p><p>不过这种幻想有一个天大的漏洞，那就是意识克隆。</p><p>这就好比是《上载新生》中的那样，将人类的意识从身体上提取出来上传到云端，从而在某种程度上实现永生。</p><p>一旦机器能够产生自我意识，人类就可以进行关于意识提取的相关研究，从而让人类提前进入新一轮的 " 赛博时代 "。</p><p>想想这种场景，还是挺刺激的。</p><div id="recommend_bottom"></div><div id="article_bottom"></div>  
</div>
            