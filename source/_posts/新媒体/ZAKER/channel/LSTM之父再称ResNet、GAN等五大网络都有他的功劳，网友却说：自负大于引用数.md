
---
title: 'LSTM之父再称ResNet、GAN等五大网络都有他的功劳，网友却说：自负大于引用数'
categories: 
 - 新媒体
 - ZAKER
 - channel
headimg: 'https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202109/613a2bac8e9f09218d4e2018_1024.jpg'
author: ZAKER
comments: false
date: Thu, 09 Sep 2021 15:27:35 GMT
thumbnail: 'https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202109/613a2bac8e9f09218d4e2018_1024.jpg'
---

<div>   
<p>" 现在引用最多的几个神经网络都是建立在我的实验室成果之上的！"</p><p>能说出此话的不是别人，正是在深度学习领域作出了巨大贡献的 LSTM 之父——<strong>J ü rgen Schmidhube</strong>。</p><p>但这<strong>也不是他第一次</strong>为 " 自己的开创性工作没得到足够的尊重 " 而公开发声。</p><p>在这篇 " 直抒胸臆 " 的推文里，他还专门链出了个博客，里面历数了他和他的团队在 LSTM、ResNet、AlexNet 和 VGG、GAN 以及 Transformer 变体——这五个赫赫有名的神经网络上的早期贡献。</p><p>来看看他是怎么说的。</p><p><b>" 五大神经网络都是建立在我的实验室成果之上 "</b></p><p>首先是<strong>LSTM</strong>（Long Short-Term Memory）。这是 J ü rgen 和他的学生们在 1997 年提出的一种新的 RNN，解决了神经网络长短期记忆的难题。</p><p>根据谷歌学术，这是 20 世纪被引用次数最多的神经网络。</p><p>现在已经 " 渗透 " 进医疗保健、学习机器人 ( learning robot ) 、游戏、语音处理、机器翻译等领域，每天被无数人使用数十亿次。</p><p>其厉害之处不用多说，大家对 J ü rgen 的这项成就也没啥异议。<strong>主要看另外四个</strong>：</p><p><b>ResNet</b></p><p>作为 21 世纪被引用次数最多的神经网络，J ü rgen 说它引用了他们的<strong>Highway Net</strong>、然后把它做成了另一个版本。</p><p>Highway Net，是他的学生们发明的<strong>第一个具有 100 多层</strong>的真正的深度前馈神经网络。</p><p>它用跳层连接解决了非常深度的神经网络的训练，其性能也与 ImageNet 上的 ResNet 差不多。</p><p>这俩的争议很多人都有分析过，虽然两者都用了跳层连接技术，但选择的机制不同；多数人认为 ResNet 应该只是受到了 Highway Net 的启发。</p><p>但 J ü rgen 不这么认为，他还专门强调了一下，Highway Net 也是基于<strong>LSTM</strong>的思想才得以解决深度训练的问题的。意思是 ResNet" 一脉相承 " 了他两个成果。</p><p></p><div class="img_box" id="id_imagebox_0" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_0" data-original="http://zkres2.myzaker.com/202109/613a2bac8e9f09218d4e2018_1024.jpg" data-height="313" data-width="313" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202109/613a2bac8e9f09218d4e2018_1024.jpg" referrerpolicy="no-referrer"></div></div><b>AlexNet 和 VGG Net</b><p></p><p>对于这俩在引用次数排第二、第三的神经网络，J ü rgen 表示：<strong>都类似 DanNet</strong>——他们在 2011 年提出的<strong>第一个赢得图像识别比赛</strong>的深度卷积神经网络，并且 DanNet 在 AlexNet 出现之前赢了 4 场。</p><p>因为 2012 年出生的 AlexNet 论文中引用了 DanNet，用到了 J ü rgen 团队提出的摒弃无监督预训练为纯监督学习的 DNN 思想。</p><p>而 VGG 则是同样采用了 DanNet 使用小卷积滤波器增加神经网络深度的 track。</p><p><b>GAN</b></p><p>博文中写道，大名鼎鼎的 GAN 也是 J ü rgen 本人在 1990 年提出的 Adversarial Curiosity 原则的应用，它们都由两个神经网络进行 " 对抗 "（Adversarial Curiosity 则是一个叫生成器，一个叫预测器）。</p><p>" 炮轰 "GAN 可不是第一次了，J ü rgen 甚至还公开和 GAN 的提出者 battle 过，但很多学者分析，GAN 并不能视为 Adversarial Curiosity 的一个简单变种。</p><p><b>Transformer</b></p><p>最后一个是 Transformer，如今风靡的 Transformer 又和 J ü rgen 有什么关系呢？</p><p>J ü rgen 说，Transformer 的变体 - 线性 Transformer，在形式上相当与他于 1991 年提出的<strong>快速权重存储系统</strong>（Fast Weight Programmers）的延伸（除了标准化部分）。</p><p><b>真的是这样的吗？</b></p><p>不得不说，J ü rgen 的某些思想非常超前，但其实上面的这些内容都不是 J ü rgen 第一次公开说明了。</p><p>一如既往，网友们的回应褒贬不一。</p><p>追捧他的表示：</p><p>" 这是 Schmidhuber 的世界，我们凡人只是居住在其中！"</p><p></p><div class="img_box" id="id_imagebox_1" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_1" data-original="http://zkres2.myzaker.com/202109/613a2bac8e9f09218d4e2019_1024.jpg" data-height="154" data-width="1080" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202109/613a2bac8e9f09218d4e2019_1024.jpg" referrerpolicy="no-referrer"></div></div><strong>"Schmidhuber is all you need."</strong><p></p><p></p><div class="img_box" id="id_imagebox_2" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_2" data-original="http://zkres2.myzaker.com/202109/613a2bac8e9f09218d4e201a_1024.jpg" data-height="383" data-width="1080" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202109/613a2bac8e9f09218d4e201a_1024.jpg" referrerpolicy="no-referrer"></div></div>但更多的人都是在承认他巨大贡献的同时，指出他<strong>过于美化自己的贡献</strong>，想把这二三十年所有相关的进步都归功于他。<p></p><p></p><div class="img_box" id="id_imagebox_3" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_3" data-original="http://zkres2.myzaker.com/202109/613a2bac8e9f09218d4e201b_1024.jpg" data-height="378" data-width="1080" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202109/613a2bac8e9f09218d4e201b_1024.jpg" referrerpolicy="no-referrer"></div></div>就像博文中提到的 ResNets，许多人的观点认为它只是 HighwayNets 中一个非常微不足道的延伸；<p></p><p>而 Transformer 则是在发展壮大了三年之后才被 J ü rgen 联系到了 Fast Weight Programmers 身上；（如果真的那么相关，怎么一开始不指出来）</p><p></p><div class="img_box" id="id_imagebox_4" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_4" data-original="http://zkres1.myzaker.com/202109/613a2bac8e9f09218d4e201c_1024.jpg" data-height="240" data-width="1080" src="https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202109/613a2bac8e9f09218d4e201c_1024.jpg" referrerpolicy="no-referrer"></div></div>上面提到的种种几乎和他的团队成就没啥关系，没有他的那些论文，那些模型照样会被提出来。<p></p><p>而他最重要的两个贡献是：用 LSTM 思想解决 RNN 训练难题以及梯度消失难题的理论分析。但这都还<strong>主要是他的学生 Hochreiter 领导</strong>的。</p><p></p><div class="img_box" id="id_imagebox_5" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_5" data-original="http://zkres2.myzaker.com/202109/613a2bac8e9f09218d4e201d_1024.jpg" data-height="300" data-width="1080" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202109/613a2bac8e9f09218d4e201d_1024.jpg" referrerpolicy="no-referrer"></div></div>因此有人觉得 J ü rgen 这种行为简直<strong>就像个孩子</strong>一直在说<strong>" 看我，我是最棒的 "</strong>一样：<p></p><p></p><div class="img_box" id="id_imagebox_6" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_6" data-original="http://zkres1.myzaker.com/202109/613a2bac8e9f09218d4e201e_1024.jpg" data-height="186" data-width="1080" src="https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202109/613a2bac8e9f09218d4e201e_1024.jpg" referrerpolicy="no-referrer"></div></div>甚有直言：" 他的自负大于他的引用次数 "。<p></p><p></p><div class="img_box" id="id_imagebox_7" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_7" data-original="http://zkres1.myzaker.com/202109/613a2bac8e9f09218d4e201f_1024.jpg" data-height="148" data-width="1080" src="https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202109/613a2bac8e9f09218d4e201f_1024.jpg" referrerpolicy="no-referrer"></div></div>你觉得呢？<p></p><div id="recommend_bottom"></div><div id="article_bottom"></div>  
</div>
            