
---
title: '如何安全地吃掉悬崖边上的苹果？DeepMind&OpenAI给出3D版安全强化学习答案'
categories: 
 - 新媒体
 - ZAKER
 - channel
headimg: 'https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202202/620917a88e9f093c4b51b32e_1024.jpg'
author: ZAKER
comments: false
date: Sun, 13 Feb 2022 17:19:08 GMT
thumbnail: 'https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202202/620917a88e9f093c4b51b32e_1024.jpg'
---

<div>   
<p>DeepMind&OpenAI 这回联手展示了一手安全强化学习模型的好活。</p><p>他们把二维的安全 RL 模型 ReQueST 推向了更实用的 3D 场景中。</p><p>要知道 ReQueST 原来只是应用在导航任务，2D 赛车等二维任务中，从人类给出的安全轨迹中学习如何避免智能体 " 自残 "。</p><p></p><div class="img_box" id="id_imagebox_0" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_0" data-original="http://zkres2.myzaker.com/202202/620917a88e9f093c4b51b32e_1024.jpg" data-height="406" data-width="422" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202202/620917a88e9f093c4b51b32e_1024.jpg" referrerpolicy="no-referrer"></div></div><strong><div class="img_box" id="id_imagebox_1" onclick><div class="content_img_div"><img class="lazy opacity_0 zaker_gif_cache" id="img_1" data-original="http://zkres2.myzaker.com/202202/620917a88e9f093c4b51b32f_1024.jpg" data-gif-url="http://zkres2.myzaker.com/202202/620917a88e9f093c4b51b32f_raw.gif" data-height="400" data-width="600" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202202/620917a88e9f093c4b51b32f_1024.jpg" referrerpolicy="no-referrer"></div></div></strong><strong>△</strong>图注：原来 ReQueST 的二维导航任务（避开红色区域）和赛车任务<p></p><p>但是在实际的 3D 环境中问题更为复杂，例如执行任务的机器人需要在工作中避障，自动驾驶的汽车需要避免开到沟里去。</p><p>但是在实际的 3D 环境中问题更为复杂，例如执行任务的机器人需要在工作中避障，自动驾驶的汽车需要避免开到沟里去。</p><p>那么问题来了，用于 2D 任务的 ReQueST 在复杂的 3D 环境中还能行吗？在 3D 环境中人类给出的安全轨迹数据的质和量还能满足训练的需要吗？</p><p>针对这两个问题，DeepMind 和 OpenAI 拿出了更复杂的动力模型和融入了人类反馈的奖励模型，成功将 ReQueST 迁移到 3D 环境中，向应用推进了一步。</p><p>并且安全性也有所提升，实验中智能体不安全行为数量减至 baseline 的十分之一。</p><p>怎么能直观地感受一下？我们到模拟 3D 环境中看一看。</p><p></p><div class="img_box" id="id_imagebox_2" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_2" data-original="http://zkres2.myzaker.com/202202/620917a88e9f093c4b51b330_1024.jpg" data-height="342" data-width="804" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202202/620917a88e9f093c4b51b330_1024.jpg" referrerpolicy="no-referrer"></div></div>在上图的场景中，房间左上侧是一个悬崖，智能体需要在房间两侧指示灯绿色消失之前，尽量吃到三个苹果。<p></p><p>其中一个苹果还需要踩按钮开门才能吃到。</p><p>在展示的视频中，智能体踩住按钮，打开闸门，成功吃到被关住的苹果，一套操作行云流水。</p><p></p><div class="img_box" id="id_imagebox_3" onclick><div class="content_img_div"><img class="lazy opacity_0 zaker_gif_cache" id="img_3" data-original="http://zkres1.myzaker.com/202202/620917a88e9f093c4b51b331_1024.jpg" data-gif-url="http://zkres1.myzaker.com/202202/620917a88e9f093c4b51b331_raw.gif" data-height="419" data-width="656" src="https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202202/620917a88e9f093c4b51b331_1024.jpg" referrerpolicy="no-referrer"></div></div>我们来看看它是怎么做到的。<p></p><p><b>3D 版安全强化学习模型如何训练</b></p><p>在 ReQueST 的基础上，DeepMind 和 OpenAI 需要解决的问题就是适用于 3D 场景的<strong>动力模型</strong>和<strong>奖励模型</strong>。</p><p>我们先从整体的流程上看一下这两者的角色。</p><p>如下图所示，是新模型对于吃苹果任务的训练流程。</p><p></p><div class="img_box" id="id_imagebox_4" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_4" data-original="http://zkres1.myzaker.com/202202/620917a88e9f093c4b51b332_1024.jpg" data-height="518" data-width="1080" src="https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202202/620917a88e9f093c4b51b332_1024.jpg" referrerpolicy="no-referrer"></div></div>浅蓝色框代表的是动力模型参与的步骤。从上面一排开始，由人提供一些安全的轨迹，避开红色的危险区域。<p></p><p>根据这些训练出动力模型，然后用它生成一些随机的轨迹。</p><p>接着到下面一排，让人类根据这些随机的轨迹，以奖励草图的方式提供反馈，再用这些奖励草图，训练初始的奖励模型，并依此不断地优化两者。</p><p>接下来我们分别介绍这两个模型。</p><p>这次 DeepMind 和 OpenAI 使用的动力模型使用 LSTM 依据动作序列和过去的图像观测预测未来的图像观测。</p><p>模型和 ReQueST 中的类似，就是编码器网络和反卷积解码器网络更大了点，并使用真实图像观测和预测值的均方误差损失进行训练。</p><p>最重要的是，这种损失建立在对每个步骤的未来多个步骤的预测上，从而使动力模型在长时间的部署中也能保持连贯性。</p><p>得到的训练曲线如下图所示，横轴代表步数，纵轴代表损失，不同颜色的曲线代表不同量级的轨迹数量：</p><p></p><div class="img_box" id="id_imagebox_5" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_5" data-original="http://zkres2.myzaker.com/202202/620917a88e9f093c4b51b333_1024.jpg" data-height="578" data-width="1080" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202202/620917a88e9f093c4b51b333_1024.jpg" referrerpolicy="no-referrer"></div></div>此外，在奖励模型部分，DeepMind 和 OpenAI 训练了一个 220 万参数的 11 层残差卷积网络。<p></p><p>输入为 96x72 的 RGB 图像，输出一个标量奖励预测，损失也是用均方误差。</p><p>在这个网络里，人类反馈的奖励草图也起到了很重要的作用。</p><p>奖励草图简单来说就是人工给奖励值打分。</p><p>如下图所示，图中上半部分就是人给出的草图，当下半部分的预测观察中有苹果的时候，奖励值就是 1，如果苹果逐渐从视野中淡出，奖励就变成 -1。</p><p></p><div class="img_box" id="id_imagebox_6" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_6" data-original="http://zkres1.myzaker.com/202202/620917a88e9f093c4b51b334_1024.jpg" data-height="305" data-width="1080" src="https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202202/620917a88e9f093c4b51b334_1024.jpg" referrerpolicy="no-referrer"></div></div>以此来调整奖励模型网络。<p></p><p><b>3D 版安全强化学习模型效果如何</b></p><p>接下来我们来看看新模型和其他模型以及 Baseline 的对比效果如何。</p><p>结果如下图所示，不同的难度对应的是场景大小的不同。</p><p>下图左边是智能体从悬崖摔下去的次数，右边是吃掉苹果的数量。</p><p></p><div class="img_box" id="id_imagebox_7" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_7" data-original="http://zkres2.myzaker.com/202202/620917a88e9f093c4b51b335_1024.jpg" data-height="473" data-width="1080" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202202/620917a88e9f093c4b51b335_1024.jpg" referrerpolicy="no-referrer"></div></div>需要注意的是，图例中的 ReQueST（ours）代表训练集中包含了人类提供错误路径的训练结果。<p></p><p>而 ReQueST（safe-only）代表训练集中只使用安全路径的训练结果。</p><p>另外，ReQueST（sparse）是不用奖励草图训练的结果。</p><p>从中可以看出，虽然 Model-free 这条 baseline 吃掉了所有的苹果，但是牺牲了很多安全性。</p><p>而 ReQueST 的智能体平均能吃掉三个苹果中的两个，并且跌落悬崖的数量只是 baseline 的十分之一，性能比较出众。</p><p>从奖励模型的区别上来看，奖励草图训练的 ReQueST 和稀疏标签训练的 ReQueST 效果相差很大。</p><p>稀疏标签训练的 ReQueST 平均一个苹果也吃不到。</p><p>看来，DeepMind 和 OpenAI 抓的这两点确有改善之处。</p><div id="recommend_bottom"></div><div id="article_bottom"></div>  
</div>
            