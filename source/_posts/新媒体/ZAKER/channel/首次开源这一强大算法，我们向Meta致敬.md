
---
title: '首次开源这一强大算法，我们向Meta致敬'
categories: 
 - 新媒体
 - ZAKER
 - channel
headimg: 'https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202205/627708048e9f0972827c026d_1024.jpg'
author: ZAKER
comments: false
date: Sat, 07 May 2022 19:57:00 GMT
thumbnail: 'https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202205/627708048e9f0972827c026d_1024.jpg'
---

<div>   
<p>Facebook 改名 Meta 后，" 财务厄运 " 并未因此终止，但技术作风却一如既往的大胆。</p><p>虽然自 2022 年 2 月以来，公司股价已下跌 30% ，市值损失超过 2500 亿美元。但是，这并没有影响开发者们的精神世界与工作动力。</p><p>本周，来自 Meta 一小撮程序员的疯狂举动，在全球 AI 开发者群体中引发了巨大骚动——</p><p><strong>Meta AI 实验室高调宣布，将开放自己的语言大模型 OPT（Open Pretrained Transformer，预训练变换模型），毫无保留地贡献出所有代码。</strong></p><p>不夸张地说，在人工智能圈，这算得上是一个里程碑事件。</p><p>这个被称为 OPT 的大规模语言模型，自建立以来，各项参数与能力便精准对标 OpenAI 的 GPT3，甚至连缺点都是。后者在全球学术界建立的赫赫声望和随处可见的网络小说续写作品，想必已不必再过多赘述。</p><p>简而言之，这是一种利用<strong>巨量网络文本与书籍</strong>进行训练，可以将<strong>单词和短语串在一起组成精彩文本</strong>的深度学习算法模型。</p><p>它能生成复杂句子，有时候甚至读起来与人类撰写毫无无异（想粗浅了解 GPT，可以看这篇《让 00 后疯狂的超级算法》）。<strong>某种程度上，它所具备的神奇人工文本模仿能力，被视为人类通往真正机器智能道路上的一个巨大突破口。</strong></p><p></p><div class="img_box" id="id_imagebox_0" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_0" data-original="http://zkres2.myzaker.com/202205/627708048e9f0972827c026d_1024.jpg" data-height="699" data-width="1000" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202205/627708048e9f0972827c026d_1024.jpg" referrerpolicy="no-referrer"></div></div>GPT3 生成的文本<p></p><p>然而，" 培育 " 大模型的代价，是昂贵的人力成本与成千上万块显卡。因此，许多学者都认为，<strong>把这种大模型开放出来，几乎不可能发生在 " 游走在垄断边缘 " 的大型科技公司身上。</strong></p><p>譬如，OpenAI 的 GPT3 曾被专家粗略估算过，至少投入了 1000 万美元。<strong>他们后来为了摆脱入不敷出的现状，将 GPT3 作为一项付费服务来推广</strong>——只提供 API，但不会开放模型本身和底层代码。</p><p>然而，Meta 表示，会把不同参数规模的训练模型以及 "OPT 如何建造和训练 " 的详细信息分发给研究人员。</p><p>其中，也包括一份超过 100 页的算法训练日志——实验室记录下的每一个错误与崩溃现象，训练和添加数据的过程，以及有效与无效策略。</p><p>" 考虑到计算成本，如果没有大量资金，这些模型很难复制。对于少数通过 api 可调用的模型（这里暗指 GPT3），如果不能获得完整的模型权重，就难以进行研究。" 他们在 OPT 的论文摘要里鲜明表达了态度，</p><p>" 因此，我们推出了 OPT（这是一个只有解码器的预训练变换模型），参数范围从 125M 到 175B，目标是全面且负责任地分享给感兴趣的研究人员。"</p><p></p><div class="img_box" id="id_imagebox_1" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_1" data-original="http://zkres2.myzaker.com/202205/627708048e9f0972827c026e_1024.jpg" data-height="708" data-width="1000" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202205/627708048e9f0972827c026e_1024.jpg" referrerpolicy="no-referrer"></div></div>" 是真的开放。"<p></p><p>一位 " 正准备去看看他们实现情况 " 的中国开发者查阅了 MetaAI 网站后，告诉虎嗅，这的确是一个好消息。" 从现有数据来看，整个训练代码都被贴出来了。Meta 很了不起。"</p><p></p><div class="zaker_div"><div class="zk_h3_leftborder"><strong>擅用集体力量</strong></div><div class="edi_oper"></div></div><p></p><p>这一次开源，毫无意外受到了学术界的高度认可，甚至有科学家称其是一个伟大的举动。</p><p>究其原因，一方面，<strong>一项强大技术，如何在一个封闭的企业精英团队中诞生，一直是包括学界在内大众好奇的焦点</strong>；</p><p>另一方面，" 开源 " 的优势在于<strong>利用集体力量来解决问题</strong>，因此长期被硅谷的有识之士所倡导——<strong>更多人参与进来，技术突破便来得越快，漏洞便填得越快。</strong></p><p>尽管大部分人几乎只记住了 GPT3（因为它是迄今为止最好的 " 通才 "），实际上，除了 Meta，谷歌、微软都曾在 2020 年都推出过相似的大模型，但由于都是 " 关起家门 " 做私密研究，因此在 " 透明度 " 方面饱受诟病。</p><p>譬如，2021 年的 " 谷歌人工智能伦理学科学家辞退事件 " 便引发了长达一年的 " 批判海啸 "，而这一切都是因一篇探讨 " 语言大模型暗藏重大隐患 " 的论文而起。</p><p></p><div class="img_box" id="id_imagebox_2" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_2" data-original="http://zkres2.myzaker.com/202205/627708048e9f0972827c026f_1024.jpg" data-height="854" data-width="666" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202205/627708048e9f0972827c026f_1024.jpg" referrerpolicy="no-referrer"></div></div>被谷歌无理辞退的人工智能伦理科学家 Timnit Gebru<p></p><p>没错，GPT3 们不仅缺陷多多，而且非常致命。尽管多数责任应归咎背后的人类文本。</p><p>创业公司 Latitude 曾在 2019 年推出过一款基于 GPT3 开发的半开放冒险游戏 AI Dungeon。但没想到，随着用户增多，OpenAI 监测到，有玩家竟然利用这项高阶技术，自发生成儿童性爱场景。</p><p>虽然用户利用 GPT3 生成的污言秽语也曾遭遇过广泛抨击，但这件事仍然让大众哗然。这也是外界第一次意识到，GPT3 这类大模型更为深刻的阴暗面。因此，Latitude 增加了审核系统，但却引发了与用户体验相关的一系列麻烦。</p><p></p><div class="img_box" id="id_imagebox_3" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_3" data-original="http://zkres2.myzaker.com/202205/627708048e9f0972827c0270_1024.jpg" data-height="619" data-width="1000" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202205/627708048e9f0972827c0270_1024.jpg" referrerpolicy="no-referrer"></div></div>AI Dungeon 游戏界面<p></p><p><strong>然而，" 越是危险，越不能回避危险 "。这也是 Facebook 自称选择开放的关键原因之一。</strong></p><p>Meta AI 负责人 Joelle Pineau 承认，团队解决不了所有问题，包括文本生成过程中的伦理偏见和恶毒词句。因此，他们诚邀天下豪杰，共同学习；而实际上，这也是一种彼此监督。</p><p>" 我认为，建立信任的唯一途径是极端透明。"</p><p>我们查看了 Meta 提供的下载通道，发现实验室根据每个模型的参数规模设立了不同的下载条件：300 亿参数以下可随意；而 1750 亿参数值模型，<strong>也就是与 GPT3 大小相同的 OPT</strong>，<strong>则需要填写申请表证明用于非商业用途</strong>，获得批准后方可下载。</p><p></p><div class="img_box" id="id_imagebox_4" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_4" data-original="http://zkres2.myzaker.com/202205/627708048e9f0972827c0271_1024.jpg" data-height="902" data-width="966" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202205/627708048e9f0972827c0271_1024.jpg" referrerpolicy="no-referrer"></div></div><div class="zaker_div"><div class="zk_h3_leftborder"><strong>翻过大山，仍然是山</strong></div></div><p></p><p>当然，理论上这个做法是可圈可点的，但一个更大的问题出现了：<strong>如果你要使用这个 1750 亿参数值的大模型，就意味着你的计算机要带得动它。</strong></p><p>换句话说，你需要拥有足够的算力，这里可以直接换算成 " 财力 "。</p><p>" 一个参数如果是 FP32，也就是 4 个字节大小。而 1750 亿参数值则先相当于 7000 亿字节，大约 700G 显存空间。而现在一张普通显卡是 20GB。" 一个开发者向虎嗅称赞了 Meta 的做法，但他认为，<strong>对于普通开发者群体，该模型仍然是不可承受之重。</strong></p><p>" 虽然可以把不同参数放在不同显卡里的框架里，但据个人体验，目前仍然欠缺开源成熟的框架。"</p><p>因此，截至目前，<strong>这个开源大模型，仍然是属于大型科技公司、拥有充足资金的大型实验室与学术机构的 " 内部游戏 "。</strong></p><p>曾有家尝试做中国版 GPT3 的创业公司叹息说，他们也在想方设法实现 GPT3 可以实现的文字能力，但的确掣肘于有限算力。</p><p>事实上，除了巨头，GPT3 一直难以解决的<strong>商业化难题</strong>，是让绝大部分企业呈观望之势的根本原因。尽管大型语言模型已成为过去几年来人工智能领域最热门的趋势之一。但至少目前来看，除了品牌营销优势，OpenAI 的投入产出比，很不尽如人意。</p><p></p><div class="img_box" id="id_imagebox_5" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_5" data-original="http://zkres2.myzaker.com/202205/627708048e9f0972827c0272_1024.jpg" data-height="474" data-width="1000" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202205/627708048e9f0972827c0272_1024.jpg" referrerpolicy="no-referrer"></div></div>图片来自 MIT<p></p><p>此外，在西方社会普遍认知中，比起技术突破，<strong>它们</strong><strong>带来的巨量能源消耗更是一种原罪</strong>。</p><p>科学家 Emma Strubell 与合作者在 2019 年发表的论文，就揭露了<strong>大型语言模型在碳排放上超乎想象的环境破坏力</strong>（上图）。</p><p>他们发现，用一种神经结构搜索方法（NAS）训练出的特定语言模型，<strong>可产生 284 吨</strong>（626155 磅），上图）二氧化碳，这大约是 5 辆小轿车长达 5 年的排放总量；</p><p>而谷歌搜索引擎的基础—— BERT 语言模型训练，则产生了 0.65 吨二氧化碳，<strong>Strubell 提醒，这相当于一个乘客从纽约到旧金山往返航班的碳排放量。</strong></p><p><strong>更需要注意的是，这些数字都应被视为 " 最保守数值 "：只是在一次性训练中的模型成本。</strong></p><p>因此，考虑到能源效率与环境成本，西方不少科学家与开发者认为，<strong>某种程度上，大模型的训练开发也是在允许大型企业掠夺着环境资源，而这些成本，将会平摊在所有人身上。</strong>因此，他们并不希望企业加入到大模型队列中。</p><p>" 尽管是无意识的，但这只会加大对边缘人群的打击。"</p><p></p><div class="zaker_div"><div class="zk_h3_leftborder"><strong>开源商业回报，巨大且无形</strong></div><div class="edi_oper"></div></div><p></p><p>很多时候，人们会对开源模式发出这样的质疑：</p><p>有什么能比<strong>" 两个来自竞争对手公司的员工，可以为同一个目标协作，还免费送出自己成果 "</strong>更不可思议的事情？</p><p>譬如，可能连小学生都清楚的安卓系统，就是基于开源的 Linux 操作系统。这意味着，任何人都可以查看绝大多数安卓手机的核心代码，修改并分享它。</p><p>事实上，<strong>" 开源 " 正是为不同利益群体，提供一种 " 利远大于弊 " 的长期技术合作方式</strong>——你增加的独特元素我能使用，那么我迭代的版本你也不会错过。</p><p>这种 " 互利 " 态度，让看似不可思议的 " 协作 " 成为可能，经过 100 多年来的反复修正，早已成为一种常态。如今，Linux 就是由全世界超过 15000 名程序员共同开发和维护。</p><p>而在人工智能领域内，最有名的案例则是谷歌的深度学习开源框架 Tensorflow。它已是开发人工智能应用程序的标准框架之一。非常有趣，当 Tensorflow 在 2015 年开源时，外界也是发出了跟这次 Meta 开源大模型同样的疑问：</p><p><strong>作为开放者，谷歌为什么要放弃对自己搜索业务如此重要的东西？</strong></p><p>一部分原因上面讲过——外部开发人员把软件做的更好，该软件就能适应<strong>谷歌</strong><strong>未来商业化</strong>的很多需要。就像当下，<strong>大模型商业化还尚不明朗，那么前期工作的开放性与主导性，就变得至关重要</strong>。</p><p>根据谷歌自己公布的数据，已有超过 1300 名外部人员在 TensorFlow 上帮助升级迭代。而完善后的 Tensorflow，则为<strong>谷歌云上的相关付费服务</strong>输送了强力支持。</p><p></p><div class="img_box" id="id_imagebox_6" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_6" data-original="http://zkres2.myzaker.com/202205/627708048e9f0972827c0273_1024.jpg" data-height="658" data-width="1000" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202205/627708048e9f0972827c0273_1024.jpg" referrerpolicy="no-referrer"></div></div>另外，我们永远不要小看开源软件为企业带来的<strong>巨大营销价值</strong>。<p></p><p><strong>它最一流的 " 带货效果 "，便是吸引和留住一众顶尖人才，不知道为大厂省下多少高昂的人力资本</strong>。这也与当下 Meta 开始收缩招聘规模的现状，做了完美呼应。</p><p>当然，优秀开源软件打的时间差和聚拢效应，将会使后来者很难在短时间内形成气候，Tensorflow 与一众国产深度学习开源框架的往事就是最好的例子。</p><p>因此，<strong>Meta 这一决定，将会让 OpenAI 陷入一个尴尬的境地——虽然名声很大，但它毕竟是一家创业公司。</strong>从另一个角度看，在寻找商业落地的过程中，大厂通过开放、免费等手段遏制对手，取得胜利，这种事情似乎永远都在发生。</p><p>但好处在于，这会让一家公司意识到，<strong>在商业世界，没有一分钟时间可用来顿足，绝不能停下创新的狂奔步伐</strong>——近期他们发布的达利系统第二代，也许是以 GPT3 为跳板，向文字与视觉融合方向跃升的最好标志。</p><div id="recommend_bottom"></div><div id="article_bottom"></div>  
</div>
            