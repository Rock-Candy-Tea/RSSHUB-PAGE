
---
title: '不看全图看局部，CNN性能竟然更强了'
categories: 
 - 新媒体
 - ZAKER
 - channel
headimg: 'https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202206/62a04ed88e9f09716421aefb_1024.jpg'
author: ZAKER
comments: false
date: Wed, 08 Jun 2022 02:08:08 GMT
thumbnail: 'https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202206/62a04ed88e9f09716421aefb_1024.jpg'
---

<div>   
<p>不给全图，只投喂 CNN 一些看上去毫无信息量的图像碎片，就能让模型学会图像分类。</p><p>更重要的是，<strong>性能完全不差，甚至还能反超用完整图像训练的模型</strong>。</p><p></p><div class="img_box" id="id_imagebox_0" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_0" data-original="http://zkres1.myzaker.com/202206/62a04ed88e9f09716421aefb_1024.jpg" data-height="410" data-width="830" src="https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202206/62a04ed88e9f09716421aefb_1024.jpg" referrerpolicy="no-referrer"></div></div>这么一项来自加州大学圣塔芭芭拉分校的新研究，这两天引发不少讨论。<p></p><p>咋地，这就是说，CNN 根本无需理解图像全局结构，一样也能 SOTA？</p><p></p><div class="img_box" id="id_imagebox_1" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_1" data-original="http://zkres1.myzaker.com/202206/62a04ed88e9f09716421aefc_1024.jpg" data-height="322" data-width="1007" src="https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202206/62a04ed88e9f09716421aefc_1024.jpg" referrerpolicy="no-referrer"></div></div>具体是怎么一回事，咱们还是直接上论文。<p></p><p><b>实验证据</b></p><p>研究人员设计了这样一个实验：</p><p>他们在 CIFAR-10、CIFAR-100、STL-10、Tiny-ImageNet-200 以及 Imagenet-1K 等数据集上训练 ResNet。</p><p>特别的是，用于训练的图像是通过<strong>随机裁剪</strong>得到的。</p><p>这个 " 随机裁剪 "，可不是往常我们会在数据增强方法中见到的那一种，而是完全不做任何填充。</p><p>举个例子，就是对图片做 PyTorch 的 RandomCrop 变换时，padding 的参数填 0。</p><p>得到的训练图像就是下面这个样式的。即使你是阅图无数的老司机，恐怕也分辨不出到底是个啥玩意儿。</p><p></p><div class="img_box" id="id_imagebox_2" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_2" data-original="http://zkres2.myzaker.com/202206/62a04ed88e9f09716421aefd_1024.jpg" data-height="201" data-width="354" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202206/62a04ed88e9f09716421aefd_1024.jpg" referrerpolicy="no-referrer"></div></div>训练图像如此碎片化，模型的识图能力又能达到几成？<p></p><p>来看实验结果：</p><p></p><div class="img_box" id="id_imagebox_3" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_3" data-original="http://zkres2.myzaker.com/202206/62a04ed88e9f09716421aefe_1024.jpg" data-height="197" data-width="809" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202206/62a04ed88e9f09716421aefe_1024.jpg" referrerpolicy="no-referrer"></div></div>好家伙，在 CIFAR-10 上，用 16 × 16 的图像碎片训练出来的模型，测试准确率能达到<strong>91%</strong>，而用完整的 32 × 32 尺寸图像训练出来的模型，测试准确率也不过<strong>90%</strong>。<p></p><p>这一波，" 残缺版 "CNN 竟然完全不落下风，甚至还反超了 " 完整版 "CNN。</p><p>要知道，被喂了碎片的 CNN 模型，看到的图像甚至可能跟标签显示的物体毫无关系，只是原图中背景的部分……</p><p>在 STL-10、Tiny-Imagenet-200 等数据集上，研究人员也得到了类似的结果。</p><p>不过，在 CIFAR-100 上，还是完整图像训练出来的模型略胜一筹。16 × 16 图像碎片训练出的模型测试准确率为 61%，而 32 × 32 完整图像训练出的模型准确率为 68%。</p><p>所以，CNN 为何会有如此表现？莫非它本来就是个 " 近视眼 "？</p><p>研究人员推测，CNN 能有如此优秀的泛化表现，是因为在这个实验中，维度诅咒的影响被削弱了。</p><p>所谓维度诅咒（curse of dimensionality），是指当维数提高时，空间体积提高太快，导致可用数据变得稀疏。</p><p>而在这项研究中，由于 CNN 学习到的不是整个图像的标签，而是图像碎片的标签，这就在两个方面降低了维度诅咒的影响：</p><p>图像碎片的像素比完整图像小得多，这减少了输入维度</p><p>训练期间可用的样本数量增加了</p><p><b>生成热图</b></p><p>基于以上实验观察结果，研究人员还提出以热图的形式，来理解 CNN 的预测行为，由此进一步对模型的错误做出 " 诊断 "。</p><p>就像这样：</p><p></p><div class="img_box" id="id_imagebox_4" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_4" data-original="http://zkres1.myzaker.com/202206/62a04ed88e9f09716421aeff_1024.jpg" data-height="422" data-width="816" src="https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202206/62a04ed88e9f09716421aeff_1024.jpg" referrerpolicy="no-referrer"></div></div>这些图像来自于 STL-10 数据集。热图显示，对于 CNN 而言，飞机图像中最能 " 刺激 " 到模型的，不是飞机本身，而是天空。<p></p><p>同样，在汽车图像中，车轮才是 CNN 用来识别图像的主要属性。</p><div id="recommend_bottom"></div><div id="article_bottom"></div>  
</div>
            