
---
title: '这个_1句话生成视频_AI爆火：支持中文输入，分辨率达到480×480，清华&智源出品'
categories: 
 - 新媒体
 - ZAKER
 - channel
headimg: 'https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202206/6299a3eb8e9f0944d61ce3a2_1024.jpg'
author: ZAKER
comments: false
date: Fri, 03 Jun 2022 03:09:13 GMT
thumbnail: 'https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202206/6299a3eb8e9f0944d61ce3a2_1024.jpg'
---

<div>   
<p>一周不到，AI 画师又 " 进阶 " 了，还是一个大跨步——</p><p>直接<strong>1 句话生成视频</strong>的那种。</p><p>输入 " 一个下午在海滩上奔跑的女人 "，立刻就蹦出一个 4 秒 32 帧的小片段：</p><p>又或是输入 " 一颗燃烧的心 "，就能看见一只被火焰包裹的心：</p><p>这个最新的文本 - 视频生成 AI，是清华 & 智源研究院出品的模型<strong>CogVideo</strong>。</p><p>Demo 刚放到网上就火了起来，有网友已经急着要论文了：</p><p></p><div class="img_box" id="id_imagebox_0" onclick><div class="content_img_div"><img class="lazy opacity_0 zaker_gif_cache" id="img_0" data-original="http://zkres2.myzaker.com/202206/6299a3eb8e9f0944d61ce3a2_1024.jpg" data-gif-url="http://zkres2.myzaker.com/202206/6299a3eb8e9f0944d61ce3a2_raw.gif" data-height="661" data-width="480" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202206/6299a3eb8e9f0944d61ce3a2_1024.jpg" referrerpolicy="no-referrer"></div></div><div class="img_box" id="id_imagebox_1" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_1" data-original="http://zkres1.myzaker.com/202206/6299a3eb8e9f0944d61ce3a3_1024.jpg" data-height="212" data-width="962" src="https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202206/6299a3eb8e9f0944d61ce3a3_1024.jpg" referrerpolicy="no-referrer"></div></div>CogVideo" 一脉相承 " 于文本 - 图像生成模型 CogView2，这个系列的 AI 模型只支持<strong>中文输入</strong>，外国朋友们想玩还得借助谷歌翻译：<p></p><p></p><div class="img_box" id="id_imagebox_2" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_2" data-original="http://zkres2.myzaker.com/202206/6299a3eb8e9f0944d61ce3a4_1024.jpg" data-height="265" data-width="1080" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202206/6299a3eb8e9f0944d61ce3a4_1024.jpg" referrerpolicy="no-referrer"></div></div>看完视频的网友直呼 " 这进展也太快了，要知道文本 - 图像生成模型 DALL-E2 和 Imagen 才刚出 "<p></p><p></p><div class="img_box" id="id_imagebox_3" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_3" data-original="http://zkres2.myzaker.com/202206/6299a3eb8e9f0944d61ce3a5_1024.jpg" data-height="206" data-width="1080" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202206/6299a3eb8e9f0944d61ce3a5_1024.jpg" referrerpolicy="no-referrer"></div></div>还有网友想象：照这个速度发展下去，马上就能看到 AI 一句话生成 VR 头显里的 3D 视频效果了：<p></p><p></p><div class="img_box" id="id_imagebox_4" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_4" data-original="http://zkres2.myzaker.com/202206/6299a3eb8e9f0944d61ce3a6_1024.jpg" data-height="430" data-width="1080" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202206/6299a3eb8e9f0944d61ce3a6_1024.jpg" referrerpolicy="no-referrer"></div></div>所以，这只名叫 CogVideo 的 AI 模型究竟是什么来头？<p></p><p><b>生成低帧视频后再插帧</b></p><p>团队表示，CogVideo 应该是当前最大的、也是首个开源的文本生成视频模型。</p><p>在设计模型上，模型一共有 90 亿参数，基于预训练文本 - 图像模型 CogView2 打造，一共分为两个模块。</p><p>第一部分先基于 CogView2，<strong>通过文本生成几帧图像</strong>，这时候合成视频的帧率还很低；</p><p>第二部分则会基于双向注意力模型对生成的几帧图像进行<strong>插帧</strong>，来生成帧率更高的完整视频。</p><p></p><div class="img_box" id="id_imagebox_5" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_5" data-original="http://zkres1.myzaker.com/202206/6299a3eb8e9f0944d61ce3a7_1024.jpg" data-height="650" data-width="1080" src="https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202206/6299a3eb8e9f0944d61ce3a7_1024.jpg" referrerpolicy="no-referrer"></div></div>在训练上，CogVideo 一共用了<strong>540 万个</strong>文本 - 视频对。<p></p><p>这里不仅仅是直接将文本和视频匹配起来 " 塞 " 给 AI，而是需要先将视频拆分成几个帧，并额外给每帧图像添加一个帧标记。</p><p>这样就避免了 AI 看见一句话，直接给你生成几张一模一样的视频帧。</p><p>其中，每个训练的视频原本是 160 × 160 分辨率，被 CogView2 上采样（放大图像）至 480 × 480 分辨率，因此最后生成的也是 480 × 480 分辨率的视频。</p><p>至于 AI 插帧的部分，设计的双向通道注意力模块则是为了让 AI 理解前后帧的语义。</p><p></p><div class="img_box" id="id_imagebox_6" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_6" data-original="http://zkres1.myzaker.com/202206/6299a3eb8e9f0944d61ce3a8_1024.jpg" data-height="464" data-width="446" src="https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202206/6299a3eb8e9f0944d61ce3a8_1024.jpg" referrerpolicy="no-referrer"></div></div>最后，生成的视频就是比较丝滑的效果了，输出的 4 秒视频帧数在 32 张左右。<p></p><p><b>在人类评估中得分最高</b></p><p>这篇论文同时用数据测试和人类打分两种方法，对模型进行了评估。</p><p>研究人员首先将 CogVideo 在 UCF-101 和 Kinetics-600 两个人类动作视频数据集上进行了测试。</p><p>其中，FVD（Fr é chet 视频距离）用于评估视频整体生成的质量，数值越低越好；IS（Inception score）主要从清晰度和生成多样性两方面来评估生成图像质量，数值越高越好。</p><p>整体来看，CogVideo 生成的视频质量处于中等水平。</p><p>但从人类偏好度来看，CogVideo 生成的视频效果就比其他模型要高出不少，甚至在当前最好的几个生成模型之中，取得了最高的分数：</p><p></p><div class="img_box" id="id_imagebox_7" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_7" data-original="http://zkres2.myzaker.com/202206/6299a3eb8e9f0944d61ce3aa_1024.jpg" data-height="363" data-width="1080" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202206/6299a3eb8e9f0944d61ce3aa_1024.jpg" referrerpolicy="no-referrer"></div></div>具体来说，研究人员会给志愿者一份打分表，让他们根据视频生成的效果，对几个模型生成的视频进行随机评估，最后判断综合得分：<p></p><p></p><div class="img_box" id="id_imagebox_8" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_8" data-original="http://zkres2.myzaker.com/202206/6299a3eb8e9f0944d61ce3ab_1024.jpg" data-height="583" data-width="1080" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202206/6299a3eb8e9f0944d61ce3ab_1024.jpg" referrerpolicy="no-referrer"></div></div>CogVideo 的共同一作洪文逸和丁铭，以及二作郑问迪，三作 Xinghan Liu 都来自清华大学计算机系。<p></p><p>此前，洪文逸、丁铭和郑问迪也是 CogView 的作者。</p><p>论文的指导老师唐杰，清华大学计算机系教授，智源研究院学术副院长，主要研究方向是 AI、数据挖掘、机器学习和知识图谱等。</p><p>对于 CogVideo，有网友表示仍然有些地方值得探究，例如 DALL-E2 和 Imagen 都有一些不同寻常的提示词来证明它们是从 0 生成的，但 CogVideo 的效果更像是从数据集中 " 拼凑 " 起来的：</p><p></p><div class="img_box" id="id_imagebox_9" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_9" data-original="http://zkres1.myzaker.com/202206/6299a3eb8e9f0944d61ce3ac_1024.jpg" data-height="458" data-width="1080" src="https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202206/6299a3eb8e9f0944d61ce3ac_1024.jpg" referrerpolicy="no-referrer"></div></div>例如，狮子直接 " 用手 " 喝水的视频，就不太符合我们的常规认知（虽然很搞笑）：<p></p><p>（是不是有点像给鸟加上两只手的魔性表情包）</p><p></p><div class="img_box" id="id_imagebox_10" onclick><div class="content_img_div"><img class="lazy opacity_0 zaker_gif_cache" id="img_10" data-original="http://zkres1.myzaker.com/202206/6299a3eb8e9f0944d61ce3ae_1024.jpg" data-gif-url="http://zkres1.myzaker.com/202206/6299a3eb8e9f0944d61ce3ae_raw.gif" data-height="307" data-width="480" src="https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202206/6299a3eb8e9f0944d61ce3ae_1024.jpg" referrerpolicy="no-referrer"></div></div>但也有网友指出，这篇论文给语言模型提供了一些新思路：<p></p><p>用视频训练可能会进一步释放语言模型的潜力。因为它不仅有大量的数据，还隐含了一些用文本比较难体现的常识和逻辑。</p><p></p><div class="img_box" id="id_imagebox_11" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_11" data-original="http://zkres2.myzaker.com/202206/6299a3eb8e9f0944d61ce3af_1024.jpg" data-height="193" data-width="1080" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202206/6299a3eb8e9f0944d61ce3af_1024.jpg" referrerpolicy="no-referrer"></div></div>目前 CogVideo 的代码还在施工中，感兴趣的小伙伴可以去蹲一波了 ~<p></p><div id="recommend_bottom"></div><div id="article_bottom"></div>  
</div>
            