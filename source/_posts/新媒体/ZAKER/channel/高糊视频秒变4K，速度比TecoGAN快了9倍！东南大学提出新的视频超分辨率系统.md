
---
title: '高糊视频秒变4K，速度比TecoGAN快了9倍！东南大学提出新的视频超分辨率系统'
categories: 
 - 新媒体
 - ZAKER
 - channel
headimg: 'https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202107/60f0fba18e9f097e3a5e2023_1024.jpg'
author: ZAKER
comments: false
date: Fri, 16 Jul 2021 06:14:00 GMT
thumbnail: 'https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202107/60f0fba18e9f097e3a5e2023_1024.jpg'
---

<div>   
<p>把高糊视频变清晰，对于 AI 而言算不上新鲜事。</p><p>但如果是实时处理，而且速度比主流方法还快了<strong>9 倍</strong>呢？</p><p></p><div class="img_box" id="id_imagebox_0" onclick><div class="content_img_div"><img class="lazy opacity_0 zaker_gif_cache" id="img_0" data-original="http://zkres2.myzaker.com/202107/60f0fba18e9f097e3a5e2023_1024.jpg" data-gif-url="http://zkres2.myzaker.com/202107/60f0fba18e9f097e3a5e2023_raw.gif" data-height="576" data-width="704" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202107/60f0fba18e9f097e3a5e2023_1024.jpg" referrerpolicy="no-referrer"></div></div>而且计算量降低了，重建图像的质量却还非常能打：<p></p><p></p><div class="img_box" id="id_imagebox_1" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_1" data-original="http://zkres1.myzaker.com/202107/60f0fba18e9f097e3a5e2024_1024.jpg" data-height="334" data-width="1280" src="https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202107/60f0fba18e9f097e3a5e2024_1024.jpg" referrerpolicy="no-referrer"></div></div>这种图像质量和速度性能之间的平衡到底是怎么做到的？<p></p><p>今天就来看看东南大学的研究者们带来的最新研究：4K 视频实时超分辨率系统<strong>EGVSR</strong>。</p><p></p><div class="img_box" id="id_imagebox_2" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_2" data-original="http://zkres2.myzaker.com/202107/60f0fba18e9f097e3a5e2025_1024.jpg" data-height="176" data-width="1280" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202107/60f0fba18e9f097e3a5e2025_1024.jpg" referrerpolicy="no-referrer"></div></div>GAN 保证重建质量<p></p><p>为了使模型具有良好的感知质量，生成对抗网络 GAN 成为了超分辨率研究中广泛使用的一种方法。</p><p>比如，要处理 VSR 任务中大规模的分辨率退化，就常常依靠 GAN 的深度特征学习能力。</p><p>于是参考 TecoGAN 的设计，EGVSR 系统引入了空间 - 时间对抗结构，用来帮助判别器理解和学习空间 - 时间信息的分布。</p><p>也避免了传统 GAN 在时域遇到的不稳定效应。</p><p>同时，研究者参照高效 CNN 架构，为 EGVSR 设计了一个轻量级的网络结构：</p><p></p><div class="img_box" id="id_imagebox_3" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_3" data-original="http://zkres2.myzaker.com/202107/60f0fba18e9f097e3a5e2026_1024.jpg" data-height="278" data-width="634" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202107/60f0fba18e9f097e3a5e2026_1024.jpg" referrerpolicy="no-referrer"></div></div><strong>△</strong>EGVSR 生成器的部分框架<p></p><p>其中，生成器部分分为 FNet 模块和 SRNet 模块，分别用于光流估计和视频帧超分辨率。</p><p>接下来，就是增强 EGVSR 的实时处理能力了。</p><p>三种方法提升速度</p><p>研究者主要通过三种方法来提高网络训练和推理的速度。</p><p><strong>一、对 BN 层进行优化</strong>。</p><p>在 EGVSR 网络中，FNet 模块里大量使用了 BN（批量归一化）层。</p><p>因此，研究者省去计算 BN 的环节，将其转换为矩阵形式，利用 1 × 1 卷积层来实现和替换 BN 层：</p><p></p><div class="img_box" id="id_imagebox_4" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_4" data-original="http://zkres2.myzaker.com/202107/60f0fba18e9f097e3a5e2027_1024.jpg" data-height="298" data-width="624" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202107/60f0fba18e9f097e3a5e2027_1024.jpg" referrerpolicy="no-referrer"></div></div>优化之后，速度就提高了<strong>5%</strong>左右。<p></p><p><strong>二、寻找高效的上采样方法</strong>。</p><p>上采样层（Upsampling layer）是超分辨率网络中最重要的部分之一。</p><p>因此，在保持其他网络结构和配置的情况下，研究者希望从以下三种上采样方法中，选择出一种在实际 SR 网络中效率最高的：</p><p>A. 调整大小卷积（使用双线性插值）</p><p>B. 去卷积（Deconvolution）</p><p>C. 子像素卷积（Sub-pixel convolution）</p><p>在使用这三种方法训练了多组 SR 网络后，可以看到子像素卷积方法的效果最佳：</p><p></p><div class="img_box" id="id_imagebox_5" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_5" data-original="http://zkres2.myzaker.com/202107/60f0fba18e9f097e3a5e2028_1024.jpg" data-height="282" data-width="668" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202107/60f0fba18e9f097e3a5e2028_1024.jpg" referrerpolicy="no-referrer"></div></div><strong>三、设计一种适合硬件部署的高效卷积算法</strong><p></p><p>传统的朴素卷积（Nna ve Convolution）方法使用了 6 个循环结构，这导致它的计算效率相当低。</p><p>因此，研究者们使用矩阵乘法<strong>（MatMul）</strong>算法通过逆向 col2im 转换得到所需的输出特征结果。</p><p>这样，就将卷积计算转换为了矩阵乘法。</p><p>也就通过内存空间节省了推理时间，最终提高计算效率。</p><p>性能提升 7.92 倍</p><p>那么最终<strong>速度提升</strong>的效果如何呢？</p><p></p><div class="img_box" id="id_imagebox_6" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_6" data-original="http://zkres1.myzaker.com/202107/60f0fba18e9f097e3a5e2029_1024.jpg" data-height="441" data-width="1280" src="https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202107/60f0fba18e9f097e3a5e2029_1024.jpg" referrerpolicy="no-referrer"></div></div>可以看到，在使用 CPU 时，对比经典的 TecoGAN 算法，VESPCN 的速度最高能提升 9.05 倍。<p></p><p>而在使用 GPU 加速时，VESPCN 最高也能比 TecoGAN 的性能提升 7.92 倍。</p><p>如果从总计算成本来看，EGVSR 仅为 VESPCN 的 29.57%，SOFVSR 的 12.63%，FRVSR 和 TecoGAN 的 14.96%。</p><p>与此同时，EGVSR 也取得了<strong>较高的图像细节重建质量</strong>，结果最接近 GT（Ground Truth）图像：</p><p></p><div class="img_box" id="id_imagebox_7" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_7" data-original="http://zkres2.myzaker.com/202107/60f0fba18e9f097e3a5e202a_1024.jpg" data-height="974" data-width="1280" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202107/60f0fba18e9f097e3a5e202a_1024.jpg" referrerpolicy="no-referrer"></div></div>而对于多张图像之间的<strong>连贯性</strong>评估，研究者们引入了两个指标来衡量 VSR 结果与相应的 GT 参考结果之间的差异：<p></p><p>tOF：测量从序列中估计的运动的像素差异；</p><p>tLP：使用深度特征图测量感知上的变化。</p><p>从结果可以看到 VESPCN 的分数最小：</p><p></p><div class="img_box" id="id_imagebox_8" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_8" data-original="http://zkres2.myzaker.com/202107/60f0fba18e9f097e3a5e202b_1024.jpg" data-height="456" data-width="1280" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202107/60f0fba18e9f097e3a5e202b_1024.jpg" referrerpolicy="no-referrer"></div></div>这说明了在满足时间连贯性的情况下，EGVSR 网络可以恢复更多的空间细节，满足人眼的主观感受。<p></p><p>所有实验的结果都表明，EGVSR 确实在保证 VSR 高视觉质量的前提下，将计算负载降低到最低要求，完成了 4K VSR 在硬件平台上的实时实现。</p><div id="recommend_bottom"></div><div id="article_bottom"></div>  
</div>
            