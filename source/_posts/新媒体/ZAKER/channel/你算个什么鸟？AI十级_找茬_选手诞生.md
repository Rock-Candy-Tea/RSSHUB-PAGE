
---
title: '你算个什么鸟？AI十级_找茬_选手诞生'
categories: 
 - 新媒体
 - ZAKER
 - channel
headimg: 'https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202107/6104d7e68e9f097b01789733_1024.jpg'
author: ZAKER
comments: false
date: Mon, 02 Aug 2021 00:45:00 GMT
thumbnail: 'https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202107/6104d7e68e9f097b01789733_1024.jpg'
---

<div>   
<p>你算个什么鸟？</p><p></p><div class="img_box" id="id_imagebox_0" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_0" data-original="http://zkres2.myzaker.com/202107/6104d7e68e9f097b01789733_1024.jpg" data-height="334" data-width="738" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202107/6104d7e68e9f097b01789733_1024.jpg" referrerpolicy="no-referrer"></div></div>面对上面这两张图，一个 AI 发出了灵魂拷问。<p></p><p>左边<strong>桃面牡丹鹦鹉</strong>，右边<strong>费氏牡丹鹦鹉</strong>。</p><p>一眼识破的它早就看到左边的鸟的喙部和眼圈与右边的不一样。</p><p>不行，再来！再来看这组。（文末揭晓答案）</p><p></p><div class="img_box" id="id_imagebox_1" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_1" data-original="http://zkres1.myzaker.com/202107/6104d7e68e9f097b01789734_1024.jpg" data-height="364" data-width="1250" src="https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202107/6104d7e68e9f097b01789734_1024.jpg" referrerpolicy="no-referrer"></div></div>好，我放弃了。<p></p><p></p><div class="img_box" id="id_imagebox_2" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_2" data-original="http://zkres2.myzaker.com/202107/6104d7e68e9f097b01789735_1024.jpg" data-height="225" data-width="225" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202107/6104d7e68e9f097b01789735_1024.jpg" referrerpolicy="no-referrer"></div></div>这个来自浙大计算机学院和阿里安全的 " 找茬 " 选手，识别准确率达到了<strong>91.3%</strong>，已经是业内最优水平。研究成果已被多媒体国际顶会 ACM MM 2021 收录。<p></p><p>不光鸟，阿猫阿狗也能行，甚至花草植物也能行。</p><p>看看这连两张照片，吉娃娃还是英国玩具梗？</p><p></p><div class="img_box" id="id_imagebox_3" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_3" data-original="http://zkres1.myzaker.com/202107/6104d7e68e9f097b01789736_1024.jpg" data-height="422" data-width="1160" src="https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202107/6104d7e68e9f097b01789736_1024.jpg" referrerpolicy="no-referrer"></div></div>再来看这一波，羊驼还是美洲驼？驴还是骡？玫瑰还是羽衣甘蓝？<p></p><p></p><div class="img_box" id="id_imagebox_4" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_4" data-original="http://zkres1.myzaker.com/202107/6104d7e68e9f097b01789737_1024.jpg" data-height="529" data-width="1280" src="https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202107/6104d7e68e9f097b01789737_1024.jpg" referrerpolicy="no-referrer"></div></div>AI 好眼力！那到底是如何练成的？<p></p><p>AI 如何练就的一副好眼力？</p><p>实际上，这涉及到计算机视觉领域一个经典问题——<strong>细粒度图像识别</strong>，让 AI 一眼锁定类别之间的细微差异。</p><p>看起来简单，实际不简，就比如下面左边这俩。</p><p></p><div class="img_box" id="id_imagebox_5" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_5" data-original="http://zkres1.myzaker.com/202107/6104d7e68e9f097b01789738_1024.jpg" data-height="367" data-width="907" src="https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202107/6104d7e68e9f097b01789738_1024.jpg" referrerpolicy="no-referrer"></div></div>对于 AI 来说，区域注意力的定位和放大是保证识别准确率一个重要因素，此前大量基于 CNN 的探索发现，<strong>CNN 的感受野有限，且缺乏全局依赖关系的建模能力</strong>。<p></p><p>感受野：网络内部的不同位置的神经元对原图像的感受范围</p><p>研究人员认为，与 CNN 相比，<strong>图像序列化</strong>是一种全新的方式。</p><p>他们把目光转向了最近在 CV 领域取得了非常多研究进展的视觉 Transformer（ViT）。</p><p>一开始，研究人员引入了 ViT 中的自注意力机制，提取图像中的长距离依赖关系。</p><p>不过 ViT 的感受野大小相对固定，对图像中的每个 patch 的关注程度<strong>没有产生区分</strong>，也就给细粒度图像识别带来了性能局限。</p><p>既然如此，那该如何让 AI<strong>找准 " 重点 "</strong>呢？</p><p>研究人员决定使用<strong>注意力权重的强度</strong>来衡量对应于原始图像的 patch 重要性，提出了<strong>多尺度循环注意力</strong>的 Transformer（RAMS-Trans）。</p><p>它利用 Transformer 的自注意力机制，以多尺度的方式循环地学习判别性区域注意力。</p><p>团队成员之一，阿里安全图灵实验室算法专家炫谦介绍道 :</p><p>我们方法的核心是<strong>动态 patch 建议模块</strong>（DPPM）引导区域放大，以完成多尺度图像 patch 块的集成。</p><p>DPPM 从全局图像开始，迭代放大区域注意力，以每个尺度上产生的注意力权重的强度为指标，从全局到局部生成新的 patch 块。</p><p></p><div class="img_box" id="id_imagebox_6" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_6" data-original="http://zkres1.myzaker.com/202107/6104d7e68e9f097b01789739_1024.jpg" data-height="924" data-width="1196" src="https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202107/6104d7e68e9f097b01789739_1024.jpg" referrerpolicy="no-referrer"></div></div>具体来说，首先提取 ViT 每层的自注意力机制，并进行归一化，然后采取累乘的方式对自注意力整合。<p></p><p>然后，得到了整合后的自注意力均值分布矩阵，由于细粒度图像识别任务的关键因素在于局部注意力，其往往存在于图像的局部区域，如鸟的尾部、喙和蛙类的头部等。</p><p>因此研究者需要通过设定<strong>阈值</strong>的方式来 " 过滤 " 不需要的部位，增强对局部判别性区域的识别能力。</p><p>最后，研究者通过插值算法将选定的 patch 块<strong>放大到原图像的尺寸</strong>，通过共享参数的模型，重新进行训练，整体结构对应于文章所提的多尺度循环机制。</p><p>下图为 RAMS-Trans 在识别鸟类时根据注意力权重生成的注意图（attention map）。</p><p></p><div class="img_box" id="id_imagebox_7" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_7" data-original="http://zkres1.myzaker.com/202107/6104d7e68e9f097b0178973a_1024.jpg" data-height="792" data-width="886" src="https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202107/6104d7e68e9f097b0178973a_1024.jpg" referrerpolicy="no-referrer"></div></div><strong>△</strong>第二、三行分别为从原始和重新训练过的注意权重生成<p></p><p>扩展到更多动物身上的效果：</p><p></p><div class="img_box" id="id_imagebox_8" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_8" data-original="http://zkres2.myzaker.com/202107/6104d7e68e9f097b0178973b_1024.jpg" data-height="882" data-width="984" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202107/6104d7e68e9f097b0178973b_1024.jpg" referrerpolicy="no-referrer"></div></div>战绩如何？<p></p><p>RAMS-Trans 只需要 ViT 本身附带的注意力权重，就可以很容易地进行端到端的训练。</p><p>实验表明，除了高效的 CNN 模型外，RAMS-Trans 的表现比同期进行的工作更好，分别在 CUB-200-2011（鸟类识别）、Stanford Dogs（狗类识别）、iNaturalist2017（动植物识别）获得 SOTA。</p><p>分别达到 91.3%、68.5%、92.4% 的识别准确率。</p><p></p><div class="img_box" id="id_imagebox_9" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_9" data-original="http://zkres1.myzaker.com/202107/6104d7e68e9f097b0178973c_1024.jpg" data-height="418" data-width="736" src="https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202107/6104d7e68e9f097b0178973c_1024.jpg" referrerpolicy="no-referrer"></div></div>在不同种类动植物的细粒度判别时，RAMS-Trans 可以聚焦到类别的独特特征区域。<p></p><p></p><div class="img_box" id="id_imagebox_10" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_10" data-original="http://zkres1.myzaker.com/202107/6104d7e68e9f097b0178973d_1024.jpg" data-height="543" data-width="1280" src="https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202107/6104d7e68e9f097b0178973d_1024.jpg" referrerpolicy="no-referrer"></div></div><strong>△</strong>第二、四、六行分别为放大到原图像尺寸的的 patch 块<p></p><p>针对不同类别识别准确率不同，甚至还有较大的区别，一作浙大博士胡云青解释道，主要有两方面的因素。</p><p>一是因为 Stanford Dogs 本身的类别数比其他两个数据集都要小。只有 120 分类（CUB 是 200，而 iNaturaList 更是达到了 5089）。</p><p>类别数越多，通常意味着该数据集的细粒度问题越严重，因此 RAMS-Trans 在更细粒度的数据集上取得的提升相对明显。</p><p>二则因为在某个类别上大部分样本具有相似的特征，而不同种类间的狗也具有明显的判别性特征。</p><p>比如大部分博美都有相似的毛色和头型；德牧和金毛之间，人眼就可以做到明显区分。</p><p>通过消融实验发现，当分辨率为 320、阈值为 1.3、patch 方案为 DPPM、patch 块大小为 16x16 时，模型效果最好。</p><p></p><div class="img_box" id="id_imagebox_11" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_11" data-original="http://zkres1.myzaker.com/202107/6104d7e68e9f097b0178973e_1024.jpg" data-height="530" data-width="1114" src="https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202107/6104d7e68e9f097b0178973e_1024.jpg" referrerpolicy="no-referrer"></div></div>接下来，团队还将在两个方面进行优化：<p></p><p>提高定位能力。</p><p>目前，RAMS-Trans 在原图上以 patch 为最小单位进行判别性区域定位和放大的过程，对于细粒度图像识别来说，这个较为 " 精细 " 的任务来说还是相当粗旷。</p><p>动态网络的引入，包括动态训练和动态推理等。</p><p>另外，已经有了可预见的应用场景，比如野生动物保护治理、山寨商标的识别。</p><p></p><div class="img_box" id="id_imagebox_12" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_12" data-original="http://zkres1.myzaker.com/202107/6104d7e68e9f097b0178973f_1024.jpg" data-height="360" data-width="462" src="https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202107/6104d7e68e9f097b0178973f_1024.jpg" referrerpolicy="no-referrer"></div></div>浙大 & 阿里安全<p></p><p></p><div class="img_box" id="id_imagebox_13" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_13" data-original="http://zkres2.myzaker.com/202107/6104d7e68e9f097b01789740_1024.jpg" data-height="395" data-width="1280" src="https://cors.zfour.workers.dev/?http://zkres2.myzaker.com/202107/6104d7e68e9f097b01789740_1024.jpg" referrerpolicy="no-referrer"></div></div>这项研究主要由浙江大学计算机学院、阿里安全图灵实验室共同完成。<p></p><div id="recommend_bottom"></div><div id="article_bottom"></div>  
</div>
            