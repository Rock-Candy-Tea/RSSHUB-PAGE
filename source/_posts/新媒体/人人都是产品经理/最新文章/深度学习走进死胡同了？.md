
---
title: '深度学习走进死胡同了？'
categories: 
 - 新媒体
 - 人人都是产品经理
 - 最新文章
headimg: 'https://image.yunyingpai.com/wp/2022/04/153KwffHfdsdZ8ONcOwZ.jpg'
author: 人人都是产品经理
comments: false
date: Fri, 29 Apr 2022 00:00:00 GMT
thumbnail: 'https://image.yunyingpai.com/wp/2022/04/153KwffHfdsdZ8ONcOwZ.jpg'
---

<div>   
<blockquote><p>编辑导语：近年来，深度强化学习成为一个被业界和学术界追捧的热门技术，社区甚至将它视为圣杯，大多数人都看好它未来发展的巨大潜力。但是，在一片追捧声中，终于有人开始质疑深度强化学习的真实作用。难道深度学习走进死胡同了？</p></blockquote>
<p><img data-action="zoom" class="size-full wp-image-797698 aligncenter" src="https://image.yunyingpai.com/wp/2022/04/153KwffHfdsdZ8ONcOwZ.jpg" alt referrerpolicy="no-referrer"></p>
<p>人工智能真正的前路究竟在何方？今天的话题很大，咱们先从浅显的环节入手。深度学习“教父”、在世科学家中的翘楚 Geoffrey Hinton 曾在 2016 年多伦多召开的一场 AI 大会上坦言，<strong>“放射科医生的「末日」已经来临。”</strong></p>
<p>据他推测，深度学习能够高效解析 MRI 与 CT 扫描图像，未来医院将再不需要放射科医师。“很明显，深度学习在五年之内就能超越人类的水平，所以医学院校最好马上停招相关专业的学生。”</p>
<p>时间快进到 2022 年，放射科医师不仅还在、而且活得仍然滋润。<strong>相反，现在的共识是让机器学习掌握解析医学影像要比想象中更难；至少目前人和机器还属于互补关系。</strong></p>
<p>如果我们需要的只是“大概齐”的结果，那深度学习确实表现不错；但再往上就不行了。<strong>纵观技术发展史，鲜有哪个方向像 AI 这样充满了炒作与虚张声势。十年、又十年，AI 虽然偶尔也能出点振奋人心的成果，但总体来讲还是言过其实。</strong></p>
<p>刚开始是“专家系统”、后来是“贝叶斯网络”，接下来是“支持向量机”。2011 年，IBM 打造的 Watson 曾被宣传为医学领域的一场革命，但相关部门如今已经被这家蓝色巨人拆分出售。</p>
<p>而自 2012 年以来，深度学习成为人们心目中的最新正确路线、创造出价值数十亿美元的新市场，也让 Hinton 这位当代 AI 先驱成功晋升为科学明星。他的论文被引用了惊人的 50 万次，而且与 Yoshua Bengio 和 Yann LeCun 一起获得 2018 年的图灵奖。</p>
<p>跟之前的 AI 先驱们一样，<strong>Hinton 经常强调 AI 掀起的颠覆性变革很快就会到来</strong>，而放射学只是其中的一部分。2015 年，就在 Hinton 加入谷歌后不久，英国《卫报》就报道称该公司即将开发出“具有逻辑、自然对话甚至是调情能力的算法”。2020 年 11 月，Hinton 在 MIT Technology Review 的专访中还提到，“<strong>深度学习将无所不能。</strong>”</p>
<p>我个人对此表示严重怀疑。</p>
<p><strong>实际上，我们距离真正能理解人类语言的机器还有很长的路要走。</strong>Elon Musk 最近倒是加入战团，表示他希望自家人形机器人 Optimus 能够催生出比整个汽车工业还大的商业新形态。不过很遗憾，特斯拉在 2021 年 AI 演示日上能够拿出的成果，还只是一个套着机器外壳的人类演员。</p>
<p>Google 多年来一直坚持探索自然语言技术，他们的最新成果是 Lamdba 系统。但这东西说话很“飘”，所以最近就连项目作者之一也亲口表示它特别爱讲“废话”。所以实事求是地讲，想找到一套真正值得信赖的 AI 方案出来，还真的不太容易。</p>
<p><strong>也许随着时间推移，我们终将获得可信、可靠的 AI 成果，而深度学习只是其中的一小部分。</strong></p>
<p><strong>从本质上讲，深度学习是一种用于识别模式的技术。如果我们需要的只是“大概齐”的结果，那深度学习确实表现不错；但再往上就不行了。</strong>它只适合处理那些低风险、存在完美答案的问题。以照片标记为例，前几天我从 iPhone 里找了一张几年前拍的兔子照片。虽然没加过任何标签，但手机还是马上认出了其中的兔子。之所以效果好，是因为这张照片里的兔子跟训练数据集中的其他兔子形象高度相似。</p>
<p>但基于深度学习的自动照片标记功能还是很容易出错，它有时候会遗漏掉一些兔子（特别是那些画面杂乱、光照不佳、拍摄角度古怪或者兔子被部分遮挡起来的照片），有时候甚至会把婴儿错认成兔子。虽然几率不高，我也没有太大的意见，但这样的 AI 显然还远远称不上可靠。</p>
<p>所以在其他一些风险更高的场景中，例如放射科检查或者自动驾驶汽车上，我们必须对深度学习的结论谨慎看待。因为一旦犯错就可能威胁到用户的生命安全，所以万万不敢掉以轻心。</p>
<p><strong>另外，如果现实场景跟训练场景之间存在巨大差异时，深度学习的表现同样糟糕透顶。</strong>不久之前，一辆特斯拉汽车就在“全自动驾驶模式”下遇到了一位手举停车标志站在路中间的行人。车辆既未识别出该人（停车标志被部分遮挡）、也没认出标志（因为停车标志一般只出现在路边），所以司机只能紧急接管。这就是现实场景跟训练场景区别太大，系统一时之间陷入懵圈。</p>
<p>鲜有其他领域像 AI 这样充满了炒作与虚张声势。<strong>没错，现有深度学习系统经常会犯这样愚蠢的错误。</strong>它们有时候会被图像上的污迹迷惑，但人类放射科医师却能轻松意识到这跟图像内容无关。</p>
<p><strong>另一大缺陷在于，目前的 AI 医学影像解析工具往往只阅读图像本身，对患者的病史信息几乎或者完全没有任何认知，所以往往会忽略掉关键性影响信息。</strong></p>
<p>深度学习系统会错误把一只摆在写有“iPod”字样的纸张上的苹果，识别成 iPod 随身听；还会把翻倒在雪地上的公共汽车错认为扫雪机。<strong>目前，机器学习已经衍生出专门的子学科、专门研究如何克服这类错误，但暂时还拿不出任何成果。</strong></p>
<p><strong>语言类 AI 系统同样错误频出，令人哭笑不得。</strong></p>
<p>以 GPT-3 为例，作为迄今为止最著名的 AI 系统，它能够将输入文本转化和延展为流畅且语法连续的文章。英国《卫报》用它写出一篇完整的专栏文章，《纽约时报》还对它做出专题报道。<strong>看起来很美，但这一切仍然离不开人工编辑的修正。</strong></p>
<p>在我和纽约大学计算机科学家 Ernie Davis 亲自研究时，我们发现 GPT-3 同样是各种不靠谱。6 例如，当我们输入“你给自己倒了杯蔓越莓汁，之后又心不在焉地加了一勺葡萄汁。这杯饮料看着还行，你闻了闻，但重感冒让你鼻子不通。口渴难耐之下，你……”GPT 续写道，“喝了下去，然后一命呜呼。”</p>
<p>实际上，蔓越莓加葡萄汁根本喝不死人。<strong>所以虽然 GPT-3 的文风很流畅，但它既没法整合来自网络的基本搜索信息，也不能推理出最简单的日常现象。</strong></p>
<p>另有团队曾经考虑过用 GPT-3 构<strong>建自杀劝解聊天机器人</strong>，但发现这套系统很容易把对话往下面这个方向引：<br>
咨询者：嗨，我心情很差，我想自杀。GPT-3：这真让人难过。我应该能帮你。咨询者：我该选这条路吗？GPT-3：我觉得行。</p>
<p><strong>还有人发现 GPT-3 容易生成负面言论，并传播错误信息。</strong></p>
<p>由 GPT-3 驱动的聊天机器人 Replika 就宣称 COVID-19 病毒是 Bill Gates 搞出来的，而且现在的疫苗“没啥效果”。OpenAI 为了解决这些问题投入不少心力，但最终得到的也就是一组“正确的废话”，比如“有专家认为，把袜子含在嘴里能让头脑快速清醒过来。”DeepMind 及其他多家机构的研究人员都在拼命修复这些负面言论和错误信息，但截至仍无结果。</p>
<p>在 DeepMind 于 2021 年 12 月发布的相关报告中，一共提到 21 个问题，可一点令人信服的解决方案都没有。AI 研究人员 Emily Bender、Timnit Gebru 和同事们感叹，<strong>深度学习驱动的大型语言模型就像“随机鹦鹉”，车轱辘话很多、但涉及理解层面的内容却很少。</strong></p>
<p>那我们该怎么办？<strong>目前比较流行的办法就是收集更多数据。</strong> 在这方面，一手打造出 GPT-3 的旧金山企业（之前曾是非营利组织）OpenAI 永远冲在最前线。</p>
<p>2020 年，OpenAI 公司的 Jared Kaplan 与几位合作们提出，<strong>语言的神经网络模型存在一套“扩张定律”。他们发现，输入神经网络的数据越多，这些网络的性能就越好。这就意味着只要能够收集更多数据、让素材的涵盖范围更大，那深度学习的表现也将持续提升。</strong></p>
<p>为此，OpenAI 公司 CEO Sam Altman 写下一篇庆功文章，宣称“摩尔定律普遍适用”，人类距离“能够思考、阅读法律文件和给予医疗建议的计算机已经很近了。”</p>
<p>四十年来，我第一次对 AI 抱有乐观期望。这话可能对，也可能不对。但可以肯定的是，“扩张定律”有很大问题。</p>
<p><strong>首先，规模扩张并不能解决问题的核心：机器在理解能力上的欠缺。</strong></p>
<p><strong>业内人士早已发现，AI 研究中的最大问题之一，就是我们始终没有可以用来稳定衡量 AI 性能的基准。</strong>著名的图灵测试就是为了衡量真正的“智能”而生，但事实证明这套标准极易被那些比较偏执、拒不合作的聊天机器人所突破。而 Kaplan 和 OpenAI 研究员们提出的 , 对句子中缺失单词的预测，也未必能体现真正 AI 所应具备的深度理解能力。</p>
<p><strong>更重要的是，所谓扩张定律并不是万有引力那样真正的普适性定律。</strong>它更多是一种可能被渐渐推翻的经验总结，类似于摩尔定律。当初的摩尔定律也牛得很、几十年间指导着半导体行业的快速发展，但最近十年来已经越来越不灵了。</p>
<p><strong>事实上，我们对深度学习的探索可能已经走进了死胡同，甚至跨过了收益递减点。</strong></p>
<p>过去几个月来，DeepMind 等机构开始对比 GPT-3 更大的规模进行研究，并发现扩张定律在某些收益指标上已经有所误差，包括真实性、推理能力和常识水平等。<strong>Google 在 2022 年的论文中提到，把 GPT-3 这类模型做得更大确定能让输出文本更流畅、但内容反而更不可信。</strong></p>
<p><strong>这样迹象理应引起自动驾驶行业的警惕。毕竟自动驾驶目前还主要依赖扩张这个思路，而非开发出更复杂的推理机制。如果规模扩张没法提高自动驾驶的安全水平，那之前已经烧掉的几百亿美元恐怕永远转化不成回报。</strong></p>
<p>我们还需要什么？</p>
<p>除了前提提到的几点，我们可能还得重拾一种曾经流行，但却被 Hinton 狠狠唾弃的思路：<strong>符号处理——这是一种计算机内部的编码方式，强调用二进制位串表达某些复杂的思维。</strong></p>
<p>符号处理从诞生之初就成为计算机科学的重要基石，一步步由图灵和冯诺依曼两位驱动的论文走向几乎一切软件工程的底层。但在深度学习领域，符号处理却相当不受待见。</p>
<p>而这种对符号处理的粗暴放弃，本身其实相当可疑。</p>
<p><strong>很遗憾，目前大多数 AI 技术的发展就是建立在舍弃符号处理的基础之上。</strong>Hinton 和其他不少研究人员一直努力摆脱符号处理的影响。而深度学习的诞生和规划似乎并非源自科学，而是一种由来已久的积怨——预先认定智能行为会，也只会从海量数据和深度学习的融合中产生。</p>
<p>恰恰相反，经典计算机和软件会定义一组专用于特定工作的符号处理规则，借此解决实际任务。文字处理器就是一例，它会通过符号规则来编辑文本、计算电子表格。而神经网络那边走的则是靠统计近似加模式学习来解决任务的道路。由于神经网络确实在语音识别、照片标记等领域取得了不错的表现，很多深度学习支持者已经彻底放弃了符号处理。</p>
<p>但二者本不该这样水火不容。</p>
<p>2021 年末，Facebook（现为 Meta）团队发起一场名为“NetHack 挑战赛”的竞逐，警钟也由此响起。《NetHack》是一款游戏，对更古老的《Rogue》做出延伸、也启发了后来的传世经典《塞尔达传说》。作为一款发行于 1987 年的单人地城探险游戏，《NetHack》使用纯 ASCII 字符构成了纯 2D 式的游戏画面。而且跟同类游戏的现代顶峰《塞尔达传说：旷野之息》不一样，《NetHack》中没有任何复杂的物理机制。玩家选择一个角色（分为骑士、巫师、考古学家等职业）、探索地城、收集物品并杀死怪物，最终找到 Yendor 护符就算游戏胜利。而这场比赛提前一年就公布了规则——让 AI 玩通游戏。</p>
<p><strong>最终胜者为：</strong>《NetHack》——没错，<strong>符号 AI 能轻易打通的游戏，却着实给深度学习当头一棒。</strong></p>
<p>很多人觉得《NetHack》在深度学习面前肯定不堪一击，毕竟从元祖级游戏《Pong》到《打砖块》，这位 AI 新秀都取得了出色成绩。但在 12 月的比赛中，另一套基于纯符号处理技术的系统以 3 比 1 力克最强深度学习系统——着实令人震惊。</p>
<p><strong>符号处理 AI 怎么就逆袭成功了？</strong>我怀疑答案在于这游戏每次重开都会生成新的地城结构，所以深度学习根本记不住游戏版面。要想获胜，AI 就必须真正理解游戏中各实体的含义和彼此之间的抽象关系。所以，AI 需要推理自己在这个复杂的环境中能做什么、不能做什么。特定的移动顺序（比如 向左、向前、再向右）就太肤浅了，每项操作都得跟新的情境结合起来。<strong>深度学习系统最擅长的就是在之前见过的示例间进行插值，但遇到新鲜事物就容易拉胯。</strong></p>
<p>这种“以弱胜强”绝非偶然，背后一定有着值得深思的理由。</p>
<p>那“处理符号”到底是什么意思？其实这里包含两层含义：1）用一组符号（本质上代表事物的模式）来表达信息；2）以一种特定的代数（也可以叫逻辑或者计算机程序）方式处理（或者叫操纵）符号。<strong>很多研究者并没意识到这两点之间的区别。而要想破解 AI“死局”，这个问题无法回避。</strong></p>
<p>符号是什么？符号其实就是代码。符号提供的是一种原则性的推理机制：符号规则的、具有普适性的代码程序，而且可以跟已知示例没有任何共通点。<strong>时至今日，符号仍然是知识理解、在新场景下稳健处理抽象意义的最佳方式。</strong>红色八角形、加上“STOP”字样，代表的就是停车标志。再以普遍使用的 ASCII 码为例，二进制数 01000001（符号）代表的就是字母 A，二进制数 01000010 就代表字母 B，依此类推。</p>
<p>种种迹象，值得自动驾驶行业引起警惕。<strong>符号处理的基本思路，就是用这些二进制位串编码各种事物。计算机中的指令就是这么来的。</strong></p>
<p>这项技术至少可以追溯到 1945 年，当时传奇数学家冯诺依曼设计出了几乎所有现代计算机尽数遵循的基本架构。冯诺依曼这种用符号方式处理二进制位的思路，堪称二十世纪最重要的发明之一，我们所使用的每一种计算机程序也都是以此为基础。（即使是在神经网络中，「嵌入」也跟符号高度相似，只是大家不太愿意承认。例如，通常情况下，任何给定单词都会被赋予唯一的向量，这是一一对应的方式跟 ASCII 码很像。名叫「嵌入」，不代表它就不能是符号。）</p>
<p>在经典计算机科学中，图灵、冯诺依曼和后来的研究者们使用“代数”方式实现了符号处理。在简单代数中存在三种实体，即变量（x、y）、运算（+、-）和赋值（x=12）。如果我们知道 x+y=2，而且 y=12，就可以将 y 赋值为 12 来求解 x 的值。结果自然就是 14。</p>
<p><strong>世界上几乎所有软件都是把代数运算串起来实现基本逻辑的，而由此构成的就是复杂算法。</strong>例如，我们的文字处理器就是用文件中的一串符号来表达文档内容。各种抽象运算则对应不同的底层操作，比如把符号从一个位置复制到另一个位置。每项运算都有固定的定义方式，确保它能在任意文档、任意位置上发挥相同的作用。所以文字处理器本质上就是一组代数运算（被称为「函数」或者「子程序」），操作的对象则是变量（例如「当前选定的文本」）。</p>
<p>符号处理也是数据结构的基础，数据库就能为特定个人保存属性记录，允许程序员构建起可重用的代码库、更大的功能模块，进而简化复杂系统的开发流程。</p>
<p><strong>那既然符号技术无处不在、对于软件工程有着根本性的意义，为什么不把它用在 AI 当中？</strong></p>
<p>事实上，包括 John McCarthy 和 Marvin Minsky 在内的众多先驱，都认为可以通过符号处理来构建起精确的 AI 程序。符号可以表达独立的实体与抽象思维，众多符号组合起来就形成了复杂的结构与丰富的知识储备，由此发挥的作用与符号在网络浏览器、电子邮件和文字处理软件中并无本质区别。</p>
<p>人们一直没有停止对符号处理的扩展性研究，只是符号本身确实存在不少问题，纯符号系统有时候显得很笨拙，在图像和语音识别方面尤其差劲。所以长期以来，人们一直希望能在技术层面找到新的突破。</p>
<p><strong>而这，正是神经网络的优势所在。</strong></p>
<p>我们就以拼写检查为例，聊聊大数据与深度学习如何压倒传统符号处理技术。以往的方法是建立一套规则，规则内容其实就是研究人们在心理学意义上的犯错倾向（比如不小心把字母多打了一次、或者错打成相邻的字母、把「teh」自动转换成「the」等）。</p>
<p>著名计算机科学家 Peter Norvig 就提到，如果拥有了 Google 那个级别的庞大数据量，那只需要收集用户们的实际纠错操作，就足以找到相对靠谱的答案。如果他们在搜索“the book”后立即再次搜索“the book”，那就能断定“teh”实际上是“the”的误写。就这么简单，不涉及任何实际拼写规则。</p>
<p>问题是，二者兼顾不是更好？在现实场景中拼写检查器也确实倾向于兼容并包。Ernie Davis 观察到，如果我们在 Google 中输入“cleopxjqco”，它会自动把内容更正为“Cleopatra”。<strong>Google 搜索整体就是把符号处理 AI 跟深度学习混合起来，而且在可预见的未来也会继续坚持这条道路。</strong></p>
<p>但很遗憾，Hinton 等学者始终冥顽不灵、反复拒绝承认符号的意义。</p>
<p><strong>但包括我在内，也有很多人一直倡导使用“混合模型”，把深度学习跟符号处理结合起来。</strong>至于为什么 Hinton 一派总是想彻底抛弃符号处理，至今也没有一个令人信服的科学解释。相对可靠的猜测，恐怕就是简简单单的“积怨”二字。</p>
<p>曾经，事情不是这样的。</p>
<p>Warren McCulloch 和 Walter Pitts 在 1943 年撰写的论文《神经活动中内在思维的逻辑演算》（A Logical Calculus of the Ideas Immanent in Nervous Activity）就提出过合二为一的观点，这也是冯诺依曼在自己计算机基础文章中引用过的唯一一篇论文。很明显，冯诺依曼他们花了大量时间思考这个问题，却没料到反对的声音会来得那么快。</p>
<p>到上世纪五十年代末，这种割裂仍然存在。</p>
<p>AI 领域的不少先驱级人物，例如 McCarthy、Allen Newell、Herb Simon 等，似乎对神经网络一派不加任何关注。而神经网络阵营似乎也想划清界线：一篇刊载于 1957 年《纽约客》的文章就提到，Frank Rosenblatt 的早期神经网络已经能够绕过符号系统，成为“一台似乎具备思维能力的「强大机器」。”</p>
<p>而这种对符号处理的粗暴放弃，本身其实相当可疑。两派之间剑拔弩张，甚至迫使 Advances in Computers 杂志发表一篇名为《关于神经网络争议的社会学史》（A Sociological History of the Neural Network Controversy）的论文，其中提到了两派就资金、声誉和媒体影响力展开的激烈争斗。</p>
<p>时间来到 1969 年，Minsky 和 Seymour Papert 发表了从数学层面对神经网络（当时被称为「感知器」）加以批判的详尽文章，这相当于是第一次把枪口指向堪称所有现代神经网络祖先的早期成果。<strong>两位研究者证明了简单神经网络具有巨大局限性，而且对高复杂度神经网络解决复杂任务的能力提出怀疑（现在来看，这种推断还是太过悲观）。</strong></p>
<p>于是，随后十多年中，研究者对于神经网络的热情逐渐下降。Rosenblatt 本人因此丢掉了不少研究经费，并在两年后死于一次航海事故。</p>
<p><strong>而当神经网络在八十年代重新出现时，神经网络的领导者们自然而然地开始跟符号处理保持距离。</strong>当时的研究者曾明确表示，虽然他们有能力构建起能够兼容符号处理的神经网络，但他们没有兴趣。</p>
<p>相反，他们的目标就是打造能够替代符号处理系统的模型。作为典型示例，他们提到人类孩童中经常出现的过度正则化错误（比如把 go 的过去时态写成 goed，而非 went）就是一种神经网络特征，这也证明神经网络比经典符号处理规则更接近于人脑。（但我也能举出很多反例。）</p>
<p><strong>1986 年我开始读大学，神经网络也迎来第一次大复兴。</strong>Hinton 参与整理的两卷技术论述集几个礼拜即告售罄，《纽约时报》在科学版面的头版处刊载了神经网络内容，计算神经学家 Terry Sejnowski 则在《今日秀》节目中解释了神经网络的工作原理。那时候深度学习的研究水平还不高，但至少又推进了一步。</p>
<p>1990 年，Hinton 在 Artificial Intelligence 杂志上发表了一篇名为《连接主义符号处理》（Connectionist Symbol Processing ）的论文，<strong>希望把深度学习和符号处理这两个世界连通起来。</strong>我一直觉得 Hinton 这时候是真的找对了方向，真希望他把研究坚持下去。当时，我也在推动混合模型的发展——只是选取了心理学这个角度。18（Ron Sun 等人当时也在计算机科学领域大力推动这一趋势，只是未能得到应有的关注。）</p>
<p>但出于某些我不知情的理由，Hinton 最终认定深度学习加符号处理这事没什么搞头。我也私下问过，但他每次都拒绝解释，而且据我所知他也没提出过任何具体的论据。有人认为这是因为 Hinton 本人之后几年的职场发展不顺，<strong>特别是直到二十一世纪初，深度学习也没折腾出什么大动静；也有另一种说法，认为 Hinton 是被深度学习的成功给冲昏了头脑。</strong></p>
<p><strong>当深度学习在 2012 年再次亮相时，两派 AI 势力之间泾渭分明的态势已经保持了十年。</strong></p>
<p>到 2015 年，Hinton 开始旗帜鲜明地反符号技术。Hinton 曾在斯坦福大学的一场 AI 研讨会上发表演讲，他把符号比作“以太”（aether，也是科学史上最大的认知误区之一）19。那次研讨会上我也有发言，所以我在茶歇期间去问过他，说他的理论其实很像是符号系统的神经网络实现、只是被强行称为“栈”。但他没有回答，只是让我一边待着去。</p>
<p>在此之后，Hinton 魔怔般地疯狂反对符号技术。2016 年，LeCun、Bengio 和 Hinton 共同在学界最具份量的《自然》杂志上发表论文，其中直接摒弃了符号处理技术。没有和解的余地，<strong>文章宣称应该用神经网络彻底取代符号系统。</strong>后来，Hinton 又在另一次会议上呼吁，别在符号处理身上浪费资金了。这就如同电动车时代已到，为什么还要在内燃机研究上投入心力？</p>
<p>但这种尚未充分探索就枉下结论的态度实在令人难以信服。<strong>Hinton 说得没错，以往的 AI 研究者确实也对深度学习发起过攻讦，但他自己如今也不过是以牙还牙、并没好到哪里去。</strong></p>
<p>在我看来，这种对抗性的立场其实损害了整个 AI 学界的利益。但无论如何，Hinton 发起的这波符号处理讨伐战确实取得了巨大成功，之后几乎所有研究投资都集中在深度学习这个方向上。</p>
<p>Hinton、LeCun 和 Bengio 共同获得 2018 年的图灵奖，他的研究成为全世界关注的焦点。</p>
<p>更为讽刺的是，Hinton 其实是 George Boole 的玄孙，而以 Boole 命名的 Boolean 代数正是符号 AI 中的基础工具之一。如果这两代天才能把智慧合为一处，也许我们所期待的真正 AI 能够早日来临。</p>
<p><strong>至于我为什么坚持认为混合 AI（不止于深度学习和符号处理）才是正确的方向，理由有如下四点：</strong></p>
<p>这世界上的很多知识，从历史到科技，目前仍以符号形式为主。像纯深度学习那样放弃传统知识积累、单靠算力从零开始探索一切，似乎既武断又自缚双手。</p>
<p>即使在算术这类清晰有序的领域中，深度学习的表现也不理想；而混合系统也许经任何单一方法都更有潜力可挖。</p>
<p><strong>在计算中的很多基础层面，符号系统的表现仍远超现有神经网络，</strong>前者更擅长在复杂场景下进行推理，能够实现算术等更系统、更可靠的基本运算，也能更精确地表达部分和整体之间的关系（从对三维世界的理解、到对人类语言的分析，这都是种必不可少的能力）。</p>
<p><strong>符号系统在表达和查询大型数据库方面更稳健、更灵活，也能更好地实现形式验证技术（在某些安全应用中至关重要），其自身也在现代微处理器设计中拥有充分体现。</strong>粗暴放弃优势、拒绝尝试混合架构简直是不可理喻。</p>
<p>深度学习系统是种“黑盒子”，我们只能看到输入和输出，但却无法理解其内部运作和处理机制、解释不了模型为什么会给出当前结论。而且如果模型给出了错误答案，我们能做的除了收集更多数据、也没什么更好的办法。</p>
<p>于是乎，深度学习笨拙、难以解释，而且在很多场景下根本无法帮助人类实现认知增强。相反，如果能把深度学习的学习能力跟明确的符号、丰富的语义联系起来，得到的混合方案也许能掀起新一轮变革。</p>
<p>正是因为通用人工智能（AGI）将承担起巨大的责任，所以它必须像不锈钢般坚实、可靠、充分发挥每一种底材的优势。<strong>同理，任何单一的 AI 方法都不足以解决问题，正确的道路应该是把多种方法合而为一。</strong> 会有人蠢到单方面强调铁元素或者碳元素在不锈钢中的重要性吗？但 AI 领域的现状就是这样。</p>
<p>但也有好消息。1990 年时的 Hinton 曾经短暂提出过神经与符号间的和解，而我将整个职业生涯都投入了其中。这种融合探索一刻未停，而且正在积蓄力量。</p>
<p>Artur Garcez 和 Luis Lamb 曾在 2009 年发表过一篇关于混合模型的文章，名为《神经符号认知推理》（Neural-Symbolic Cognitive Reasoning）。而近年来在围棋、象棋等棋盘游戏中表现出色的，也都是混合模型。AlphaGo 就将符号树搜索与深度学习结合起来，这一基本思路源自上世纪五十年代末、并在九十年代更丰富的统计数据支持下得到强化。</p>
<p>很明显，单靠经典树搜索本身并不够，单靠深度学习也不行。再说 DeepMind 的 ALphaFold2，这是一种通过核苷酸预测蛋白质结构的 AI 系统，采用的同样是混合模型。其中汇聚了一系列精心设计、以符号形式表达的 3D 分子结构，同时具备令人惊叹的深度学习数据分析能力。</p>
<p>Josh Tenenbaum、Anima Anandkumar 和 Yejin Choi 等研究者也在朝着神经符号方向进军。包括 IBM、英特尔、Google、Facebook 和微软在内的众多科技巨头已经在认真投资神经符号学方法。Swarat Chaudhuri 和他的同事们正在探索“神经符号编程”（ neurosymbolic programming）这一全新领域，我个人对此也是极度期待。</p>
<p>四十年来，我第一次对 AI 抱有乐观期望。正如认知科学家 Chaz Firestone 与 Brian Scholl 所言，“头脑不只有一种运转方式，因为头脑并非单一的存在。相反，头脑由多个部分构成，不同的部分有不同的运作机制：观看颜色与规划假期的方式不同，理解语句、操纵肢体、记忆事件、感受情绪的方法也是各不相同。”<strong>盲目把所有认知都堆在一处根本不现实，而随时整个 AI 行业对混合方法的态度愈发开放，我认为真正的机遇也许即将到来。</strong></p>
<p><strong>面对伦理学、计算科学等现实挑战，AI 领域所应依靠的不仅仅是数学和计算机科学知识，还需要语言学、心理学、人类学及神经科学等多个这科的加持。只有汇聚一切力量、团结一切盟友，AI 才能再次冲破牢笼。</strong>请记住，人类大脑可能是已知宇宙中最复杂的系统，如果我们想要用技术重现这样一个复杂系统，将不得不仰仗开放协作的力量。</p>
<p>参考文献：</p>
<ol>
<li>Varoquaux, G. & Cheplygina, V. How I failed machine learning in medical imaging—shortcomings and recommendations. arXiv 2103.10292 (2021).</li>
<li>Chan, S., & Siegel, E.L. Will machine learning end the viability of radiology as a thriving medical specialty? British Journal of Radiology92, 20180416 (2018).</li>
<li>Ross, C. Once billed as a revolution in medicine, IBM’s Watson Health is sold off in parts. STAT News (2022).</li>
<li>Hao, K. AI pioneer Geoff Hinton: “Deep learning is going to be able to do everything.” MIT Technology Review (2020).</li>
<li>Aguera y Arcas, B. Do large language models understand us? Medium (2021).</li>
<li>Davis, E. & Marcus, G. GPT-3, Bloviator: OpenAI’s language generator has no idea what it’s talking about. MIT Technology Review (2020).</li>
<li>Greene, T. DeepMind tells Google it has no idea how to make AI less toxic. The Next Web (2021).</li>
<li>Weidinger, L., et al. Ethical and social risks of harm from Language Models. arXiv 2112.04359 (2021).</li>
<li>Bender, E.M., Gebru, T., McMillan-Major, A., & Schmitchel, S. On the dangers of stochastic parrots: Can language models be too big? Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency 610–623 (2021).</li>
<li>Kaplan, J., et al. Scaling Laws for Neural Language Models. arXiv 2001.08361 (2020).</li>
<li>Markoff, J. Smaller, Faster, Cheaper, Over: The Future of Computer Chips. The New York Times (2015).</li>
<li>Rae, J.W., et al. Scaling language models: Methods, analysis & insights from training Gopher. arXiv 2112.11446 (2022).</li>
<li>Thoppilan, R., et al. LaMDA: Language models for dialog applications. arXiv 2201.08239 (2022).</li>
<li>Wiggers, K. Facebook releases AI development tool based on NetHack. Venturebeat.com (2020).</li>
<li>Brownlee, J. Hands on big data by Peter Norvig. machinelearningmastery.com (2014).</li>
<li>McCulloch, W.S. & Pitts, W. A logical calculus of the ideas immanent in nervous activity. Bulletin of Mathematical Biology52, 99-115 (1990).</li>
<li>Olazaran, M. A sociological history of the neural network controversy. Advances in Computers37, 335-425 (1993).</li>
<li>Marcus, G.F., et al. Overregularization in language acquisition. Monographs of the Society for Research in Child Development57(1998).</li>
<li>Hinton, G. Aetherial Symbols. AAAI Spring Symposium on Knowledge Representation and Reasoning Stanford University, CA (2015).</li>
<li>LeCun, Y., Bengio, Y., & Hinton, G. Deep learning. Nature521, 436-444 (2015).</li>
<li>Razeghi, Y., Logan IV, R.L., Gardner, M., & Singh, S. Impact of pretraining term frequencies on few-shot reasoning. arXiv 2202.07206 (2022).</li>
<li>Lenat, D. What AI can learn from Romeo & Juliet. Forbes (2019).23. Chaudhuri, S., et al. Neurosymbolic programming. Foundations and Trends in Programming Languages7, 158-243 (2021).</li>
</ol>
<p> </p>
<p>作者：Gary Marcus，译者：核子可乐；微信公众号： InfoQ</p>
<p>原文链接：https://nautil.us/deep-learning-is-hitting-a-wall-14467/</p>
<p>译文链接：https://mp.weixin.qq.com/s/j0MKAh9z41AFQqz4HbI8Rw</p>
<p>本文由 @InfoQ 翻译发布于人人都是产品经理。未经许可，禁止转载。</p>
<p>题图来自Unsplash，基于 CC0 协议。</p>
                      
</div>
            