
---
title: '策略产品经理必读系列—梯度下降法'
categories: 
 - 新媒体
 - 人人都是产品经理
 - 最新文章
headimg: 'https://image.woshipm.com/wp-files/2022/08/cVHadcATk6TjFuiEd30n.jpg'
author: 人人都是产品经理
comments: false
date: Sun, 07 Aug 2022 00:00:00 GMT
thumbnail: 'https://image.woshipm.com/wp-files/2022/08/cVHadcATk6TjFuiEd30n.jpg'
---

<div>   
<blockquote><p>编辑导语：机器学习是策略产品经理需要了解的一个方面，而梯度下降法则是学习机器学习必须了解的思想，本文作者通过一个案例介绍了梯度下降法，一起来看一下吧。</p></blockquote>
<p><img data-action="zoom" class="aligncenter size-full wp-image-5556874" src="https://image.woshipm.com/wp-files/2022/08/cVHadcATk6TjFuiEd30n.jpg" alt width="900" height="420" referrerpolicy="no-referrer"></p>
<p>策略产品经理必须要对机器学习有一定的了解，而梯度下降法则是学习机器学习必须要了解的思想，本篇通过一个生动的案例来为大家介绍到底什么是梯度下降法。</p>
<h2 id="toc-1"><b>01 引入</b></h2>
<p>我们先从一个案例入手，下图是一组上海市静安区的房价信息：</p>
<p><img data-action="zoom" class="aligncenter" src="https://image.woshipm.com/wp-files/2022/08/iCSKTfYTbbdlkPDEOl2K.png" alt width="270" height="280" referrerpolicy="no-referrer"></p>
<p>我们用Python在坐标系上面画出来如下图：</p>
<p><img data-action="zoom" class="aligncenter" src="https://image.woshipm.com/wp-files/2022/08/XIDFnLMamBSaxSdOMznE.png" alt width="830" height="518" referrerpolicy="no-referrer"></p>
<p>我们现在想拟合一个线性函数来表示房屋面积和房价的关系。我们初中都学过的一元一次函数表达式为：y=kx+b（k≠0）。很明显不可能有一对组合（k,b）全部经过上图7个点，我们只能尽可能地找到一对组合，使得该线性函数离上图7个点的总距离最近。</p>
<p><img data-action="zoom" class="aligncenter" src="https://image.woshipm.com/wp-files/2022/08/dLshXmuFBNAxtTxDMxcd.jpeg" alt width="908" height="294" referrerpolicy="no-referrer"></p>
<p>如上图所示，实际值与预测值之间差异的均方差我们把它称为损失函数，也有叫做成本函数或者代价函数的，意义都一样。我们希望找到一个组合（k，b）可以使得损失函数的值最小。</p>
<p>上述只有一个输入变量x，如果我们多加入几个输入变量，比如卧室的数量、离最近地铁站的距离。最终目标变量和损失函数我们用下述函数表达式来表达：</p>
<p><img data-action="zoom" class="aligncenter" src="https://image.woshipm.com/wp-files/2022/08/myhK0ffrEGCvtUAEn9MP.jpeg" alt width="936" height="498" referrerpolicy="no-referrer"></p>
<p>现在我们的任务就是求出一组θ，在已知【x，y】的前提下使得损失函数的值最小。那么如何计算出θ了，使用什么方法了？我们首先回到损失函数表达式本身，损失函数本身是一个y=x^2的形式，高中数学大家应该都学过这是一个开口向上的抛物线方程，大概长下图这样：</p>
<p><img data-action="zoom" class="aligncenter" src="https://image.woshipm.com/wp-files/2022/08/vRCj04aYBt1QvONQPO4n.png" alt width="348" height="318" referrerpolicy="no-referrer"></p>
<p>我们如何找到这个函数的最低点？上图是一个二维图，我们很轻松就可以肉眼看出x=0时，y最小。如果维度更多，比如z = (x-10)^2 + (y-10)^2，则得到下图：</p>
<p><img data-action="zoom" class="aligncenter" src="https://image.woshipm.com/wp-files/2022/08/LbBxy6DAWSXXW0VeArwf.jpeg" alt width="418" height="305" referrerpolicy="no-referrer"></p>
<p>我们如何定位出最小值，特别强调一点，这里的x是一个“大”参数的概念，x应该等于下述公式</p>
<p><img data-action="zoom" class="aligncenter" src="https://image.woshipm.com/wp-files/2022/08/uOpUv024fnfDH5glIbPw.png" alt width="728" height="156" referrerpolicy="no-referrer"></p>
<p>大家要明确上图横坐标是x和y，函数表达式里的θ已经知道了，所以我们是找到最合适的（x,y）使得函数值最小。如果我们现在是已知样本（x,y），那么上图的变量就变为了θ_0和θ_i，并不是x_i，我们是以θ_0和θ_i作为输入变量做的图，x_i和y_i都是已知的固定值，这一点必须明确了。上图的纵坐标的值就变为损失函数的值。</p>
<p>我们的问题是已知样本的坐标（x,y），来求解一组θ参数，使得损失函数的值最小。我们如何找到上图中的最低点？因为找到最低点，那么最低点对应的横坐标所有维度就是我们想得到的θ_0和θ_i，而纵坐标就是损失函数的最小值。找到最低点所有答案就全部解出来了。</p>
<p>现在问题来了？有没有一种算法让我们可以慢慢定位出最小值，这个算法就是梯度下降法。</p>
<h2 id="toc-2"><b>02 梯度下降法简介</b></h2>
<h3><b>1. 梯度下降法的思想</b></h3>
<p>我们首先介绍梯度下降法的整体思想。假设你现在站在某个山峰的峰顶，你要在天黑前到达山峰的最低点，那里有食品水源供给站，可以进行能量补充。你不需要考虑下山的安全性，即使选择最陡峭的悬崖下山，你也可以全身而退，那么如何下山最快了？</p>
<p><img data-action="zoom" class="aligncenter" src="https://image.woshipm.com/wp-files/2022/08/7t0NAQUmbPXXizJh9zJD.jpeg" alt width="1440" height="680" referrerpolicy="no-referrer"></p>
<p>最快的方法就是以当前的位置为基准，寻找该位置最陡峭的地方，然后沿该方向往下走。走一段距离后，再以当前位置为基准，重新寻找最陡峭的地方，一直重复最终我们就可以到达最低点。我们需要不停地去重新定位最陡峭的地方，这样才不会限于局部最优。</p>
<p>那么整个下山过程中我们会面临两个问题：</p>
<ol>
<li>如何测量山峰的“陡峭”程度</li>
<li>每一次走多长距离后重新进行陡峭程度测量；走太长，那么整体的测量次数就会比较少，可能会导致走的并不是最佳路线，错过了最低点。走太短，测量次数过于频繁，整体耗时太长，还没有到达食品供给站就已经GG了。这里的步长如何设置？</li>
</ol>
<p><img data-action="zoom" class="aligncenter" src="https://image.woshipm.com/wp-files/2022/08/GfQHSaXbsLqrKslMW4p6.jpeg" alt width="1440" height="490" referrerpolicy="no-referrer"></p>
<p>Part1里面介绍了如何从一个开口向上的抛物线高点定位到最低点的问题和下山的场景是完全类似的，抛物线就相当于一个山峰，我们的目标就是找到抛物线的最低点，也就是山底。</p>
<p>最快的下山方式就是找到当前位置最陡峭的方向，然后沿着此方向向下走，对应到抛物线中，就是计算给定点的梯度，然后朝着梯度相反的方向（ Part 2.3里面会解释为什么是朝着梯度相反的方向），就能让抛物线值下降的最快。同时我们也要和下山一样，不停地定位新位置，再计算新位置的梯度，然后按照新方向下降，最后慢慢定位到抛物线的最低点。</p>
<h3><b>2. 梯度下降法算法</b></h3>
<p>Part2.1里面已经介绍了梯度下降法的思想，遗留了两个问题。第一就是如何计算“陡峭”程度，我们这里把它叫做梯度，我们用∇J_θ来代替。第二个也就是步长问题，我们用一个α学习率来代表这个步长，α越大代表步长越大。知道了这两个值，我们如何去得到θ参数的更新表达式了？</p>
<p>J是关于θ的一个函数，假设初始时我们在θ_1这个位置，要从这个点走到J的最小值点，也就是山底。首先我们先确定前进的方向，也就是梯度的反向“-∇J_θ”，然后走一段距离的步长，也就是α，走完这个段步长，就到达了θ_2这个点了。表达式如下图：</p>
<p><img data-action="zoom" class="aligncenter" src="https://image.woshipm.com/wp-files/2022/08/WKfOVO4vHIjgzA0GNVhi.png" alt width="770" height="236" referrerpolicy="no-referrer"></p>
<p>我们按照上述表达式一直不停地更新θ的值，一直到θ收敛不变为止，当我们到达山底，此时函数的梯度就是0了，θ值也就不会再更新了，因为表达式的后半部分一直是0了。</p>
<p>整个下降过程中损失函数的值是一定在减少，但是我们想学习出来的参数值θ不一定一直在减小。因为我们需要找到损失函数最小时的坐标点，这个坐标点的坐标不一定是原点，很可能是（2，3）甚至是（4，6），我们找到的是最合适的θ值使得损失函数最小。下图我们用一个例子来进行说明：</p>
<p><img data-action="zoom" class="aligncenter" src="https://image.woshipm.com/wp-files/2022/08/gdUhYwtKorW74fIKJi2t.jpeg" alt width="1440" height="829" referrerpolicy="no-referrer"></p>
<p>上图的最低点很明显就是原点，我们通过梯度下降法来逼近这个最低点。我们可以看到损失函数的值在一直减少，θ的值也在往0这个值进行收敛。</p>
<h3><b>3. 梯度下降法数学计算</b></h3>
<p>Part1和2介绍了梯度下降的思想和θ更新的表达式，现在我们从数学层面进行解释：</p>
<p><b>1）为什么是向梯度相反的方向下降</b></p>
<p><img data-action="zoom" class="aligncenter" src="https://image.woshipm.com/wp-files/2022/08/MnpSBIMIhanNobS0gfX1.jpeg" alt width="1264" height="628" referrerpolicy="no-referrer"></p>
<p>上图应该很形象地显示为什么要朝着梯度的反方向了。梯度是一个向量，梯度的方向是函数在指定点上升最快的方向，那么梯度的反方向自然是下降最快的方向了。</p>
<p><b>2）泛化的θ参数更新公式</b></p>
<p>Part2.2里面的例子我们选择的是一个最简单的函数表达式，θ参数分为两种，一种是和输入变量x配对的参数θ_i，一种是固定的偏差θ_0。我们用已知的样本数据（x,y）来求解出使得损失函数最小的一组θ参数。下面我们来计算一个通用泛化的θ参数更新表达式。<b>我们只需要用到高中数学中的导数知识即可，朋友们相信我真的很easy。</b></p>
<p>下图是对和输入变量x配对的参数θ_i更新表达式：</p>
<p><img data-action="zoom" class="aligncenter" src="https://image.woshipm.com/wp-files/2022/08/XQMIqs83cSjXKbUI7Crm.png" alt width="799" height="430" referrerpolicy="no-referrer"></p>
<p>下图是对固定的偏差θ_0的更新表达式：</p>
<p><img data-action="zoom" class="aligncenter" src="https://image.woshipm.com/wp-files/2022/08/Z6jsvUjKKR0FE2fgVSp6.png" alt width="785" height="434" referrerpolicy="no-referrer"></p>
<p>上面的数学过程也就是高中我们学习导数里面最简单的求导过程了。那么至此我们也就将梯度下降算法的思想和数学解释全部介绍完了。</p>
<h3><b>4. 梯度下降法分类</b></h3>
<p>Part2.3里面的公式大家也看到了我们要借助样本的（x，y）的数据来进行参数θ的更新，如果现在样本有100条数据，我们如何来更新。正常情况下，我们更新的方式有两种：</p>
<p><b>1）随机梯度下降(Stochastic Gradient Descent)</b></p>
<p>我们每次只使用单个训练样本来更新θ参数，依次遍历训练集，而不是一次更新中考虑所有的样本。就像开头介绍那7条房价数据，我们一个一个来计算，计算一次更新一次θ，直到θ收敛或者达到后期更新幅度已经小于我们设置的阀值。</p>
<p><b>2）批量梯度下降(Batch Gradient Descent)</b></p>
<p>我们每次更新都遍历训练集中所有的样本，以它们的预测误差之和为依据更新。我们会一次性将7条样本数据的预测误差都汇总，然后进行一次更新。更新完以后，继续以7条样本数据的预测误差之和进行汇总，再更新，直到θ收敛或者达到后期更新幅度已经小于我们设置的阀值。</p>
<p>当训练样本数很大时，批量梯度下降的每次更新都会是计算量很大的操作，而随机梯度下降可以利用单个训练样本立即更新，因此随机梯度下降 通常是一个更快的方法。但随机梯度下降也有一个缺点，那就是θ可能不会收敛，而是在最小值附近振荡，但在实际中也都会得到一个足够好的近似。</p>
<p>所以实际情况下，我们一般不用固定的学习率，而是让它随着算法的运行逐渐减小到零，也就是在接近“山底”的时候慢慢减小下降的“步幅”，换成用“小碎步”走，这样它就更容易收敛于全局最小值而不是围绕它振荡了。</p>
<h2 id="toc-3"><b>03 梯度下降法Python实践</b></h2>
<p>以下就是通过实际运行程序得到的相关结果图。</p>
<h3><b>1. 单变量：y = x^2求最低点</b></h3>
<p>假设X的初始值是10，我们让程序迭代10次得到的结果如下图：</p>
<p><img data-action="zoom" class="aligncenter" src="https://image.woshipm.com/wp-files/2022/08/4FLK8dXnlYlnSgxqoXOm.png" alt width="431" height="503" referrerpolicy="no-referrer"></p>
<h3><b>2. 多变量：z = (x-10)^2 + (y-10)^2求最低点</b></h3>
<p>假设X和Y的初始值都是20，我们让模型迭代100次得到的效果如下图：</p>
<p><img data-action="zoom" class="aligncenter" src="https://image.woshipm.com/wp-files/2022/08/4UB8CxguShup9X3pHTzr.jpeg" alt width="429" height="305" referrerpolicy="no-referrer"></p>
<h3><b>3. 根据给定样本求解出最佳θ组合</b></h3>
<p>假设样本中X和Y的值如下：</p>
<p>x = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20]</p>
<p>y = [3,4,5,5,2,4,7,8,11,8,10,11,13,13,16,17,16,17,18,20]</p>
<p>我们希望找到一组参数θ_0和θ_1来拟合一个X和Y之间的最优线性模型，最终拟合结果如下：</p>
<p><img data-action="zoom" class="aligncenter" src="https://image.woshipm.com/wp-files/2022/08/w1VznnRraDXjpjzaRZt6.png" alt width="665" height="303" referrerpolicy="no-referrer"></p>
<p>本篇文章前半部分通俗易懂地将整个梯度下降算法全面地讲解了一遍，后半部分通过Python实际落地了各种案例，希望看完后大家对于梯度下降法有一个全面且具象的了解。</p>
<p> </p>
<p>本文由 @King James 原创发布于人人都是产品经理。未经许可，禁止转载。</p>
<p>题图来自 Unsplash，基于 CC0 协议</p>
<div class="support-author"><div class="support-title">给作者打赏，鼓励TA抓紧创作！</div><button class="button--pay" data-post-id="5556168" data-author="1142016" data-avatar="https://image.woshipm.com/wp-files/2022/07/qCErAk05vPlJdEQnt9OQ.jpeg"><svg width="13" height="16" class="svgIcon--use" viewBox="0 0 13 16"><path d="M9.113,4.571 C9.951,3.771 10.895,2.742 10.685,2.057 C10.475,1.485 10.056,0.799 9.427,0.571 C8.903,0.342 8.379,0.456 7.750,0.799 C7.540,0.342 7.016,0.114 6.596,-0.001 C5.863,-0.001 5.234,0.228 4.814,0.914 C4.080,0.571 3.451,0.685 2.927,1.028 C2.613,1.256 2.298,1.713 2.298,2.628 C2.298,3.542 3.137,4.228 3.766,4.685 C2.508,5.599 -0.218,7.885 -0.008,12.228 C-0.218,15.656 2.613,15.999 2.613,15.999 L10.371,15.999 C11.314,15.885 12.991,14.971 12.991,12.571 L12.991,12.228 C13.201,7.771 10.371,5.371 9.113,4.571 L9.113,4.571 ZM8.932,11.835 L6.940,11.835 L6.940,13.207 C6.940,13.435 6.731,13.549 6.521,13.549 C6.311,13.549 6.102,13.435 6.102,13.207 L6.102,11.835 L4.110,11.835 C3.900,11.835 3.795,11.606 3.795,11.378 C3.795,11.149 3.900,10.921 4.110,10.921 L6.102,10.921 L6.102,10.121 L4.949,10.121 C4.739,10.121 4.634,9.892 4.634,9.664 C4.634,9.435 4.739,9.206 4.949,9.206 L5.892,9.206 L4.739,7.950 C4.634,7.835 4.739,7.606 4.949,7.492 C5.158,7.378 5.368,7.264 5.473,7.378 L6.521,8.635 L7.674,7.264 C7.779,7.149 7.989,7.264 8.198,7.378 C8.408,7.492 8.408,7.721 8.408,7.835 L7.150,9.321 L8.094,9.321 C8.303,9.321 8.408,9.549 8.408,9.778 C8.408,10.007 8.303,10.235 8.094,10.235 L6.940,10.235 L6.940,11.035 L8.932,11.035 C9.142,11.035 9.247,11.264 9.247,11.493 C9.247,11.606 9.037,11.835 8.932,11.835 L8.932,11.835 Z"/></svg>
赞赏</button></div>                      
</div>
            