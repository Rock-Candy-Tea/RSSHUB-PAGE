
---
title: 'Meta AI开发可解码大脑语音的模型，助力脑损伤患者更好与人交流，有望创造计算机交互新方式'
categories: 
 - 新媒体
 - MIT 科技评论
 - 首页
headimg: 'https://p3.toutiaoimg.com/origin/tos-cn-i-qvj2lq49k0/6a09685ec9264708911209a4fc57271f'
author: MIT 科技评论
comments: false
date: Sat, 03 Sep 2022 11:53:13 GMT
thumbnail: 'https://p3.toutiaoimg.com/origin/tos-cn-i-qvj2lq49k0/6a09685ec9264708911209a4fc57271f'
---

<div>   
<div style="caret-color: rgb(0, 0, 0); color: rgb(0, 0, 0);"><img src="https://p3.toutiaoimg.com/origin/tos-cn-i-qvj2lq49k0/6a09685ec9264708911209a4fc57271f" referrerpolicy="no-referrer"><a href="http://www.sciphi.cn/article/view/undefined"></a><p><span style="letter-spacing: 1px;"><span style="color: rgb(89, 89, 89); --tt-darkmode-color: #595959;"><br></span></span></p><p><span style="letter-spacing: 1px;"><span style="color: rgb(89, 89, 89); --tt-darkmode-color: #595959;">据了解，每年遭受创伤性脑损伤的人数近 7000 万人，他们大都无法再通过语音，甚至手势与他人进行交流。</span></span></p><p><span style="letter-spacing: 1px;"><span style="color: rgb(89, 89, 89); --tt-darkmode-color: #595959;"><br></span></span></p><p><span style="letter-spacing: 1px;"><span style="color: rgb(89, 89, 89); --tt-darkmode-color: #595959;">因此，从大脑活动中直接解码语言成为医疗保健和神经科学中期待已久的目标。这样，不需要人开口说话，就能了解他/她想要表达的意思。</span></span></p><p><span style="letter-spacing: 1px;"><span style="color: rgb(89, 89, 89); --tt-darkmode-color: #595959;"><br></span></span></p><p><span style="letter-spacing: 1px;"><span style="color: rgb(89, 89, 89); --tt-darkmode-color: #595959;">近日，Meta AI 的让-雷米·金（Jean-Remi King）等研究人员开发出一种 AI 模型，可以根据大脑活动的无创记录解码语音。相关论文也以《从非侵入性大脑记录解码语音》（Decoding speech from non-invasive brain recordings）为题提交到 </span></span><em><span style="letter-spacing: 1px;"><span style="color: rgb(89, 89, 89); --tt-darkmode-color: #595959;">arXiv </span></span></em><span style="letter-spacing: 1px;"><span style="color: rgb(89, 89, 89); --tt-darkmode-color: #595959;">上。</span></span></p><p><span style="letter-spacing: 1px;"><span style="color: rgb(89, 89, 89); --tt-darkmode-color: #595959;"><br></span></span></p><div><img src="https://p9.toutiaoimg.com/origin/tos-cn-i-qvj2lq49k0/ff7c18a2943a4172816b56dd451478b3" referrerpolicy="no-referrer">（来源：Meta AI）</div><div><br><a href="http://www.sciphi.cn/article/view/undefined"></a><p style="text-align: center;"></p><p><span style="letter-spacing: 1px;"><span style="color: rgb(89, 89, 89); --tt-darkmode-color: #595959;">“大脑中控制嘴的部分和涉及理解和形成语言的部分是分开的。我们一直在将 AI 和大脑方面的知识结合在一起做研究，以帮助那些遭受创伤性神经损伤的人们。”金告诉媒体。他还提到，虽然常见的磁共振成像和计算机断层扫描等装置也能产生详细的大脑图像，但其显示的是结构而非大脑活动。</span></span></p><p><span style="letter-spacing: 1px;"><span style="color: rgb(89, 89, 89); --tt-darkmode-color: #595959;"><br></span></span></p><p><span style="letter-spacing: 1px;"><span style="color: rgb(89, 89, 89); --tt-darkmode-color: #595959;">目前，从大脑活动中解码语音的大多数进展，依赖于侵入性大脑记录技术，例如立体定向脑电图和脑皮层电图。但相较于无创方法，它们需要打开头骨并将电极直接放在大脑本身上，对人有一定风险和危害性。</span></span></p><p><span style="letter-spacing: 1px;"><span style="color: rgb(89, 89, 89); --tt-darkmode-color: #595959;"><br></span></span></p><p><span style="letter-spacing: 1px;"><span style="color: rgb(89, 89, 89); --tt-darkmode-color: #595959;">而不需要任何手术的方法，如脑电图、脑磁图等非侵入性技术，可从外部扫描并观察人脑活动。使用这些方法解码语音更安全、更具可扩展性，并能让更多人获益。</span></span></p><p><span style="letter-spacing: 1px;"><span style="color: rgb(89, 89, 89); --tt-darkmode-color: #595959;"><br></span></span></p><p><span style="letter-spacing: 1px;"><span style="color: rgb(89, 89, 89); --tt-darkmode-color: #595959;">不过，非侵入性录音往往是“嘈杂”的，并且由于每个人大脑的差异与记录期间传感器的位置等多种原因，不同人的会话录制可能会有很大差异。</span></span></p><p><span style="letter-spacing: 1px;"><span style="color: rgb(89, 89, 89); --tt-darkmode-color: #595959;"><br></span></span></p><p><span style="letter-spacing: 1px;"><span style="color: rgb(89, 89, 89); --tt-darkmode-color: #595959;">为解决这个问题，Meta AI 的研究人员转向机器学习算法来帮助“清理”噪声。他们使用的模型称为 wav</span></span><span style="color: rgb(89, 89, 89); letter-spacing: 1px;">e2vec 2.0，这是该团队在 2020 年研发的开源自监督学习 AI 工具，可用于从嘈杂的音频中识别正确的语音。</span></p><p><span style="color: rgb(89, 89, 89); letter-spacing: 1px;"><br></span></p><p><span style="letter-spacing: 1px;"><span style="color: rgb(89, 89, 89); --tt-darkmode-color: #595959;">他们使用该模型来辨别志愿者大脑中的语音表示。结果表明，对于我们日常使用的大部分单词，模型的识别准确率可达 73%。</span></span></p><p><span style="letter-spacing: 1px;"><span style="color: rgb(89, 89, 89); --tt-darkmode-color: #595959;"><br></span></span></p><p><span style="letter-spacing: 1px;"><span style="color: rgb(89, 89, 89); --tt-darkmode-color: #595959;">具体来说，研究人员专注于脑电图和脑磁图两种非侵入性技术，他们在这两种方式的四个开源录音上，对 wave2vec 2.0 进行了训练。训练数据集包括来自 169 名健康志愿者，在听有声读物和孤立句子（英语和荷兰语）时，大脑活动的 150 多个小时的录音。</span></span></p><p><span style="letter-spacing: 1px;"><span style="color: rgb(89, 89, 89); --tt-darkmode-color: #595959;"><br></span></span></p><p><span style="letter-spacing: 1px;"><span style="color: rgb(89, 89, 89); --tt-darkmode-color: #595959;">该团队还将这些脑电图和脑磁图的记录输入到一个“大脑”模型中，该模型由一个带有残差连接的标准深度卷积网络组成。</span></span></p><p><span style="letter-spacing: 1px;"><span style="color: rgb(89, 89, 89); --tt-darkmode-color: #595959;"><br></span></span></p><p><span style="letter-spacing: 1px;"><span style="color: rgb(89, 89, 89); --tt-darkmode-color: #595959;">在实践中，分析大脑数据通常需要一个复杂的工程管道，用于重新调整模板大脑上的大脑信号。</span></span></p><p><span style="letter-spacing: 1px;"><span style="color: rgb(89, 89, 89); --tt-darkmode-color: #595959;"><br></span></span></p><p><span style="letter-spacing: 1px;"><span style="color: rgb(89, 89, 89); --tt-darkmode-color: #595959;">本次研究里，研究者还设计了一个新的主题嵌入层，该层经过端到端训练，可以将所有大脑记录对齐到一个公共空间中。</span></span></p><p><span style="letter-spacing: 1px;"><span style="color: rgb(89, 89, 89); --tt-darkmode-color: #595959;"><br></span></span></p><p><span style="letter-spacing: 1px;"><span style="color: rgb(89, 89, 89); --tt-darkmode-color: #595959;">即为了从非侵入性大脑信号中解码语音，研究人员训练了一个具有对比学习的模型，以对齐语音及其相应的大脑活动。两者保持一致，就能轻易找出“大脑”模型输出所对应的语音。</span></span></p><p><span style="letter-spacing: 1px;"><span style="color: rgb(89, 89, 89); --tt-darkmode-color: #595959;"><br></span></span></p><p><span style="letter-spacing: 1px;"><span style="color: rgb(89, 89, 89); --tt-darkmode-color: #595959;">值得一提的是，Meta AI 在之前工作中，使用 wav2vec 2.0 证明了该算法可生成与大脑相似的语音表示。如下图所示。</span></span></p><div><br></div><div><img src="https://p3.toutiaoimg.com/origin/tos-cn-i-qvj2lq49k0/0eb88fb676f642c6aaace62d380cee7c" referrerpolicy="no-referrer"></div><div>▲图 | wav2vec 2.0（左）的激活映射到大脑（右）以响应相同的语音（来源：Meta AI）</div><div><br><a href="http://www.sciphi.cn/article/view/undefined"></a><p><span style="letter-spacing: 1px;"><span style="color: rgb(89, 89, 89); --tt-darkmode-color: #595959;">wav2vec 2.0 中语音“类脑”表示的特点使其成为构建解码器的理想选择，它有助于了解应该从大脑信号中提取哪些表示。</span></span></p><p><span style="letter-spacing: 1px;"><span style="color: rgb(89, 89, 89); --tt-darkmode-color: #595959;"><br></span></span></p><p><span style="letter-spacing: 1px;"><span style="color: rgb(89, 89, 89); --tt-darkmode-color: #595959;">Meta AI 在官方博文中提到：“训练后，我们的系统执行所谓的零样本分类：给定一个大脑活动片段，它可以从众多音频片段中确定这个人实际听到的是哪个片段。即算法能推断出人们最有可能听到的词。”</span></span></p><p><span style="letter-spacing: 1px;"><span style="color: rgb(89, 89, 89); --tt-darkmode-color: #595959;"><br></span></span></p><p><span style="letter-spacing: 1px;"><span style="color: rgb(89, 89, 89); --tt-darkmode-color: #595959;">同时，分析进一步表明，该算法的几个组成部分，包括 wav2vec 2.0 和主题层，对解码性能有益。此外，算法随着脑电图和脑磁图记录的数量而改进。</span></span></p><p><span style="letter-spacing: 1px;"><span style="color: rgb(89, 89, 89); --tt-darkmode-color: #595959;"><br></span></span></p><p><span style="letter-spacing: 1px;"><span style="color: rgb(89, 89, 89); --tt-darkmode-color: #595959;">这意味着该方法受益于大量异构数据的提取，并且原则上可以帮助改进小型数据集的解码。这一点十分重要，因为在许多情况下，很难为特定的参与者收集大量数据。</span></span></p><p><span style="letter-spacing: 1px;"><span style="color: rgb(89, 89, 89); --tt-darkmode-color: #595959;"><br></span></span></p><p><span style="letter-spacing: 1px;"><span style="color: rgb(89, 89, 89); --tt-darkmode-color: #595959;">下一步，研究人员计划扩展他们的模型，以最终能直接从大脑活动中解码语音，而不需要音频剪辑数据集，即转向安全和多功能的通用语音解码器。</span></span></p><p><span style="letter-spacing: 1px;"><span style="color: rgb(89, 89, 89); --tt-darkmode-color: #595959;"><br></span></span></p><p><span style="letter-spacing: 1px;"><span style="color: rgb(89, 89, 89); --tt-darkmode-color: #595959;">总的来说，该项工作专注于解码语音感知，但若要实现与患者交流的目标，还得将研究扩展到语音生产。</span></span></p><p><span style="letter-spacing: 1px;"><span style="color: rgb(89, 89, 89); --tt-darkmode-color: #595959;"><br></span></span></p><p><span style="letter-spacing: 1px;"><span style="color: rgb(89, 89, 89); --tt-darkmode-color: #595959;">整体来看，该项研究是科学界使用 AI 更好地了解人类大脑广泛努力的一部分。这一研究领域除了可以帮助患者，它甚至可能推动技术的进一步的提升，比如创造与计算机交互的新方式。</span></span></p><p><span style="letter-spacing: 1px;"><span style="color: rgb(89, 89, 89); --tt-darkmode-color: #595959;"><br></span></span></p><div><img src="https://p9.toutiaoimg.com/origin/tos-cn-i-qvj2lq49k0/6929be3581fc4b0a8f36e72be94b4737" referrerpolicy="no-referrer"><a href="http://www.sciphi.cn/article/view/undefined"></a><p><span style="letter-spacing: 1px;"><strong><span style="color: rgb(178, 178, 178); --tt-darkmode-color: #A3A3A3;"><br></span></strong></span></p><p><span style="letter-spacing: 1px;"><strong><span style="color: rgb(178, 178, 178); --tt-darkmode-color: #A3A3A3;">参考资料：</span></strong></span></p><p><span style="letter-spacing: 1px;"><span style="color: rgb(178, 178, 178); --tt-darkmode-color: #A3A3A3;">https://ai.facebook.com/blog/ai-speech-brain-activity/</span></span></p><p><span style="letter-spacing: 1px;"><span style="color: rgb(178, 178, 178); --tt-darkmode-color: #A3A3A3;">https://arxiv.org/abs/2208.12266</span></span></p><p><span style="letter-spacing: 1px;"><span style="color: rgb(178, 178, 178); --tt-darkmode-color: #A3A3A3;">https://siliconangle.com/2022/08/31/meta-ai-researchers-develop-ways-read-speech-peoples-brains/</span></span></p><p><span style="letter-spacing: 1px;"><span style="color: rgb(178, 178, 178); --tt-darkmode-color: #A3A3A3;"><br></span></span></p><div><img src="https://p3.toutiaoimg.com/origin/tos-cn-i-qvj2lq49k0/ede8718b1a6648de9816265acf6d3e6c" referrerpolicy="no-referrer"><a href="http://www.sciphi.cn/article/view/undefined"></a></div></div></div></div></div>  
</div>
            