
---
title: '谷歌联合高校发布机器人导航系统LM-Nav，可结合三种预训练模型，无需用户注释即可执行自然语言命令'
categories: 
 - 新媒体
 - MIT 科技评论
 - 首页
headimg: 'https://p6.toutiaoimg.com/origin/tos-cn-i-qvj2lq49k0/3950f25a928b4bf09a25808c43ac0dcc'
author: MIT 科技评论
comments: false
date: Sun, 31 Jul 2022 11:04:14 GMT
thumbnail: 'https://p6.toutiaoimg.com/origin/tos-cn-i-qvj2lq49k0/3950f25a928b4bf09a25808c43ac0dcc'
---

<div>   
<div style="caret-color: rgb(0, 0, 0); color: rgb(0, 0, 0);"><img src="https://p6.toutiaoimg.com/origin/tos-cn-i-qvj2lq49k0/3950f25a928b4bf09a25808c43ac0dcc" referrerpolicy="no-referrer"><a href="http://www.sciphi.cn/article/view/undefined"></a><p><span style="letter-spacing: 1px;"><span style="color: rgb(89, 89, 89); --tt-darkmode-color: #595959;"><br></span></span></p><p><span style="letter-spacing: 1px;"><span style="color: rgb(89, 89, 89); --tt-darkmode-color: #595959;">在机器人领域中，最大的挑战之一就是如何让机器人实时听懂人类指令，并立即根据新指令、以及环境变化做出反应，实时进行新规划并完成人类要求的任务。</span></span></p><p><span style="letter-spacing: 1px;"><span style="color: rgb(89, 89, 89); --tt-darkmode-color: #595959;"><br></span></span></p><p><span style="letter-spacing: 1px;"><span style="color: rgb(89, 89, 89); --tt-darkmode-color: #595959;">这一过程对于机器人有着方方面面的要求，例如在依照人类指令导航到达目的地这一任务中，不仅需要机器人理解人类的指令，即要求其有自然语言理解能力，还得具备实时感知周围环境、即视觉识别能力，而且还需要机器人能将语言指令与感知到的环境互相“翻译”，才能最终按照人类指示的要求到达目的地。</span></span></p><p><span style="letter-spacing: 1px;"><span style="color: rgb(89, 89, 89); --tt-darkmode-color: #595959;"><br></span></span></p><p><span style="letter-spacing: 1px;"><span style="color: rgb(89, 89, 89); --tt-darkmode-color: #595959;">针对这类任务，该领域此前的主要解决方法是：让机器人理解文本时，需要事先训练它从大量带有文本指令注释的类似任务中进行学习。但是，这一方法需要注释数据，进而会消耗大量成本，最终阻碍了机器人在更多场合的应用。</span></span></p><p><span style="letter-spacing: 1px;"><span style="color: rgb(89, 89, 89); --tt-darkmode-color: #595959;"><br></span></span></p><p><span style="letter-spacing: 1px;"><span style="color: rgb(89, 89, 89); --tt-darkmode-color: #595959;">而最近越来越多的研究表明，如下新方法是可行的：即在没有事先标记的情况下，通过自我监督训练的目标条件策略，来训练机器人从大型的、未标记的数据集中进行基于视觉的导航的学习。而且，该具有更好的可扩展性与鲁棒性。</span></span></p><p><span style="letter-spacing: 1px;"><span style="color: rgb(89, 89, 89); --tt-darkmode-color: #595959;"><br></span></span></p><p><span style="letter-spacing: 1px;"><span style="color: rgb(89, 89, 89); --tt-darkmode-color: #595959;">正是在这一思路的启发下，谷歌研究人员开发了大型模型导航系统 LM-Nav 系统，该系统结合了上面所说的两种方法的优点，利用了预训练模型的功能，即使在导航数据没有经过任何用户注释的前提下，也可让机器人导航系统通过其自我监督系统，去理解自然语言指令并完成任务要求。</span></span></p><p><span style="letter-spacing: 1px;"><span style="color: rgb(89, 89, 89); --tt-darkmode-color: #595959;"><br></span></span></p><p><span style="letter-spacing: 1px;"><span style="color: rgb(89, 89, 89); --tt-darkmode-color: #595959;">尤其重要的是，该系统内的预训练语言和视觉语言模型的泛化能力非常强大，从而使机器人理解并执行更为复杂的高级指令。</span></span></p><p><span style="letter-spacing: 1px;"><span style="color: rgb(89, 89, 89); --tt-darkmode-color: #595959;"><br></span></span></p><p><span style="letter-spacing: 1px;"><span style="color: rgb(89, 89, 89); --tt-darkmode-color: #595959;">近日，相关论文以《LM-Nav：具有大型预训练语言、视觉和动作模型的机器人导航系统》（LM-Nav: Robotic Navigation with Large Pre-Trained Models of Language, Vision, and Action）为题发表在 </span></span><em><span style="letter-spacing: 1px;"><span style="color: rgb(89, 89, 89); --tt-darkmode-color: #595959;">arXiv </span></span></em><span style="letter-spacing: 1px;"><span style="color: rgb(89, 89, 89); --tt-darkmode-color: #595959;">上，美国加州大学伯克利分校和波兰华沙大学一起参与了研究。</span></span></p><p><span style="letter-spacing: 1px;"><span style="color: rgb(89, 89, 89); --tt-darkmode-color: #595959;"><br></span></span></p><div><img src="https://p3.toutiaoimg.com/origin/tos-cn-i-qvj2lq49k0/36515b944521419f97d5887f1218b4aa" referrerpolicy="no-referrer">（来源：arXiv）</div><div><br><a href="http://www.sciphi.cn/article/view/undefined"></a><p style="text-align: center;"></p><p><span style="letter-spacing: 1px;"><span style="color: rgb(89, 89, 89); --tt-darkmode-color: #595959;">LM-Nav 导航系统包含三个大型预训练模型，分别用于进行语言处理、将图像与语言关联、以及视觉导航。具体如下：</span></span></p><p><span style="letter-spacing: 1px;"><span style="color: rgb(89, 89, 89); --tt-darkmode-color: #595959;"><br></span></span></p><p><span style="letter-spacing: 1px;"><span style="color: rgb(89, 89, 89); --tt-darkmode-color: #595959;">首先，大型语言模型（Large language models，LLM）用于完成自然语言理解的任务，该模型经过了大型网络文本语料库上的训练，可以将用户给出的文本指令解析为一系列地标。LM-Nav 导航系统中选择的 LLM 正是知名的 GPT-3 模型。</span></span></p><p><span style="letter-spacing: 1px;"><span style="color: rgb(89, 89, 89); --tt-darkmode-color: #595959;"><br></span></span></p><p><span style="letter-spacing: 1px;"><span style="color: rgb(89, 89, 89); --tt-darkmode-color: #595959;">其次，视觉和语言模型（Vision-and-language models，VLM）可以将图像和文本所表达的信息进行关联。在导航任务中，视觉和语言模型可将用户指令中的地标，与机器人视觉感知到的周边环境进行关联。据介绍，该系统选择的视觉和语言模型是美国人工智能研究公司 OpenAI 的 CLIP 模型。</span></span></p><p><span style="letter-spacing: 1px;"><span style="color: rgb(89, 89, 89); --tt-darkmode-color: #595959;"><br></span></span></p><p><span style="letter-spacing: 1px;"><span style="color: rgb(89, 89, 89); --tt-darkmode-color: #595959;">第三，视觉导航模型（Visual navigation models，VNM）用于从其视觉观察的信息中直接进行导航工作，它可以将图像和之后执行的动作按时间进行关联。LM-Nav 系统选择了加州人工智能公司 DeepAI 的目标条件模型 ViNG 作为视觉导航模型。</span></span></p><p><span style="letter-spacing: 1px;"><span style="color: rgb(89, 89, 89); --tt-darkmode-color: #595959;"><br></span></span></p><div><img src="https://p6.toutiaoimg.com/origin/tos-cn-i-qvj2lq49k0/9ae855fd1fa442c38b92ad27352bb7a1" referrerpolicy="no-referrer">图｜LM-Nav 导航系统（来源：arXiv）</div><div><br><a href="http://www.sciphi.cn/article/view/undefined"></a><p style="text-align: center;"></p><p><span style="letter-spacing: 1px;"><span style="color: rgb(89, 89, 89); --tt-darkmode-color: #595959;">简单来说，LM-Nav 导航系统的主要工作过程如下图所示。</span></span></p><p><span style="letter-spacing: 1px;"><span style="color: rgb(89, 89, 89); --tt-darkmode-color: #595959;"><br></span></span></p><div><img src="https://p3.toutiaoimg.com/origin/tos-cn-i-qvj2lq49k0/ba39e818f81c43c6970c3ae9958cb390" referrerpolicy="no-referrer">图 ｜LM-Nav 导航系统的主要工作过程（来源：arXiv）</div><div><br><a href="http://www.sciphi.cn/article/view/undefined"></a><p style="text-align: center;"></p><p><span style="letter-spacing: 1px;"><span style="color: rgb(89, 89, 89); --tt-darkmode-color: #595959;">首先，系统以目的地环境的初始观察结果、以及用户给的文本指令作为输入，通过系统中的三个预训练模型得出执行计划：LLM 用于提取指令中的地标，VLM 用于将文本地标与图像关联，而 VNM 用于执行导航任务。正是有了这些，即使在复杂环境中，LM-Nav 也无需任何微调，完全根据实时视觉观察到的信息，来执行各种用户指令。</span></span></p><p><span style="letter-spacing: 1px;"><span style="color: rgb(89, 89, 89); --tt-darkmode-color: #595959;"><br></span></span></p><p><span style="letter-spacing: 1px;"><span style="color: rgb(89, 89, 89); --tt-darkmode-color: #595959;">为对这一系统进行评估，研究人员将 LM-Nav 模型在机器人研究平台 Clearpath Jackal UGV 上进行部署和应用。在这一平台的传感器套件，包含了一个 6 自由度的 IMU、一个用于近似定位的 GPS 单元、一个车轮编码器，以及用于进行视觉观察捕捉的 170°视野前后 RGB 摄像头。</span></span></p><p><span style="letter-spacing: 1px;"><span style="color: rgb(89, 89, 89); --tt-darkmode-color: #595959;"><br></span></span></p><p><span style="letter-spacing: 1px;"><span style="color: rgb(89, 89, 89); --tt-darkmode-color: #595959;">实验过程包括在不同难度的环境中，对该系统进行的 20 个导航测试，机器人共行走的总长度超过 6 千米。</span></span></p><div><br></div><div><img src="https://p3.toutiaoimg.com/origin/tos-cn-i-qvj2lq49k0/531149315ad9429f92433e5b8483cfb3" referrerpolicy="no-referrer">图｜LM-Nav 系统的应用，要求机器人在实际环境中按照用户指令执行任务（来源：arXiv）</div><div><br><a href="http://www.sciphi.cn/article/view/undefined"></a><p style="text-align: center;"></p><p><span style="letter-spacing: 1px;"><span style="color: rgb(89, 89, 89); --tt-darkmode-color: #595959;">如上图，左侧文本中下划线部分，为 LLM 提取出的地标；中间俯视图中标记的路标，为通过 VLM 进行语言-图像关联的结果；右侧为按照 VNM 执行导航的实况。</span></span></p><p><span style="letter-spacing: 1px;"><span style="color: rgb(89, 89, 89); --tt-darkmode-color: #595959;"><br></span></span></p><div><img src="https://p9.toutiaoimg.com/origin/tos-cn-i-qvj2lq49k0/39d376adfcfa466ba8296227fc3e13bd" referrerpolicy="no-referrer">图｜LM-Nav 系统与没有 VNM 的 GPS-Nav 系统的性能对比结果（来源：arXiv）</div><div><br><a href="http://www.sciphi.cn/article/view/undefined"></a><p style="text-align: center;"></p><p><span style="letter-spacing: 1px;"><span style="color: rgb(89, 89, 89); --tt-darkmode-color: #595959;">研究人员还引入了规划成功性、效率、平均人工干预次数等性能指标，以用于将 LM-Nav 的性能与 GPS-NAV 导航系统进行对比。结果显示，LM-Nav 在各方面的性能均优于 GPS-Nav 系统。</span></span></p><p><span style="letter-spacing: 1px;"><span style="color: rgb(89, 89, 89); --tt-darkmode-color: #595959;"><br></span></span></p><p><span style="letter-spacing: 1px;"><strong><span style="color: rgb(178, 178, 178); --tt-darkmode-color: #A3A3A3;">参考资料：</span></strong></span></p><p><span style="letter-spacing: 1px;"><span style="color: rgb(178, 178, 178); --tt-darkmode-color: #A3A3A3;">https://github.com/blazejosinski/lm_nav</span></span></p><p><span style="letter-spacing: 1px;"><span style="color: rgb(178, 178, 178); --tt-darkmode-color: #A3A3A3;">https://github.com/blazejosinski/lm_nav</span></span></p><p><span style="letter-spacing: 1px;"><span style="color: rgb(178, 178, 178); --tt-darkmode-color: #A3A3A3;"><br></span></span></p><div><img src="https://p26.toutiaoimg.com/origin/tos-cn-i-qvj2lq49k0/7a984e28339646fc9670178cfd5aaa9f" referrerpolicy="no-referrer"><a href="http://www.sciphi.cn/article/view/undefined"></a><div><br></div><div><img src="https://p6.toutiaoimg.com/origin/tos-cn-i-qvj2lq49k0/b0ea2e47e0ac48f38253027b05a2df72" referrerpolicy="no-referrer"><a href="http://www.sciphi.cn/article/view/undefined"></a></div></div></div></div></div></div></div></div>  
</div>
            