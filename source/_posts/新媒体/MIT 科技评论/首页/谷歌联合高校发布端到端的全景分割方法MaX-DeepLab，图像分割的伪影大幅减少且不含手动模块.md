
---
title: '谷歌联合高校发布端到端的全景分割方法MaX-DeepLab，图像分割的伪影大幅减少且不含手动模块'
categories: 
 - 新媒体
 - MIT 科技评论
 - 首页
headimg: 'https://p3.toutiaoimg.com/origin/tos-cn-i-qvj2lq49k0/6238aba8a40e4ec18025687cd4633a9b'
author: MIT 科技评论
comments: false
date: Sun, 07 Aug 2022 13:11:18 GMT
thumbnail: 'https://p3.toutiaoimg.com/origin/tos-cn-i-qvj2lq49k0/6238aba8a40e4ec18025687cd4633a9b'
---

<div>   
<div style="caret-color: rgb(0, 0, 0); color: rgb(0, 0, 0);"><img src="https://p3.toutiaoimg.com/origin/tos-cn-i-qvj2lq49k0/6238aba8a40e4ec18025687cd4633a9b" referrerpolicy="no-referrer"><a href="http://www.sciphi.cn/article/view/undefined"></a><p><span style="letter-spacing: 1px;"><span style="color: rgb(89, 89, 89); --tt-darkmode-color: #595959;"><br></span></span></p><p><span style="letter-spacing: 1px;"><span style="color: rgb(89, 89, 89); --tt-darkmode-color: #595959;">图像分割技术在计算机视觉领域十分重要，通过它计算机才能将图像中的不同对象进行分割，进而理解图像内容并用于下一步的处理，这一技术被用于自动驾驶、图像识别等中。</span></span></p><p><span style="letter-spacing: 1px;"><span style="color: rgb(89, 89, 89); --tt-darkmode-color: #595959;"><br></span></span></p><p><span style="letter-spacing: 1px;"><span style="color: rgb(89, 89, 89); --tt-darkmode-color: #595959;">一般来说图像分割任务是通过为属于同一类的每个像素分配标签来实现的，而其中的任务也通常涉及到对对象进行分类、检测和标记等。</span></span></p><p><span style="letter-spacing: 1px;"><span style="color: rgb(89, 89, 89); --tt-darkmode-color: #595959;"><br></span></span></p><p><span style="letter-spacing: 1px;"><span style="color: rgb(89, 89, 89); --tt-darkmode-color: #595959;">图像分割大体可以分为三类：语义分割、实例分割和全景分割。“语义分割旨在为图像中的所有像素分配一个类标签；而实例分割的目的则是清楚地检测出图片中某个类的每个实例；全景分割则融合了这两种任务，是近年来新兴的分支，它主要通过预测一组不重叠的掩码及其对应的类标签来完成任务，”如下图所示。</span></span></p><p><span style="letter-spacing: 1px;"><span style="color: rgb(89, 89, 89); --tt-darkmode-color: #595959;"><br></span></span></p><div><img src="https://p26.toutiaoimg.com/origin/tos-cn-i-qvj2lq49k0/62551cadfc8141ab91115e24f3a34e6a" referrerpolicy="no-referrer"></div><div>▲图｜语义分割、实例分割和全景分割（来源：opengenus）</div><div><br><a href="http://www.sciphi.cn/article/view/undefined"></a><p style="text-align: center;"></p><p><span style="letter-spacing: 1px;"><span style="color: rgb(89, 89, 89); --tt-darkmode-color: #595959;">目前的全景分割方法中的对掩码的预测这一步骤，都是通过采用多个代理子任务逐渐逼近目标来完成的。这使得之前的全景分割方法比较依赖代理子任务。</span></span></p><p><span style="letter-spacing: 1px;"><span style="color: rgb(89, 89, 89); --tt-darkmode-color: #595959;"><br></span></span></p><p><span style="letter-spacing: 1px;"><span style="color: rgb(89, 89, 89); --tt-darkmode-color: #595959;">例如 Panoptic-FPN 方法的架构，就是采用了含有三个代理子任务层级的”基于框的管道”，如下图中粉红颜色的数状结构所示，全景分割掩码任务被分解为合并语义和实例分割这两个子任务，而实例分割又被进一步分解为框检测和基于框的分割，而框检测是通过锚回归和锚分类实现的。</span></span></p><p><span style="letter-spacing: 1px;"><span style="color: rgb(89, 89, 89); --tt-darkmode-color: #595959;"><br></span></span></p><div><img src="https://p3.toutiaoimg.com/origin/tos-cn-i-qvj2lq49k0/ea665ae902f342ed8129f661d9ee5399" referrerpolicy="no-referrer">（来源：CVF）</div><div><br><a href="http://www.sciphi.cn/article/view/undefined"></a><p><span style="letter-spacing: 1px;"><span style="color: rgb(89, 89, 89); --tt-darkmode-color: #595959;">而其中的每个层级都含有手动设计的模块，这样，尽管这些子任务本身都有着很好的结果方案，但是当将其用于全景分割时，会为整体带来不希望有的伪影。</span></span></p><p><span style="letter-spacing: 1px;"><span style="color: rgb(89, 89, 89); --tt-darkmode-color: #595959;"><br></span></span></p><p><span style="letter-spacing: 1px;"><span style="color: rgb(89, 89, 89); --tt-darkmode-color: #595959;">为了解决这一问题，谷歌研究院联合大学专家提出了一种端到端的方法 MaX-DeepLab。该方法直接从图像中来预测全景分割的掩码，大大简化了整个过程，而且全程不涉及手动模块。</span></span></p><p><span style="letter-spacing: 1px;"><span style="color: rgb(89, 89, 89); --tt-darkmode-color: #595959;"><br></span></span></p><p><span style="letter-spacing: 1px;"><span style="color: rgb(89, 89, 89); --tt-darkmode-color: #595959;">这一研究由约翰霍普金斯大学的王慧宇（Huiyu Wang）和来自谷歌研究的朱裕琨（Yukun Zhu）等人共同完成。</span></span></p><p><span style="letter-spacing: 1px;"><span style="color: rgb(89, 89, 89); --tt-darkmode-color: #595959;"><br></span></span></p><p><span style="letter-spacing: 1px;"><span style="color: rgb(89, 89, 89); --tt-darkmode-color: #595959;">相关论文以《MaX-DeepLab：使用掩模转换器的端到端全景分割方法》（MaX-DeepLab: End-to-End Panoptic Segmentation with Mask Transformers）为题发表在了 CVF 上。</span></span></p><p><span style="letter-spacing: 1px;"><span style="color: rgb(89, 89, 89); --tt-darkmode-color: #595959;"><br></span></span></p><p><span style="letter-spacing: 1px;"><span style="color: rgb(89, 89, 89); --tt-darkmode-color: #595959;">MaX-DeepLab 方法的提出，受到了同样也是端到端的 DETR 方法的启发。但是与 DETR 方法不同</span></span><span style="color: rgb(89, 89, 89); letter-spacing: 1px;">的是，该方法并非采用框监测，而是采用了掩码转换器，来对一组掩码以及每个掩码所对应的语义标签进行预测。</span></p><p><span style="color: rgb(89, 89, 89); letter-spacing: 1px;"><br></span></p><p><span style="letter-spacing: 1px;"><span style="color: rgb(89, 89, 89); --tt-darkmode-color: #595959;">在该方法中，输出掩码和类的优化目标，采用的是全景质量（Panoptic Quality/PQ）。而两个标记为类的掩码之间的相似度度量，在该方法中则被定义为二者之间的掩码相似度以及两者的类相似度的乘积。</span></span></p><p><span style="letter-spacing: 1px;"><span style="color: rgb(89, 89, 89); --tt-darkmode-color: #595959;"><br></span></span></p><p><span style="letter-spacing: 1px;"><span style="color: rgb(89, 89, 89); --tt-darkmode-color: #595959;">该模型在被训练逐渐不断接近优化目标的过程中，采用了一对一的二分匹配、将地面真相掩码最大化以及预测掩码之间的相似性等方法。MaX-DeepLab 就是通过这样的方式，实现了端到端的训练和推理，并且有效去除了此前的无论是基于框的还是无框的方法中都含有的手工模块。</span></span></p><p><span style="letter-spacing: 1px;"><span style="color: rgb(89, 89, 89); --tt-darkmode-color: #595959;"><br></span></span></p><div><img src="https://p9.toutiaoimg.com/origin/tos-cn-i-qvj2lq49k0/37fd839ffa34454f8c63bd48479926e2" referrerpolicy="no-referrer">（来源：CVF）</div><div><br><a href="http://www.sciphi.cn/article/view/undefined"></a><p><span style="letter-spacing: 1px;"><span style="color: rgb(89, 89, 89); --tt-darkmode-color: #595959;">为了对 MaX-DeepLab 进行评估，研究团队还将其在目前最具挑战性的全景分割数据集之一 COCO 上进行了训练。同时团队还将结果与此前最先进的无框方法 Axial-DeepLab、以及之前最先进的基于框的方法 DetectoRS 在 COCO 上训练的结果进行了对比。</span></span></p><p><span style="letter-spacing: 1px;"><span style="color: rgb(89, 89, 89); --tt-darkmode-color: #595959;"><br></span></span></p><p><span style="letter-spacing: 1px;"><span style="color: rgb(89, 89, 89); --tt-darkmode-color: #595959;">结果显示，即使在没有测试时间增强 (Test-time augmentation/TTA) 的前提下，MaX-DeepLab 模型在数据集 COCO 中的训练最终也得出了 PQ 值为 51.3% 的好成绩。该成绩比使用 TTA 的 Axial-DeepLab 方法的 PQ 值高出了 7.1%，比使用 TTA 的 DetectorRS 也高出了 1.7%。</span></span></p><p><span style="letter-spacing: 1px;"><span style="color: rgb(89, 89, 89); --tt-darkmode-color: #595959;"><br></span></span></p><div><img src="https://p3.toutiaoimg.com/origin/tos-cn-i-qvj2lq49k0/f9cbd37c425f4f6a9702842ad5b2169c" referrerpolicy="no-referrer"></div><div>▲图｜不同全景分割方法在 COCO 中的验证集中的训练结果（来源：CVF）</div><div><br><a href="http://www.sciphi.cn/article/view/undefined"></a><p style="text-align: center;"></p><div><img src="https://p26.toutiaoimg.com/origin/tos-cn-i-qvj2lq49k0/8211bb5121e346d78c808803fa6806e7" referrerpolicy="no-referrer"></div><div>▲图｜不同全景分割方法在 COCO 中的测试开发集中的训练结果（来源：CVF）</div><div><br><a href="http://www.sciphi.cn/article/view/undefined"></a><p style="text-align: center;"></p><p><span style="letter-spacing: 1px;"><span style="color: rgb(89, 89, 89); --tt-darkmode-color: #595959;">另外，为了与同样是端到端的基于框的 DETR 方法进行对比，团队还对 MaX-DeepLab 进行了调整，并将其在 COCO 数据集中的验证集（val set）和测试开发集（test-dev set）上进行了训练。</span></span></p><p><span style="letter-spacing: 1px;"><span style="color: rgb(89, 89, 89); --tt-darkmode-color: #595959;"><br></span></span></p><p><span style="letter-spacing: 1px;"><span style="color: rgb(89, 89, 89); --tt-darkmode-color: #595959;">团队将该模型调整为参数数量和 M-Adds 量都和 DETR 相当的轻量级模型 MaX-DeepLab-S，结果显示，相比 DETR，MaX-DeepLab-S 模型的 PQ 值不仅在验证集中高出 3.3%，在测试开发集中也高出 3.0%。</span></span></p><p><span style="letter-spacing: 1px;"><span style="color: rgb(89, 89, 89); --tt-darkmode-color: #595959;"><br></span></span></p><p><span style="letter-spacing: 1px;"><strong><span style="color: rgb(178, 178, 178); --tt-darkmode-color: #A3A3A3;">参考资料：</span></strong></span></p><p><span style="letter-spacing: 1px;"><span style="color: rgb(178, 178, 178); --tt-darkmode-color: #A3A3A3;">https://iq.opengenus.org/panoptic-segmentation/</span></span></p><p><span style="letter-spacing: 1px;"><span style="color: rgb(178, 178, 178); --tt-darkmode-color: #A3A3A3;">https://openaccess.thecvf.com/content/CVPR2021/papers/Wang_MaX-DeepLab_End-to-End_Panoptic_Segmentation_With_Mask_Transformers_CVPR_2021_paper.pdf</span></span></p><p><span style="letter-spacing: 1px;"><span style="color: rgb(178, 178, 178); --tt-darkmode-color: #A3A3A3;"><br></span></span></p><div><img src="https://p26.toutiaoimg.com/origin/tos-cn-i-qvj2lq49k0/4b27839ad88d4723bad2ea8363955b59" referrerpolicy="no-referrer"><a href="http://www.sciphi.cn/article/view/undefined"></a></div></div></div></div></div></div></div>  
</div>
            