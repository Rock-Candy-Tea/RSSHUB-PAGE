
---
title: 'Language modelling at scale_ Gopher, ethical considerations, and retrieval'
categories: 
 - 新媒体
 - DeepMind
 - Blog
headimg: 'https://picsum.photos/400/300?random=7057'
author: DeepMind
comments: false
date: Wed, 08 Dec 2021 00:00:00 GMT
thumbnail: 'https://picsum.photos/400/300?random=7057'
---

<div>   
<p>Language, and its role in demonstrating and facilitating comprehension - or intelligence - is a fundamental part of being human. It gives people the ability to communicate thoughts and concepts, express ideas, create memories, and build mutual understanding. These are foundational parts of social intelligence. It’s why our teams at DeepMind study aspects of language processing and communication, both in artificial agents and in humans.</p>
<p>As part of a broader portfolio of AI research, we believe the development and study of more powerful language models – systems that predict and generate text –  have tremendous potential for building advanced AI systems that can be used safely and efficiently to summarise information, provide expert advice and follow instructions via natural language. Developing beneficial language models requires research into their potential impacts, including the risks they pose. This includes collaboration between experts from varied backgrounds to thoughtfully anticipate and address the challenges that training algorithms on existing datasets can create.</p>
<p>Today we are releasing three papers on language models that reflect this interdisciplinary approach. They include a detailed study of <a href="https://arxiv.org/abs/2112.11446" rel="noopener" target="_blank">a 280 billion parameter transformer language model called <em>Gopher</em></a>, <a href="https://arxiv.org/abs/2112.04359" rel="noopener" target="_blank">a study of ethical and social risks associated with large language models</a>, and <a href="https://arxiv.org/abs/2112.04426" rel="noopener" target="_blank">a paper investigating a new architecture with better training efficiency.</a></p>
<h5>Gopher - A 280 billion parameter language model</h5>
<p>In the quest to explore language models and develop new ones, we trained a series of transformer language models of different sizes, ranging from 44 million parameters to 280 billion parameters (the largest model we named <em>Gopher</em>).</p>
<p>Our research investigated the strengths and weaknesses of those different-sized models, highlighting areas where increasing the scale of a model continues to boost performance – for example, in areas like reading comprehension, fact-checking, and the identification of toxic language. We also surface results where model scale does not significantly improve results — for instance, in logical reasoning and common-sense tasks.</p>  
</div>
            