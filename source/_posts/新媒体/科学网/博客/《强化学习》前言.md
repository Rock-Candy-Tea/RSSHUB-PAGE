
---
title: '《强化学习》前言'
categories: 
 - 新媒体
 - 科学网
 - 博客
headimg: 'https://cors.zfour.workers.dev/?http://bbs.sciencenet.cn/home.php?mod=attachment&filename=image.png&id=949122'
author: 科学网
comments: false
date: Fri, 19 Aug 2022 09:30:00 GMT
thumbnail: 'https://cors.zfour.workers.dev/?http://bbs.sciencenet.cn/home.php?mod=attachment&filename=image.png&id=949122'
---

<div>   
<p style="font-family: Calibri; white-space: normal; line-height: 32px; text-indent: 2em;"><strong><span style="font-size: 19px; font-family: SimSun;">参考文献：</span></strong></p><p style="font-family: Calibri; white-space: normal; line-height: 24px; text-indent: 2em;"><span style="font-size: 16px"><span style="font-family:SimSun">魏庆来</span>, <span style="font-family:SimSun">王飞跃</span><span style="font-family:Calibri">. </span><span style="font-family:SimSun">强化学习 </span><span style="font-family:Calibri">[M].  </span><span style="font-family:SimSun">北京</span><span style="font-family:Calibri">: </span><span style="font-family:SimSun">清华大学出版社</span><span style="font-family:Calibri">, 2022.07.</span></span></p><p style="font-family: Calibri; white-space: normal; line-height: 24px; text-indent: 2em;"><span style="font-family: Arial;color: #222222;letter-spacing: 0;font-size: 16px;background-color: #FFFFFF"><span style="font-family:SimSun">王飞跃</span>, <span style="font-family:SimSun">曹东璞</span><span style="font-family:Arial">, </span><span style="font-family:SimSun">魏庆来</span><span style="font-family:Arial">. </span><span style="font-family:SimSun">强化学习</span><span style="font-family:Arial">: </span><span style="font-family:SimSun">迈向知行合一的智能机制与算法</span><span style="font-family:Arial">[J]. </span><span style="font-family:SimSun">智能科学与技术学报</span><span style="font-family:Arial">, 2020, 2(2): 101-106.</span></span></p><p style="font-family: Calibri; white-space: normal; line-height: 24px; text-indent: 2em;"><strong><span style="font-size: 19px"> </span></strong></p><p style=";font-family: Calibri;white-space: normal;text-align: center;line-height: 32px"><strong><span style="font-size: 24px; font-family: SimSun;">《强化学习》</span></strong><strong><span style="font-size: 24px; font-family: SimSun;">前言</span></strong></p><p style="font-family: Calibri; white-space: normal; line-height: 32px;"><strong><span style="font-size: 24px; font-family: SimSun;"><br></span></strong></p><p style=";font-family: Calibri;white-space: normal;text-align: center;line-height: 32px"><strong><span style="font-size: 19px"><span style="font-family:SimSun">强化学习：</span> <span style="font-family:SimSun">迈向知行合一的智能机制与算法</span></span></strong></p><p style="font-family: Calibri; white-space: normal; line-height: 32px;"><strong><span style="font-size: 19px"><span style="font-family:SimSun"><br></span></span></strong></p><p style=";font-family: Calibri;white-space: normal;text-align: center;line-height: 32px"><strong><span style="font-size: 19px; font-family: SimSun;">魏庆来、王飞跃</span></strong></p><p style="font-family: Calibri; white-space: normal; line-height: 32px;"><strong><span style="font-size: 19px; font-family: SimSun;"><br></span></strong></p><p style="margin: 12px 0;font-family: Calibri;white-space: normal;text-indent: 28px;text-align: justify"><span style="font-size: 16px; font-family: "times new roman";"><span style="font-size: 16px;">人工智能的异军突起，除计算能力和海量数据之外，最大的贡献者当属机器学习，其中最引人注目的核心技术与基础方法是深度学习和强化学习（</span>reinforment learning<span style="font-size: 16px;">），前者是前台的</span><span style="font-size: 16px;">“</span><span style="font-size: 16px;">明星</span><span style="font-size: 16px;">”</span><span style="font-size: 16px;">，后者是背后的</span><span style="font-size: 16px;">“</span><span style="font-size: 16px;">英雄</span><span style="font-size: 16px;">”[1]</span><span style="font-size: 16px;">。与新兴的深度学习相比，强化学习相对</span><span style="font-size: 16px;">“</span><span style="font-size: 16px;">古老</span><span style="font-size: 16px;">”</span><span style="font-size: 16px;">，其思想源自人类</span><span style="font-size: 16px;">“</span><span style="font-size: 16px;">趋利避害</span><span style="font-size: 16px;">”</span><span style="font-size: 16px;">和</span><span style="font-size: 16px;">“</span><span style="font-size: 16px;">吃一堑、长一智</span><span style="font-size: 16px;">”</span><span style="font-size: 16px;">的朴素意识，其最初的</span><span style="font-size: 16px;">“</span><span style="font-size: 16px;">尝试法</span><span style="font-size: 16px;">”</span><span style="font-size: 16px;">或</span><span style="font-size: 16px;">“</span><span style="font-size: 16px;">试错法</span><span style="font-size: 16px;">”</span><span style="font-size: 16px;">，远在人工智能技术出现之前就在各行各业广为流行，并成为人工智能起步时的核心技术之一。</span><span style="font-size: 16px;">AlphaGo </span><span style="font-size: 16px;">在围棋人机大战中的胜利使社会大众普遍认识到有监督的深度学习和无监督的强化学习的威力</span><span style="font-size: 16px;">[2,3,4]</span><span style="font-size: 16px;">。近年来，人工智能算法在一些多角色游戏中大胜人类顶级专业选手，使人们对强化学习的功力有了更加深刻的印象和理解。</span></span></p><p style="margin: 12px 0;font-family: Calibri;white-space: normal;text-indent: 28px;text-align: center"><span style="font-size: 16px; font-family: "times new roman";"><img src="https://cors.zfour.workers.dev/?http://bbs.sciencenet.cn/home.php?mod=attachment&filename=image.png&id=949122" title alt="image.png" referrerpolicy="no-referrer"> </span></p><p style="margin: 12px 0;font-family: Calibri;white-space: normal;text-indent: 28px;text-align: justify"><span style="font-size: 16px; font-family: "times new roman";"><span style="font-size: 16px; font-family: SimSun;">例如，以强化学习为核心技术之一的人工智能系统</span>Pluribus<span style="font-size: 16px; font-family: SimSun;">在六人桌无限制的德州扑克比赛中，在一万手回合里分别以单机对五人和五机对单人的方式，共击败 </span><span style="font-size: 16px; font-family: Calibri;">15 </span><span style="font-size: 16px; font-family: SimSun;">名全球最佳专业玩家，突破了过去人工智能仅能在国际象棋等二人游戏中战胜人类的局限，成为游戏中机器胜人的又一个里程碑，被《科学》杂志评选为</span><span style="font-size: 16px; font-family: Calibri;">2019 </span><span style="font-size: 16px; font-family: SimSun;">年十大科学突破之一。</span><span style="font-size: 16px; font-family: Calibri;">Pluribus</span><span style="font-size: 16px; font-family: SimSun;">这项工作之所以重要，主要原因如下。</span></span></p><p style="margin: 12px 0;font-family: Calibri;white-space: normal;text-indent: 28px;text-align: justify"><span style="font-size: 16px; font-family: "times new roman";"> </span></p><p style="margin: 12px 0;font-family: Calibri;white-space: normal;text-indent: 28px;text-align: justify"><span style="font-size: 16px; font-family: "times new roman";"><span style="font-size: 16px; font-family: Calibri;">• </span><span style="font-size: 16px; font-family: SimSun;">人工智能算法必须处理不完备信息，需要在不知道对手策略和资源的情况下进行决策，并在不同博弈之间寻求平衡。</span></span></p><p style="margin: 12px 0;font-family: Calibri;white-space: normal;text-indent: 28px;text-align: justify"><span style="font-size: 16px; font-family: "times new roman";"> </span></p><p style="margin: 12px 0;font-family: Calibri;white-space: normal;text-indent: 28px;text-align: justify"><span style="font-size: 16px; font-family: "times new roman";"><span style="font-size: 16px; font-family: Calibri;">• </span><span style="font-size: 16px; font-family: SimSun;">博弈最佳的理论结果是纳什平衡，但随着玩家数目的增加，求解纳什平衡的计算复杂度呈指数增长，算法要求的算力在物理上不可能实现，必须引入智力。</span></span></p><p style="margin: 12px 0;font-family: Calibri;white-space: normal;text-indent: 28px;text-align: justify"><span style="font-size: 16px; font-family: "times new roman";"> </span></p><p style="margin: 12px 0;font-family: Calibri;white-space: normal;text-indent: 28px;text-align: justify"><span style="font-size: 16px; font-family: "times new roman";"><span style="font-size: 16px; font-family: Calibri;">• </span><span style="font-size: 16px; font-family: SimSun;">掌握</span><span style="font-size: 16px; font-family: Calibri;">“</span><span style="font-size: 16px; font-family: SimSun;">诈唬</span><span style="font-size: 16px; font-family: Calibri;">”</span><span style="font-size: 16px; font-family: SimSun;">等心理技巧是游戏胜利的关键之一，必须考虑并采用此类心理</span><span style="font-size: 16px; font-family: Calibri;">“</span><span style="font-size: 16px; font-family: SimSun;">算计</span><span style="font-size: 16px; font-family: Calibri;">”</span><span style="font-size: 16px; font-family: SimSun;">，在博弈中有效推理并隐藏意图，产生让对手难以预测和分析的策略。</span></span></p><p style="margin: 12px 0;font-family: Calibri;white-space: normal;text-indent: 28px;text-align: justify"><span style="font-size: 16px; font-family: "times new roman";"> </span></p><p style="margin: 12px 0;font-family: Calibri;white-space: normal;text-indent: 28px;text-align: justify"><span style="font-size: 16px; font-family: "times new roman";"><span style="font-size: 16px; font-family: SimSun;">解决这些问题正是人工智能进一步发展必须面对的核心任务，也是强化学习之所以关键的主要因素</span>[5]<span style="font-size: 16px; font-family: SimSun;">。这些问题的有效解决和其解决方案的广泛应用，不但可为多角色、多玩家场景下的博弈和电子竞技做出贡献，更将为人工智能在工业控制、商务决策、企业管理和军事控制等重大领域的大规模实际应用提供有效的方法和坚实的技术支撑。</span></span></p><p style="margin: 12px 0;font-family: Calibri;white-space: normal;text-indent: 28px;text-align: justify"><span style="font-size: 16px; font-family: "times new roman";"> </span></p><p style="margin: 12px 0;font-family: Calibri;white-space: normal;text-indent: 28px;text-align: justify"><span style="font-size: 16px; font-family: "times new roman";"><span style="font-size: 16px; font-family: SimSun;">强化学习为何有如此强大的功能和作用？其实强化学习的发展经历了漫长而曲折的过程，与有监督的学习方法不同，强化学习面对的是更加复杂艰巨而且</span><span style="font-size: 16px; font-family: Calibri;">“</span><span style="font-size: 16px; font-family: SimSun;">不知对错、无论好坏</span><span style="font-size: 16px; font-family: Calibri;">”</span><span style="font-size: 16px; font-family: SimSun;">的学习任务：决策或行动实施之前，没有关于正确与错误的理性推断依据；决策实施之后，没有关于好与坏的客观评价依据。然而，一百年来，科学家们坚持不懈地尝试了许多方法，包括经典条件反射（</span><span style="font-size: 16px; font-family: Calibri;">classical conditioning</span><span style="font-size: 16px; font-family: SimSun;">）、试错法（</span><span style="font-size: 16px; font-family: Calibri;">trial and error method</span><span style="font-size: 16px; font-family: SimSun;">）等</span><span style="font-size: 16px; font-family: Calibri;">“</span><span style="font-size: 16px; font-family: SimSun;">先行后知</span><span style="font-size: 16px; font-family: Calibri;">”</span><span style="font-size: 16px; font-family: SimSun;">的动物学习方法，系统模型、价值函数、动态规划、学习控制等</span><span style="font-size: 16px; font-family: Calibri;">“</span><span style="font-size: 16px; font-family: SimSun;">先知后行</span><span style="font-size: 16px; font-family: Calibri;">”</span><span style="font-size: 16px; font-family: SimSun;">的最优控制方法以及集估计、预测、自适应等于一体的时序差分（</span><span style="font-size: 16px; font-family: Calibri;">temporal difference</span><span style="font-size: 16px; font-family: SimSun;">）学习方法</span><span style="font-size: 16px; font-family: Calibri;">[6,7]</span><span style="font-size: 16px; font-family: SimSun;">。目前，强化学习正在整合算力、数据、知识图谱、逻辑推理、智能控制和知识自动化等技术，统一关于现状、回顾、展望等因素的分析，迈向</span><span style="font-size: 16px; font-family: Calibri;">“</span><span style="font-size: 16px; font-family: SimSun;">知行合一</span><span style="font-size: 16px; font-family: Calibri;">”</span><span style="font-size: 16px; font-family: SimSun;">的复杂自适应智能机制与算法体系。图</span><span style="font-size: 16px; font-family: Calibri;">1</span><span style="font-size: 16px; font-family: SimSun;">给出了由</span><span style="font-size: 16px; font-family: Calibri;">F.Woergoetter</span><span style="font-size: 16px; font-family: SimSun;">和</span><span style="font-size: 16px; font-family: Calibri;">B.Porr</span><span style="font-size: 16px; font-family: SimSun;">总结的强化学习前因后世</span><span style="font-size: 16px; font-family: Calibri;">[8]</span><span style="font-size: 16px; font-family: SimSun;">，比较完整地反映了这一方法的核心内容与相关问题。</span></span></p><p style="margin: 12px 0;font-family: Calibri;white-space: normal;text-indent: 28px;text-align: justify"><span style="font-size: 16px; font-family: "times new roman";"> </span></p><p style="margin: 12px 0;font-family: Calibri;white-space: normal;text-indent: 32px;text-align: justify"><span style="font-size: 16px; font-family: "times new roman";"><strong> <span style="font-size: 16px; font-family: SimSun;">先行后知的起步</span></strong></span></p><p style="margin: 12px 0;font-family: Calibri;white-space: normal;text-indent: 28px;text-align: justify"><span style="font-size: 16px; font-family: "times new roman";"> </span></p><p style="margin: 12px 0;font-family: Calibri;white-space: normal;text-indent: 28px;text-align: justify"><span style="font-size: 16px; font-family: "times new roman";"><span style="font-size: 16px; font-family: SimSun;">作为一项科学研究，强化学习始于</span><span style="font-size: 16px; font-family: Calibri;">“</span><span style="font-size: 16px; font-family: SimSun;">摸着石子过河</span><span style="font-size: 16px; font-family: Calibri;">”</span><span style="font-size: 16px; font-family: SimSun;">的启发式思维，在学术文献上可追溯到英国著名学者 </span><span style="font-size: 16px; font-family: Calibri;">Alexander Bain</span><span style="font-size: 16px; font-family: SimSun;">的</span><span style="font-size: 16px; font-family: Calibri;">“</span><span style="font-size: 16px; font-family: SimSun;">摸索与实验（</span><span style="font-size: 16px; font-family: Calibri;">groping and experiment</span><span style="font-size: 16px; font-family: SimSun;">）</span><span style="font-size: 16px; font-family: Calibri;">”</span><span style="font-size: 16px; font-family: SimSun;">学习原理。</span><span style="font-size: 16px; font-family: Calibri;">Alexander Bain</span><span style="font-size: 16px; font-family: SimSun;">是著名杂志</span><span style="font-size: 16px; font-family: Calibri;">Mind</span><span style="font-size: 16px; font-family: SimSun;">的创办人，正是这份杂志于</span><span style="font-size: 16px; font-family: Calibri;">1950</span><span style="font-size: 16px; font-family: SimSun;">年发表了 </span><span style="font-size: 16px; font-family: Calibri;">Alan Turing </span><span style="font-size: 16px; font-family: SimSun;">的文章</span><span style="font-size: 16px; font-family: Calibri;">“</span><span style="font-size: 16px; font-family: SimSun;">计算机器与智能（</span><span style="font-size: 16px; font-family: Calibri;">computing machinery and intelligence</span><span style="font-size: 16px; font-family: SimSun;">）</span><span style="font-size: 16px; font-family: Calibri;">”</span><span style="font-size: 16px; font-family: SimSun;">，提出使用</span><span style="font-size: 16px; font-family: Calibri;">“</span><span style="font-size: 16px; font-family: SimSun;">图灵测试</span><span style="font-size: 16px; font-family: Calibri;">”</span><span style="font-size: 16px; font-family: SimSun;">判断机器智能水平，开启了人工智能研究领域。作为一种方法，强化学习源自</span><span style="font-size: 16px; font-family: Calibri;">“</span><span style="font-size: 16px; font-family: SimSun;">试错学习（</span><span style="font-size: 16px; font-family: Calibri;">trial-and-error learning</span><span style="font-size: 16px; font-family: SimSun;">）</span><span style="font-size: 16px; font-family: Calibri;">”</span><span style="font-size: 16px; font-family: SimSun;">，由英国生物和心理学家 </span><span style="font-size: 16px; font-family: Calibri;">Conway Morgan </span><span style="font-size: 16px; font-family: SimSun;">正式提出，并以</span><span style="font-size: 16px; font-family: Calibri;">“</span><span style="font-size: 16px; font-family: SimSun;">摩根法则（</span><span style="font-size: 16px; font-family: Calibri;">Morgan’s canon</span><span style="font-size: 16px; font-family: SimSun;">）</span><span style="font-size: 16px; font-family: Calibri;">”</span><span style="font-size: 16px; font-family: SimSun;">为指导原则，即尽可能用低级心理功能解释生物行为的节约原则，后被美国心理学家、学习理论专家、联结主义创始人之一的</span><span style="font-size: 16px; font-family: Calibri;">Edward Thorndike</span><span style="font-size: 16px; font-family: SimSun;">进一步简化为</span><span style="font-size: 16px; font-family: Calibri;">“</span><span style="font-size: 16px; font-family: SimSun;">效果定律（</span><span style="font-size: 16px; font-family: Calibri;">law of effect</span><span style="font-size: 16px; font-family: SimSun;">）</span><span style="font-size: 16px; font-family: Calibri;">”</span><span style="font-size: 16px; font-family: SimSun;">，这成为后来的学习规则</span><span style="font-size: 16px; font-family: Calibri;">—— Hebb </span><span style="font-size: 16px; font-family: SimSun;">定律和神经网络误差反向传播（</span><span style="font-size: 16px; font-family: Calibri;">back propagation</span><span style="font-size: 16px; font-family: SimSun;">，</span><span style="font-size: 16px; font-family: Calibri;">BP</span><span style="font-size: 16px; font-family: SimSun;">）算法的鼻祖。强化学习的正式出现要归功于生理学家巴甫洛夫及其经典条件反射理论和激励响应（</span><span style="font-size: 16px; font-family: Calibri;">stimulus-response</span><span style="font-size: 16px; font-family: SimSun;">）理论，特别是他通过狗进行的一系列刺激反应试验总结出来的条件反射定律。在美国，心理学家</span><span style="font-size: 16px; font-family: Calibri;">Burrhus Frederic Skinner</span><span style="font-size: 16px; font-family: SimSun;">提出的工具条件反射（</span><span style="font-size: 16px; font-family: Calibri;">operant or instrumental conditioning</span><span style="font-size: 16px; font-family: SimSun;">）和工具学习（</span><span style="font-size: 16px; font-family: Calibri;">instrumental learning</span><span style="font-size: 16px; font-family: SimSun;">）及其利用老鼠进行试验的 </span><span style="font-size: 16px; font-family: Calibri;">Skinner-Box </span><span style="font-size: 16px; font-family: SimSun;">技术也推动了强化学习的行为分析试错法研究。</span></span></p><p style="margin: 12px 0;font-family: Calibri;white-space: normal;text-indent: 28px;text-align: justify"><span style="font-size: 16px; font-family: "times new roman";"> </span></p><p style="margin: 12px 0;font-family: Calibri;white-space: normal;text-indent: 28px;text-align: justify"><span style="font-size: 16px; font-family: "times new roman";"><span style="font-size: 16px; font-family: SimSun;">自此之后，强化学习在动物行为研究、生理和心理学以及认知科学等领域发挥了重要作用，成为相应的核心方法与技术。在人工智能之初，从图灵基于效果定律的</span><span style="font-size: 16px; font-family: Calibri;">“</span><span style="font-size: 16px; font-family: SimSun;">快乐</span><span style="font-size: 16px; font-family: Calibri;">-</span><span style="font-size: 16px; font-family: SimSun;">痛苦系统（</span><span style="font-size: 16px; font-family: Calibri;">pleasure-pain system</span><span style="font-size: 16px; font-family: SimSun;">）</span><span style="font-size: 16px; font-family: Calibri;">”</span><span style="font-size: 16px; font-family: SimSun;">、</span><span style="font-size: 16px; font-family: Calibri;">Marvin Minsky</span><span style="font-size: 16px; font-family: SimSun;">基于加强学习的随机神经模拟强化计算器（</span><span style="font-size: 16px; font-family: Calibri;">stochastic neural analog reinforcement calculator</span><span style="font-size: 16px; font-family: SimSun;">，</span><span style="font-size: 16px; font-family: Calibri;">SNARC</span><span style="font-size: 16px; font-family: SimSun;">）及其</span><span style="font-size: 16px; font-family: Calibri;">“</span><span style="font-size: 16px; font-family: SimSun;">迈向人工智能（</span><span style="font-size: 16px; font-family: Calibri;">steps toward artificial intelligence</span><span style="font-size: 16px; font-family: SimSun;">）</span><span style="font-size: 16px; font-family: Calibri;">”</span><span style="font-size: 16px; font-family: SimSun;">一文中提出的复杂强化学习的</span><span style="font-size: 16px; font-family: Calibri;">“</span><span style="font-size: 16px; font-family: SimSun;">基本信用（功劳）分配问题</span><span style="font-size: 16px; font-family: Calibri;">”</span><span style="font-size: 16px; font-family: SimSun;">，再到 </span><span style="font-size: 16px; font-family: Calibri;">Donald Michie </span><span style="font-size: 16px; font-family: SimSun;">基于强化学习的 </span><span style="font-size: 16px; font-family: Calibri;">MENACE </span><span style="font-size: 16px; font-family: SimSun;">和</span><span style="font-size: 16px; font-family: Calibri;">GLEE</span><span style="font-size: 16px; font-family: SimSun;">学习引擎、</span><span style="font-size: 16px; font-family: Calibri;">Nils Nilsson</span><span style="font-size: 16px; font-family: SimSun;">学习自动机（</span><span style="font-size: 16px; font-family: Calibri;">learning automata</span><span style="font-size: 16px; font-family: SimSun;">）和学习机器（</span><span style="font-size: 16px; font-family: Calibri;">learning machines</span><span style="font-size: 16px; font-family: SimSun;">），还有</span><span style="font-size: 16px; font-family: Calibri;">John Holland</span><span style="font-size: 16px; font-family: SimSun;">的</span><span style="font-size: 16px; font-family: Calibri;">“</span><span style="font-size: 16px; font-family: SimSun;">分类系统（</span><span style="font-size: 16px; font-family: Calibri;">classifier systems</span><span style="font-size: 16px; font-family: SimSun;">）</span><span style="font-size: 16px; font-family: Calibri;">”</span><span style="font-size: 16px; font-family: SimSun;">及其遗传算法，强化学习的思想和方法对许多人工智能机制和算法的设计产生了深刻的影响。然而，相对于许多机器学习方法而言，人们对强化学习的期望远大于其成果，在相当长的时间里，强化学习实际上并不是人工智能及其相关领域的主流方法和技术。</span></span></p><p style="margin: 12px 0px; font-family: Calibri; white-space: normal; text-align: center; text-indent: 0em;"><span style="font-size: 16px; font-family: "times new roman";"><img src="https://cors.zfour.workers.dev/?http://bbs.sciencenet.cn/home.php?mod=attachment&filename=image.png&id=949124" title alt="image.png" width="590" height="336" style="width: 590px; height: 336px;" referrerpolicy="no-referrer"> </span></p><p style="margin: 12px 0;font-family: Calibri;white-space: normal;text-indent: 28px;text-align: center"><span style="font-size: 16px; font-family: "times new roman";"><span style="font-size: 16px; font-family: SimSun;">图</span>1   <span style="font-size: 16px; font-family: SimSun;">强化学习的核心内容与相关问题</span></span></p><p style="margin: 12px 0;font-family: Calibri;white-space: normal;text-align: justify"><span style="font-size: 16px; font-family: "times new roman";"> </span></p><p style="margin: 12px 0;font-family: Calibri;white-space: normal;text-indent: 32px;text-align: justify"><span style="font-size: 16px; font-family: "times new roman";"><strong><span style="font-size: 16px; font-family: SimSun;">先知后行的重铸</span></strong></span></p><p style="margin: 12px 0;font-family: Calibri;white-space: normal;text-indent: 28px;text-align: justify"><span style="font-size: 16px; font-family: "times new roman";"> </span></p><p style="margin: 12px 0;font-family: Calibri;white-space: normal;text-indent: 28px;text-align: justify"><span style="font-size: 16px; font-family: "times new roman";"><span style="font-size: 16px; font-family: SimSun;">基于生物和心理学并以试错法为主的强化学习没有用到太多的数学概念和工具，而且也很少在工程上应用，直到</span>20<span style="font-size: 16px; font-family: SimSun;">世纪</span><span style="font-size: 16px; font-family: Calibri;">50</span><span style="font-size: 16px; font-family: SimSun;">年代，随着工程数学化的深入和现代控制理论的兴起，特别是基于系统动力学模型的最优控制的出现，加上 </span><span style="font-size: 16px; font-family: Calibri;">Richard Bellman </span><span style="font-size: 16px; font-family: SimSun;">的杰出工作，强化学习走上了一条数学化和工程应用的崭新道路，局面大为改观：朴素的奖励惩罚变成了</span><span style="font-size: 16px; font-family: Calibri;">“</span><span style="font-size: 16px; font-family: SimSun;">价值函数（</span><span style="font-size: 16px; font-family: Calibri;">value function</span><span style="font-size: 16px; font-family: SimSun;">）</span><span style="font-size: 16px; font-family: Calibri;">”</span><span style="font-size: 16px; font-family: SimSun;">，简单的行为选择升华为</span><span style="font-size: 16px; font-family: Calibri;">“</span><span style="font-size: 16px; font-family: SimSun;">动态规划（</span><span style="font-size: 16px; font-family: Calibri;">dynamic programming</span><span style="font-size: 16px; font-family: SimSun;">）</span><span style="font-size: 16px; font-family: Calibri;">”</span><span style="font-size: 16px; font-family: SimSun;">，非线性随机微分方程来了，伊藤积分（</span><span style="font-size: 16px; font-family: Calibri;">Ito integral</span><span style="font-size: 16px; font-family: SimSun;">）用上了，马尔可夫随机过程成了离散情况下的标配，有时还必须引入博弈论。强化学习从极其具体实在的动物行为学习突然变为十分复杂抽象的马尔可夫决策过程（</span><span style="font-size: 16px; font-family: Calibri;">Markov decision process</span><span style="font-size: 16px; font-family: SimSun;">，</span><span style="font-size: 16px; font-family: Calibri;">MDP</span><span style="font-size: 16px; font-family: SimSun;">）和</span><span style="font-size: 16px; font-family: Calibri;">Bellman</span><span style="font-size: 16px; font-family: SimSun;">方程，甚至是更难认知和求解的哈密顿</span><span style="font-size: 16px; font-family: Calibri;">-</span><span style="font-size: 16px; font-family: SimSun;">雅可比</span><span style="font-size: 16px; font-family: Calibri;">-</span><span style="font-size: 16px; font-family: SimSun;">贝尔曼（</span><span style="font-size: 16px; font-family: Calibri;">Hamilton-Jacobi-Bellman</span><span style="font-size: 16px; font-family: SimSun;">，</span><span style="font-size: 16px; font-family: Calibri;">BHJ</span><span style="font-size: 16px; font-family: SimSun;">）偏微分方程。一时间，原来</span><span style="font-size: 16px; font-family: Calibri;">“</span><span style="font-size: 16px; font-family: SimSun;">先行后知</span><span style="font-size: 16px; font-family: Calibri;">”</span><span style="font-size: 16px; font-family: SimSun;">的试错行为不见了，取而代之的是</span><span style="font-size: 16px; font-family: Calibri;">“</span><span style="font-size: 16px; font-family: SimSun;">先知后行</span><span style="font-size: 16px; font-family: Calibri;">”</span><span style="font-size: 16px; font-family: SimSun;">式的方程求解，尽管看起来有些</span><span style="font-size: 16px; font-family: Calibri;">“</span><span style="font-size: 16px; font-family: SimSun;">风马牛不相及</span><span style="font-size: 16px; font-family: Calibri;">”</span><span style="font-size: 16px; font-family: SimSun;">，但其一下子成为控制理论与工程的一部分，让许多研究者惊奇之余看到希望和曙光。</span></span></p><p style="margin: 12px 0;font-family: Calibri;white-space: normal;text-indent: 28px;text-align: justify"><span style="font-size: 16px; font-family: "times new roman";"> </span></p><p style="margin: 12px 0;font-family: Calibri;white-space: normal;text-indent: 28px;text-align: justify"><span style="font-size: 16px; font-family: "times new roman";"><span style="font-size: 16px; font-family: SimSun;">然而，这道曙光仅带来了短暂的黎明，很快又沉入</span><span style="font-size: 16px; font-family: Calibri;">“</span><span style="font-size: 16px; font-family: SimSun;">黑暗</span><span style="font-size: 16px; font-family: Calibri;">”</span><span style="font-size: 16px; font-family: SimSun;">，强化学习在新的道路上刚起步就遭遇</span><span style="font-size: 16px; font-family: Calibri;">“</span><span style="font-size: 16px; font-family: SimSun;">维数灾难（</span><span style="font-size: 16px; font-family: Calibri;">curse of dimensionality</span><span style="font-size: 16px; font-family: SimSun;">）</span><span style="font-size: 16px; font-family: Calibri;">”</span><span style="font-size: 16px; font-family: SimSun;">，当问题变得复杂（维数增加）时，动态规划求解方程的计算量呈指数增加，没有计算机可以应对，强化学习的</span><span style="font-size: 16px; font-family: Calibri;">“</span><span style="font-size: 16px; font-family: SimSun;">先知后行</span><span style="font-size: 16px; font-family: Calibri;">”</span><span style="font-size: 16px; font-family: SimSun;">变得无法实施。</span></span></p><p style="margin: 12px 0;font-family: Calibri;white-space: normal;text-indent: 28px;text-align: justify"><span style="font-size: 16px; font-family: "times new roman";"> </span></p><p style="margin: 12px 0;font-family: Calibri;white-space: normal;text-indent: 28px;text-align: justify"><span style="font-size: 16px; font-family: "times new roman";"><span style="font-size: 16px; font-family: SimSun;">为了克服</span><span style="font-size: 16px; font-family: Calibri;">“</span><span style="font-size: 16px; font-family: SimSun;">维数灾难</span><span style="font-size: 16px; font-family: Calibri;">”</span><span style="font-size: 16px; font-family: SimSun;">，智能这面旗帜被再次举起，最优控制从以数学推理为主演化为以智能技术为主的学习控制和智能控制。智能控制最初的代表人物是美国普渡大学的</span><span style="font-size: 16px; font-family: Calibri;">King-Sun Fu</span><span style="font-size: 16px; font-family: SimSun;">和</span><span style="font-size: 16px; font-family: Calibri;">George N.Saridis</span><span style="font-size: 16px; font-family: SimSun;">，后来两人分别成为模式识别和机器人与自动化领域的创始人和早期开拓者之一。受当时人工智能逻辑化、解析化思潮的影响，智能控制在</span><span style="font-size: 16px; font-family: Calibri;">30 </span><span style="font-size: 16px; font-family: SimSun;">多年的初创时期主要围绕形式语言、语法分析、决策自动机、图式学习、随机逼近、蒙特卡洛法、最小二乘法、参数识别、自适应算法、自组织系统、迭代学习、强化学习等技术展开，并被应用于模式识别、机器人与自动化、无人系统、计算机集成制造、金融科技等领域，但无论是在规模还是在效益方面都没有完全摆脱</span><span style="font-size: 16px; font-family: Calibri;">“</span><span style="font-size: 16px; font-family: SimSun;">维数灾难</span><span style="font-size: 16px; font-family: Calibri;">”</span><span style="font-size: 16px; font-family: SimSun;">的阴影，其发展到</span><span style="font-size: 16px; font-family: Calibri;">20</span><span style="font-size: 16px; font-family: SimSun;">世纪 </span><span style="font-size: 16px; font-family: Calibri;">90 </span><span style="font-size: 16px; font-family: SimSun;">年代中期就陷入瓶颈，相关工作几乎停滞不前。</span></span></p><p style="margin: 12px 0;font-family: Calibri;white-space: normal;text-indent: 28px;text-align: justify"><span style="font-size: 16px; font-family: "times new roman";"> </span></p><p style="margin: 12px 0;font-family: Calibri;white-space: normal;text-indent: 28px;text-align: justify"><span style="font-size: 16px; font-family: "times new roman";"><span style="font-size: 16px; font-family: Calibri;">“</span><span style="font-size: 16px; font-family: SimSun;">山重水复疑无路，柳暗花明又一村</span><span style="font-size: 16px; font-family: Calibri;">”</span><span style="font-size: 16px; font-family: SimSun;">。</span><span style="font-size: 16px; font-family: Calibri;">Paul Werbos</span><span style="font-size: 16px; font-family: SimSun;">在</span><span style="font-size: 16px; font-family: Calibri;">20</span><span style="font-size: 16px; font-family: SimSun;">世纪</span><span style="font-size: 16px; font-family: Calibri;">70</span><span style="font-size: 16px; font-family: SimSun;">年代中期推出神经元网络误差反向传播算法的同时，就开始研究新优化方法在策略分析中的应用，并于</span><span style="font-size: 16px; font-family: Calibri;">20</span><span style="font-size: 16px; font-family: SimSun;">世纪</span><span style="font-size: 16px; font-family: Calibri;">80</span><span style="font-size: 16px; font-family: SimSun;">年代末正式提出近似动态规划（</span><span style="font-size: 16px; font-family: Calibri;">approximate dynamic programming</span><span style="font-size: 16px; font-family: SimSun;">）的思想。同一时期，</span><span style="font-size: 16px; font-family: Calibri;">Saridis</span><span style="font-size: 16px; font-family: SimSun;">和王飞跃也针对非线性确定系统和随机系统提出了类似的次优控制迭代策略。经过</span><span style="font-size: 16px; font-family: Calibri;">Wurren B.Powell</span><span style="font-size: 16px; font-family: SimSun;">、</span><span style="font-size: 16px; font-family: Calibri;">Dimitri Panteli Bertsekas</span><span style="font-size: 16px; font-family: SimSun;">、</span><span style="font-size: 16px; font-family: Calibri;">John N.Tsitsiklis</span><span style="font-size: 16px; font-family: SimSun;">等人的研究，这一方法进一步与神经网络技术结合，从近似动态规划发展到神经动态规划（</span><span style="font-size: 16px; font-family: Calibri;">neuro-dynamic programming</span><span style="font-size: 16px; font-family: SimSun;">），最后发展到目前的自适应动态规划（</span><span style="font-size: 16px; font-family: Calibri;">adaptive dynamic programming</span><span style="font-size: 16px; font-family: SimSun;">，</span><span style="font-size: 16px; font-family: Calibri;">ADP</span><span style="font-size: 16px; font-family: SimSun;">）。自</span><span style="font-size: 16px; font-family: Calibri;">20</span><span style="font-size: 16px; font-family: SimSun;">世纪</span><span style="font-size: 16px; font-family: Calibri;">80</span><span style="font-size: 16px; font-family: SimSun;">年代中期以来，王飞跃、刘德荣和魏庆来从不同角度开展</span><span style="font-size: 16px; font-family: Calibri;">ADP </span><span style="font-size: 16px; font-family: SimSun;">相关研究工作，经过十余年的努力，形成中国科学院自动化研究所复杂系统管理与控制国家重点实验室自适应动态规划团体，致力于</span><span style="font-size: 16px; font-family: Calibri;">ADP</span><span style="font-size: 16px; font-family: SimSun;">方法的进一步发展和应用，从智能控制的角度推动了强化学习的理论研究与工程实践。</span></span></p><p style="margin: 12px 0;font-family: Calibri;white-space: normal;text-indent: 28px;text-align: justify"><span style="font-size: 16px; font-family: "times new roman";"> </span></p><p style="margin: 12px 0;font-family: Calibri;white-space: normal;text-indent: 32px;text-align: justify"><span style="font-size: 16px; font-family: "times new roman";"><strong><span style="font-size: 16px; font-family: SimSun;">时序差分的再生</span></strong></span></p><p style="margin: 12px 0;font-family: Calibri;white-space: normal;text-indent: 28px;text-align: justify"><span style="font-size: 16px; font-family: "times new roman";"> </span></p><p style="margin: 12px 0;font-family: Calibri;white-space: normal;text-indent: 28px;text-align: justify"><span style="font-size: 16px; font-family: "times new roman";"><span style="font-size: 16px; font-family: SimSun;">尽管</span>Werbos<span style="font-size: 16px; font-family: SimSun;">在</span><span style="font-size: 16px; font-family: Calibri;">20</span><span style="font-size: 16px; font-family: SimSun;">世纪</span><span style="font-size: 16px; font-family: Calibri;">70</span><span style="font-size: 16px; font-family: SimSun;">年代末就试图整合统一试错学习和最优控制的学习方法，但在相当长的时间里，基于这两种方法的强化学习几乎各自独立、没有交叉，直到</span><span style="font-size: 16px; font-family: Calibri;">20</span><span style="font-size: 16px; font-family: SimSun;">世纪</span><span style="font-size: 16px; font-family: Calibri;">80</span><span style="font-size: 16px; font-family: SimSun;">年代以</span><span style="font-size: 16px; font-family: Calibri;">Andrem G.Barto</span><span style="font-size: 16px; font-family: SimSun;">、</span><span style="font-size: 16px; font-family: Calibri;">Richard S.Sutton</span><span style="font-size: 16px; font-family: SimSun;">和</span><span style="font-size: 16px; font-family: Calibri;">Charles W.Anderson</span><span style="font-size: 16px; font-family: SimSun;">为核心的学者重新推出时序差分（</span><span style="font-size: 16px; font-family: Calibri;">temporal difference</span><span style="font-size: 16px; font-family: SimSun;">， </span><span style="font-size: 16px; font-family: Calibri;">TD</span><span style="font-size: 16px; font-family: SimSun;">）（</span><span style="font-size: 16px; font-family: Calibri;">TD</span><span style="font-size: 16px; font-family: SimSun;">也有时间差分的译法，但本意是暂时差别）的概念和方法，局势才开始改变，强化学习渐渐走上</span><span style="font-size: 16px; font-family: Calibri;">“</span><span style="font-size: 16px; font-family: SimSun;">先行后知</span><span style="font-size: 16px; font-family: Calibri;">”</span><span style="font-size: 16px; font-family: SimSun;">与</span><span style="font-size: 16px; font-family: Calibri;">“</span><span style="font-size: 16px; font-family: SimSun;">先知后行</span><span style="font-size: 16px; font-family: Calibri;">”</span><span style="font-size: 16px; font-family: SimSun;">为一体的</span><span style="font-size: 16px; font-family: Calibri;">“</span><span style="font-size: 16px; font-family: SimSun;">知行合一</span><span style="font-size: 16px; font-family: Calibri;">”</span><span style="font-size: 16px; font-family: SimSun;">之途。简言之，时序差分集现状、回顾、展望的不同需求和分析于一体，在试错和规划上充分考虑并利用不同时段的系统预估与环境反馈之间的差别，显著地提高了学习和决策的系统性和效率。由此，强化学习进入了时序差分学习阶段，理论研究和工程应用的水平都得到了很大的提升。</span></span></p><p style="margin: 12px 0;font-family: Calibri;white-space: normal;text-indent: 28px;text-align: justify"><span style="font-size: 16px; font-family: "times new roman";"> </span></p><p style="margin: 12px 0;font-family: Calibri;white-space: normal;text-indent: 28px;text-align: justify"><span style="font-size: 16px; font-family: "times new roman";"><span style="font-size: 16px; font-family: SimSun;">时序差分学习的理念源自动物学习心理学中与主要强化因子匹配的</span><span style="font-size: 16px; font-family: Calibri;">“</span><span style="font-size: 16px; font-family: SimSun;">次要强化因子（</span><span style="font-size: 16px; font-family: Calibri;">secondary reinforces</span><span style="font-size: 16px; font-family: SimSun;">）</span><span style="font-size: 16px; font-family: Calibri;">”</span><span style="font-size: 16px; font-family: SimSun;">概念。</span><span style="font-size: 16px; font-family: Calibri;">Minsky</span><span style="font-size: 16px; font-family: SimSun;">在人工智能之初就认定这一心理学方法对人工学习系统具有重要的意义，计算机游戏博弈技术的开创者</span><span style="font-size: 16px; font-family: Calibri;">Arthur Lee Samuel</span><span style="font-size: 16px; font-family: SimSun;">在其著名的跳棋程序中也采用了时序差分的理念，使</span><span style="font-size: 16px; font-family: Calibri;">“</span><span style="font-size: 16px; font-family: SimSun;">机器学习</span><span style="font-size: 16px; font-family: Calibri;">”</span><span style="font-size: 16px; font-family: SimSun;">一词成为广为人知的术语。</span><span style="font-size: 16px; font-family: Calibri;">20</span><span style="font-size: 16px; font-family: SimSun;">世纪</span><span style="font-size: 16px; font-family: Calibri;">70</span><span style="font-size: 16px; font-family: SimSun;">年代初，</span><span style="font-size: 16px; font-family: Calibri;">A.Harry Klopf</span><span style="font-size: 16px; font-family: SimSun;">认识到强化学习与监督学习的本质不同，强调强化学习内在的趋利（</span><span style="font-size: 16px; font-family: Calibri;">hedonistic</span><span style="font-size: 16px; font-family: SimSun;">）特性，试图将试错学习与时序差分学习结合，提出了</span><span style="font-size: 16px; font-family: Calibri;">“</span><span style="font-size: 16px; font-family: SimSun;">局部强化（</span><span style="font-size: 16px; font-family: Calibri;">local reinforcement</span><span style="font-size: 16px; font-family: SimSun;">）</span><span style="font-size: 16px; font-family: Calibri;">”</span><span style="font-size: 16px; font-family: SimSun;">和</span><span style="font-size: 16px; font-family: Calibri;">“</span><span style="font-size: 16px; font-family: SimSun;">广义强化（</span><span style="font-size: 16px; font-family: Calibri;">generalized reinforcement</span><span style="font-size: 16px; font-family: SimSun;">）</span><span style="font-size: 16px; font-family: Calibri;">”</span><span style="font-size: 16px; font-family: SimSun;">等概念，但与现代的时序差分并不完全相同；加上</span><span style="font-size: 16px; font-family: Calibri;">A.Harry Klopf</span><span style="font-size: 16px; font-family: SimSun;">英年早逝，其工作不算十分成功。新西兰学者</span><span style="font-size: 16px; font-family: Calibri;">Ian H.Witten</span><span style="font-size: 16px; font-family: SimSun;">在其 </span><span style="font-size: 16px; font-family: Calibri;">1976</span><span style="font-size: 16px; font-family: SimSun;">年的博士论文中第一次明确指出了时序差分学习规则。</span><span style="font-size: 16px; font-family: Calibri;">A.Harry Klopf</span><span style="font-size: 16px; font-family: SimSun;">的工作对</span><span style="font-size: 16px; font-family: Calibri;">Andrew G.Barto</span><span style="font-size: 16px; font-family: SimSun;">、</span><span style="font-size: 16px; font-family: Calibri;">Richard S.Sutton</span><span style="font-size: 16px; font-family: SimSun;">和</span><span style="font-size: 16px; font-family: Calibri;">Charles W.Anderson</span><span style="font-size: 16px; font-family: SimSun;">的启发很大，促使他们在</span><span style="font-size: 16px; font-family: Calibri;">20</span><span style="font-size: 16px; font-family: SimSun;">世纪</span><span style="font-size: 16px; font-family: Calibri;">80</span><span style="font-size: 16px; font-family: SimSun;">年代初将时序差分学习与试错学习结合，提出著名的</span><span style="font-size: 16px; font-family: Calibri;">“</span><span style="font-size: 16px; font-family: SimSun;">行动者</span><span style="font-size: 16px; font-family: Calibri;">-</span><span style="font-size: 16px; font-family: SimSun;">评论者框架（</span><span style="font-size: 16px; font-family: Calibri;">actor-critic architecture</span><span style="font-size: 16px; font-family: SimSun;">）</span><span style="font-size: 16px; font-family: Calibri;">”</span><span style="font-size: 16px; font-family: SimSun;">，时序差分的强化学习由此正式登场。然而，将时序差分与动态规划和试错方法全部整合在一起是在</span><span style="font-size: 16px; font-family: Calibri;">20</span><span style="font-size: 16px; font-family: SimSun;">世纪</span><span style="font-size: 16px; font-family: Calibri;">80</span><span style="font-size: 16px; font-family: SimSun;">年代末，这归功于英国学者</span><span style="font-size: 16px; font-family: Calibri;">Chris J.Watkins </span><span style="font-size: 16px; font-family: SimSun;">在其</span><span style="font-size: 16px; font-family: Calibri;">1989</span><span style="font-size: 16px; font-family: SimSun;">年的博士论文中提出的</span><span style="font-size: 16px; font-family: Calibri;">Q</span><span style="font-size: 16px; font-family: SimSun;">学习（</span><span style="font-size: 16px; font-family: Calibri;">Q-learning</span><span style="font-size: 16px; font-family: SimSun;">）算法。</span><span style="font-size: 16px; font-family: Calibri;">1992</span><span style="font-size: 16px; font-family: SimSun;">年，</span><span style="font-size: 16px; font-family: Calibri;">IBM</span><span style="font-size: 16px; font-family: SimSun;">公司的</span><span style="font-size: 16px; font-family: Calibri;">Gerald Tesauro</span><span style="font-size: 16px; font-family: SimSun;">利用时序差分构造了多层神经网络 </span><span style="font-size: 16px; font-family: Calibri;">TD-Gammon</span><span style="font-size: 16px; font-family: SimSun;">，并在古老的西洋双陆棋中战胜了人类世界冠军，引起广泛关注，这也使时序差分的强化学习方法广为人知。同年，</span><span style="font-size: 16px; font-family: Calibri;">Chris J.Watkins</span><span style="font-size: 16px; font-family: SimSun;">和</span><span style="font-size: 16px; font-family: Calibri;">Peter Dayan</span><span style="font-size: 16px; font-family: SimSun;">给出</span><span style="font-size: 16px; font-family: Calibri;">Q</span><span style="font-size: 16px; font-family: SimSun;">学习算法收敛性的第一个严格证明，更加加深了人们对</span><span style="font-size: 16px; font-family: Calibri;">Q</span><span style="font-size: 16px; font-family: SimSun;">学习和强化学习的兴趣。当前，时序差分己从专注预测的 </span><span style="font-size: 16px; font-family: Calibri;">TD</span><span style="font-size: 16px; font-family: SimSun;">（</span><span style="font-size: 16px; font-family: Calibri;">lambda</span><span style="font-size: 16px; font-family: SimSun;">）发展到预估决策控制一体的</span><span style="font-size: 16px; font-family: Calibri;">SARSA</span><span style="font-size: 16px; font-family: SimSun;">（</span><span style="font-size: 16px; font-family: Calibri;">lambda</span><span style="font-size: 16px; font-family: SimSun;">），</span><span style="font-size: 16px; font-family: Calibri;">Barto</span><span style="font-size: 16px; font-family: SimSun;">和</span><span style="font-size: 16px; font-family: Calibri;">Sutton</span><span style="font-size: 16px; font-family: SimSun;">合著的《强化学习导论（</span><span style="font-size: 16px; font-family: Calibri;">reinforcement learning:an introduction</span><span style="font-size: 16px; font-family: SimSun;">）》已成为机器学习领域的经典之作。</span></span></p><p style="margin: 12px 0;font-family: Calibri;white-space: normal;text-indent: 28px;text-align: justify"><span style="font-size: 16px; font-family: "times new roman";"> </span></p><p style="margin: 12px 0;font-family: Calibri;white-space: normal;text-indent: 32px;text-align: justify"><span style="font-size: 16px; font-family: "times new roman";"><strong><span style="font-size: 16px; font-family: SimSun;">平行强化的体系</span></strong></span></p><p style="margin: 12px 0;font-family: Calibri;white-space: normal;text-indent: 32px;text-align: justify"><span style="font-size: 16px; font-family: "times new roman";"><strong><span style="font-size: 16px; font-family: SimSun;"><br></span></strong></span></p><p style="margin: 12px 0;font-family: Calibri;white-space: normal;text-indent: 28px;text-align: justify"><span style="font-size: 16px; font-family: "times new roman";"><span style="font-size: 16px; font-family: SimSun;">基于大规模多层人工神经元网络的深度学习的成功，特别是</span>AlphaGo<span style="font-size: 16px; font-family: SimSun;">和</span><span style="font-size: 16px; font-family: Calibri;">Pluribus</span><span style="font-size: 16px; font-family: SimSun;">的巨大影响，使强化学习方法登上了一个更新、更高的层次。然而，随着深度强化学习（</span><span style="font-size: 16px; font-family: Calibri;">deep reinforcement learning</span><span style="font-size: 16px; font-family: SimSun;">，</span><span style="font-size: 16px; font-family: Calibri;">DRL</span><span style="font-size: 16px; font-family: SimSun;">）和深度</span><span style="font-size: 16px; font-family: Calibri;">Q</span><span style="font-size: 16px; font-family: SimSun;">学习（</span><span style="font-size: 16px; font-family: Calibri;">deep Q learning</span><span style="font-size: 16px; font-family: SimSun;">， </span><span style="font-size: 16px; font-family: Calibri;">DQL</span><span style="font-size: 16px; font-family: SimSun;">）等的不断涌现和广泛应用，数据再次成为重大问题，而且图</span><span style="font-size: 16px; font-family: Calibri;">1 </span><span style="font-size: 16px; font-family: SimSun;">右边所示的强化学习大脑神经科学的部分内容，特别是 </span><span style="font-size: 16px; font-family: Calibri;">Hebb </span><span style="font-size: 16px; font-family: SimSun;">学习规则的重新评估和计算复杂化与有效性问题，也更加引人注意。</span></span></p><p style="margin: 12px 0;font-family: Calibri;white-space: normal;text-indent: 28px;text-align: justify"><span style="font-size: 16px; font-family: "times new roman";"> </span></p><p style="margin: 12px 0;font-family: Calibri;white-space: normal;text-indent: 28px;text-align: justify"><span style="font-size: 16px; font-family: "times new roman";"><span style="font-size: 16px; font-family: SimSun;">在以试错法为主的先行后知强化学习中，因实验周期长、成本高，数据来源受到</span><span style="font-size: 16px; font-family: Calibri;">“</span><span style="font-size: 16px; font-family: SimSun;">经济诅咒</span><span style="font-size: 16px; font-family: Calibri;">”</span><span style="font-size: 16px; font-family: SimSun;">的制约；而在动态规划类的先知后行强化方法中，算法实施又遇</span><span style="font-size: 16px; font-family: Calibri;">“</span><span style="font-size: 16px; font-family: SimSun;">维数灾难</span><span style="font-size: 16px; font-family: Calibri;">”</span><span style="font-size: 16px; font-family: SimSun;">，导致其无效、不可行， </span><span style="font-size: 16px; font-family: Calibri;">TD</span><span style="font-size: 16px; font-family: SimSun;">强化学习，特别是</span><span style="font-size: 16px; font-family: Calibri;">TD-Gammon</span><span style="font-size: 16px; font-family: SimSun;">借助</span><span style="font-size: 16px; font-family: Calibri;">Self Play</span><span style="font-size: 16px; font-family: SimSun;">在一定程度上为解决数据生成和算法效率指明了一条道路，而</span><span style="font-size: 16px; font-family: Calibri;">AlphaGo</span><span style="font-size: 16px; font-family: SimSun;">和</span><span style="font-size: 16px; font-family: Calibri;">Pluribus</span><span style="font-size: 16px; font-family: SimSun;">进一步强化了这条道路的有效性。实际上，这是一条通过虚实平行运作，由</span><span style="font-size: 16px; font-family: Calibri;">“</span><span style="font-size: 16px; font-family: SimSun;">小数据</span><span style="font-size: 16px; font-family: Calibri;">”</span><span style="font-size: 16px; font-family: SimSun;">生成大数据，再与蒙特卡洛法或各类决策树等有效搜索技术结合，从大数据中锤炼出针对具体问题的</span><span style="font-size: 16px; font-family: Calibri;">“</span><span style="font-size: 16px; font-family: SimSun;">小智能</span><span style="font-size: 16px; font-family: Calibri;">”</span><span style="font-size: 16px; font-family: SimSun;">般的精确知识之道。人们应当通过知识图谱和知识范畴（</span><span style="font-size: 16px; font-family: Calibri;">knowledge categories</span><span style="font-size: 16px; font-family: SimSun;">）等工具，将这一数据生成和知识制造的过程形式化，并加以软件定义，为强化学习系统组态的生成和实际应用的自动化创造基础。</span></span></p><p style="margin: 12px 0;font-family: Calibri;white-space: normal;text-indent: 28px;text-align: justify"><span style="font-size: 16px; font-family: "times new roman";"> </span></p><p style="margin: 12px 0;font-family: Calibri;white-space: normal;text-indent: 28px;text-align: justify"><span style="font-size: 16px; font-family: "times new roman";"><span style="font-size: 16px; font-family: SimSun;">此外，抽象数字化的强化学习还必须与大脑生物化的功能强化实现平行互联。除了快慢过程的微分</span> Hebb <span style="font-size: 16px; font-family: SimSun;">学习规则，人们更应关注强化学习与动物的无条件</span><span style="font-size: 16px; font-family: Calibri;">/</span><span style="font-size: 16px; font-family: SimSun;">工具性反射、典型惯性和目标导向行为以及认知图（</span><span style="font-size: 16px; font-family: Calibri;">cognitive maps</span><span style="font-size: 16px; font-family: SimSun;">）生成构造等问题的内在关联，并将其应用于针对不同病状的各种机器人辅助和智能康复系统的设计、操作、监控和运维以及与脑和神经相关的疾病的智能诊疗系统中。同时，强化学习机制应成为虚实互动的平行学习和平行大脑的核心基础，扩展突触可塑、</span><span style="font-size: 16px; font-family: Calibri;">Hedonist</span><span style="font-size: 16px; font-family: SimSun;">神经元、多巴胺神经元与响应、奖励预估误差机制、神经行动者评估者结构等大脑神经基础构成问题的计算和智能研究手段，使人类生物智能与人工智能的研发更加密切地结合到一起。图</span><span style="font-size: 16px; font-family: Calibri;">2</span><span style="font-size: 16px; font-family: SimSun;">给出了虚实互动、实践与理论融合的平行强化学习体系的基本框架，目前流行的数字双胞胎（又称数字孪生）是其中的一个重要组成部分。平行强化学习的目的是通过交换世界实现</span><span style="font-size: 16px; font-family: Calibri;">“</span><span style="font-size: 16px; font-family: SimSun;">吃一堑、长一智</span><span style="font-size: 16px; font-family: Calibri;">”</span><span style="font-size: 16px; font-family: SimSun;">：在虚拟的人工世界吃一堑、吃多堑，在现实的自然世界长一智、长多智，以此降低成本，提升效益，克服</span><span style="font-size: 16px; font-family: Calibri;">“</span><span style="font-size: 16px; font-family: SimSun;">经济诅咒</span><span style="font-size: 16px; font-family: Calibri;">”</span><span style="font-size: 16px; font-family: SimSun;">和</span><span style="font-size: 16px; font-family: Calibri;">“</span><span style="font-size: 16px; font-family: SimSun;">维数灾难</span><span style="font-size: 16px; font-family: Calibri;">”</span><span style="font-size: 16px; font-family: SimSun;">，走向智能知行合一的机器强化学习。</span></span></p><p style="margin: 12px 0;font-family: Calibri;white-space: normal;text-indent: 28px;text-align: justify"><span style="font-size: 16px; font-family: "times new roman";"> </span></p><p style="margin: 12px 0px; font-family: Calibri; white-space: normal; text-align: center; text-indent: 0em;"><span style="font-size: 16px; font-family: "times new roman";"><img src="https://cors.zfour.workers.dev/?http://bbs.sciencenet.cn/home.php?mod=attachment&filename=image.png&id=949123" title alt="image.png" width="460" height="284" style="width: 460px; height: 284px;" referrerpolicy="no-referrer"> </span></p><p style="margin: 12px 0;font-family: Calibri;white-space: normal;text-indent: 28px;text-align: center"><span style="font-size: 16px; font-family: "times new roman";"><span style="font-size: 16px; font-family: SimSun;">图</span>2   <span style="font-size: 16px; font-family: SimSun;">平行强化学习体系的基本框架</span></span></p><p style="margin: 12px 0;font-family: Calibri;white-space: normal;text-indent: 28px;text-align: justify"><span style="font-size: 16px; font-family: "times new roman";"> </span></p><p style="margin: 12px 0;font-family: Calibri;white-space: normal;text-indent: 32px;text-align: justify"><span style="font-size: 16px; font-family: "times new roman";"><strong><span style="font-size: 16px; font-family: SimSun;">知行合一的智能</span></strong></span></p><p style="margin: 12px 0;font-family: Calibri;white-space: normal;text-indent: 28px;text-align: justify"><span style="font-size: 16px; font-family: "times new roman";"> </span></p><p style="margin: 12px 0;font-family: Calibri;white-space: normal;text-indent: 28px;text-align: justify"><span style="font-size: 16px; font-family: "times new roman";"><span style="font-size: 16px; font-family: SimSun;">学习是人类获取知识的通用且可靠的途径，这是人类文明有史以来的共识与实践。强化学习是机器具有机器智能的基础和关键手段，这是人工智能研究开创以来的认识与方向，在很大程度上，也是目前从事智能科学与技术研发工作者的共识。然而，要使强化学习真正成为机器学习的核心与智能机制和智能算法的基础与关键技术，从</span><span style="font-size: 16px; font-family: Calibri;">“</span><span style="font-size: 16px; font-family: SimSun;">不知对错、无论好坏</span><span style="font-size: 16px; font-family: Calibri;">”</span><span style="font-size: 16px; font-family: SimSun;">的先行后知与先知后行，到知行合一、虚实互动的混合平行智能，仍有许多理论和实践的任务必须完成。</span></span></p><p style="margin: 12px 0;font-family: Calibri;white-space: normal;text-indent: 28px;text-align: justify"><span style="font-size: 16px; font-family: "times new roman";"><br></span></p><p style="margin: 12px 0;font-family: Calibri;white-space: normal;text-indent: 28px;text-align: justify"><span style="font-size: 16px; font-family: "times new roman";"><span style="font-size: 16px; font-family: SimSun;">首先，强化学习面临的许多经典问题依然存在，并没有被彻底有效地解决，如</span><span style="font-size: 16px; font-family: Calibri;">“</span><span style="font-size: 16px; font-family: SimSun;">维数灾难</span><span style="font-size: 16px; font-family: Calibri;">”</span><span style="font-size: 16px; font-family: SimSun;">、信用分配、信息不完备、非稳环境、状态行动</span><span style="font-size: 16px; font-family: Calibri;">Space Tiling</span><span style="font-size: 16px; font-family: SimSun;">、探索与利用的矛盾等，需要更加深入和系统地研究。其次，对于目前被广泛应用的各类深度强化学习方法来说，其引人注目的</span><span style="font-size: 16px; font-family: Calibri;">“</span><span style="font-size: 16px; font-family: SimSun;">超人</span><span style="font-size: 16px; font-family: Calibri;">”</span><span style="font-size: 16px; font-family: SimSun;">表现源自其解决特定问题的特点，但这也正是其难以被推广和普及的问题所在。必须考虑这些深度强化学习方法构建与应用过程中的形式化和软件定义问题，从而使过程的迁移及其自动化成为可能，完成从特别应用到相对通用的转化。最后，必须引入针对强化学习的软硬件平台，边缘与云端的支撑环境和相应的开源基础设施使强化学习真正应用到生产、商业、交通、健康、服务等领域，使强化学习、强化控制、强化管理、强化医疗、强化经济、强化法律、强化安保等成为一个有效且普适的智能工程项目。</span></span></p><p style="margin: 12px 0;font-family: Calibri;white-space: normal;text-indent: 28px;text-align: justify"><span style="font-size: 16px; font-family: "times new roman";"><br></span></p><p style="margin: 12px 0;font-family: Calibri;white-space: normal;text-indent: 28px;text-align: justify"><span style="font-size: 16px; font-family: "times new roman";"><span style="font-size: 16px; font-family: SimSun;">为此，研究者需要从更高更广的角度重新审视强化学习的方法和技术，使其真正成为</span><span style="font-size: 16px; font-family: Calibri;">“</span><span style="font-size: 16px; font-family: SimSun;">人机结合、知行合一、虚实一体</span><span style="font-size: 16px; font-family: Calibri;">”</span><span style="font-size: 16px; font-family: SimSun;">的</span><span style="font-size: 16px; font-family: Calibri;">“</span><span style="font-size: 16px; font-family: SimSun;">合一体</span><span style="font-size: 16px; font-family: Calibri;">”</span><span style="font-size: 16px; font-family: SimSun;">的核心与关键，化智能代理（</span><span style="font-size: 16px; font-family: Calibri;">agents</span><span style="font-size: 16px; font-family: SimSun;">）为知识机器人，深入推动和完善智能社会的知识自动化进程。</span></span></p><p style="margin: 12px 0;font-family: Calibri;white-space: normal;text-indent: 28px;text-align: justify"><span style="font-size: 16px; font-family: "times new roman";"><br></span></p><p style="margin: 12px 0;font-family: Calibri;white-space: normal;text-indent: 28px;text-align: justify"><span style="font-size: 16px; font-family: "times new roman";">《强化学习》一书就是为此目的而撰写的。本书主要讲述了强化学习的基本原理和基本方法，基于强化学习的控制、决策和优化方法设计与理论分析，深度强化学习原理以及平行强化学习等未来强化学习的发展新方向，展示从先行后知，先知后行，再到知行合一的混合平行智能思路。本书可作为高等学校人工智能、机器学习、智能控制、智能决策、智慧管理、系统工程以及应用数学等专业的本科生或研究生教材，亦可供相关专业的科研人员和工程技术人员参考。</span></p><p style="margin: 12px 0;font-family: Calibri;white-space: normal;text-indent: 28px;text-align: justify"><span style="font-size: 16px; font-family: "times new roman";"><br></span></p><p style="margin: 12px 0;font-family: Calibri;white-space: normal;text-indent: 28px;text-align: justify"><span style="font-size: 16px; font-family: "times new roman";"><span style="font-size: 16px; font-family: SimSun;">本书的写作计划自</span>2015<span style="font-size: 16px; font-family: SimSun;">年开始，最初作为复杂系统管理与控制国家重点实验室相关团队和中国科学院大学计算机与控制学院的教材，后纳入</span><span style="font-size: 16px; font-family: Calibri;">“</span><span style="font-size: 16px; font-family: SimSun;">智能科学与系统</span><span style="font-size: 16px; font-family: Calibri;">”</span><span style="font-size: 16px; font-family: SimSun;">博士学位培养课程的选用教科书系列。当时，相关中英文的著作很少，但经过</span><span style="font-size: 16px; font-family: Calibri;">5</span><span style="font-size: 16px; font-family: SimSun;">年多的发展，强化学习的研究和教材状况已发生了天翻地覆的变化，为写作增加了许多变数。尽管作者与团队付出了相当多的心血和努力，但限于水平，仍有许多地方需要改进完善。</span></span></p><p style="margin: 12px 0;font-family: Calibri;white-space: normal;text-indent: 28px;text-align: justify"><span style="font-size: 16px; font-family: "times new roman";"><br></span></p><p style="margin: 12px 0;font-family: Calibri;white-space: normal;text-indent: 28px;text-align: justify"><span style="font-size: 16px; font-family: "times new roman";"><span style="font-size: 16px; font-family: SimSun;">本书的出版得到了国家自然科学基金</span>(61722312<span style="font-size: 16px; font-family: SimSun;">，</span><span style="font-size: 16px; font-family: Calibri;">61533019)</span><span style="font-size: 16px; font-family: SimSun;">资助，在此表示感谢。</span></span></p><p style="margin: 12px 0;font-family: Calibri;white-space: normal;text-indent: 28px;text-align: justify"><span style="font-size: 16px; font-family: "times new roman";"><br></span></p><p style="margin: 12px 0;font-family: Calibri;white-space: normal;text-indent: 28px;text-align: justify"><span style="font-size: 16px; font-family: "times new roman";">本书在撰写过程中得到了北京科技大学宋睿卓教授、中南大学罗彪教授和广东工业大学刘德荣教授的大力支持，在此，对他们的指导深表感谢！本书的完成参阅了大量国内外学者的相关论著，均在参考文献中列出，在此，对这些论著的作者深表感谢！本书的写作得到了中国科学院自动化研究所复杂系统管理与控制国家重点实验室的许多同事支持，特别是助理工程师朱辽和杨湛宇，研究生谢玉龙、李俊松、李洪阳、李涛、王凌霄、王鑫、卢经纬、夏丽娜、杜康豪、王子洋、阎钰天、韩立元等。最后，感谢清华大学出版社贾斌先生在本书的编辑和出版过程中所给予的热心帮助。</span></p><p style="margin: 12px 0;font-family: Calibri;white-space: normal;text-indent: 28px;text-align: justify"><span style="font-size: 16px; font-family: "times new roman";"><br></span></p><p style="margin: 12px 0;font-family: Calibri;white-space: normal;text-indent: 28px;text-align: justify"><span style="font-size: 16px; font-family: "times new roman";">对于书中出现的不妥之处，殷切希望广大读者批评指正。</span></p><p style="margin: 12px 0;font-family: Calibri;white-space: normal;text-indent: 28px;text-align: justify"><span style="font-size: 16px; font-family: "times new roman";"> </span></p><p style="margin: 12px 0;font-family: Calibri;white-space: normal;text-indent: 28px;text-align: right"><span style="font-size: 16px; font-family: "times new roman";">魏庆来、王飞跃</span></p><p style="margin: 12px 0;font-family: Calibri;white-space: normal;text-indent: 28px;text-align: right"><span style="font-size: 16px; font-family: "times new roman";">中国科学院自动化研究所复杂系统管理与控制国家重点实验室</span></p><p style="margin: 12px 0;font-family: Calibri;white-space: normal;text-indent: 28px;text-align: right"><span style="font-size: 16px; font-family: "times new roman";">北京怀德海智能学院</span></p><p style="margin: 12px 0;font-family: Calibri;white-space: normal;text-indent: 28px;text-align: right"><span style="font-size: 16px; font-family: "times new roman";">2022<span style="font-size: 16px; font-family: SimSun;">年</span><span style="font-size: 16px; font-family: Calibri;">5</span><span style="font-size: 16px; font-family: SimSun;">月</span></span></p><p><br></p>                    <br><br>
                                        <label style="font-size:13px; color:#850f0f">转载本文请联系原作者获取授权，同时请注明本文来自王飞跃科学网博客。<br>链接地址：</label><a href="https://blog.sciencenet.cn/blog-2374-1351757.html" target="_blank" style="font-size:13px; color:#850f0f">https://blog.sciencenet.cn/blog-2374-1351757.html </a>
  <br><br>上一篇：<a href="http://blog.sciencenet.cn/blog-2374-1351469.html" target="_black">平行智能与元宇宙</a><br>                    <!--大赛结束-->
                                        
  
</div>
            