
---
title: '相比于机器，我们对人类的错误更加宽容吗？'
categories: 
 - 新媒体
 - 少数派 sspai
 - Matrix
headimg: 'https://cdn.sspai.com/2021/11/06/article/7c68c55aa274fd9e35a7f89b2db66a91?imageView2/2/w/1120/q/40/interlace/1/ignore-error/1'
author: 少数派 sspai
comments: false
date: Sat, 06 Nov 2021 14:33:59 GMT
thumbnail: 'https://cdn.sspai.com/2021/11/06/article/7c68c55aa274fd9e35a7f89b2db66a91?imageView2/2/w/1120/q/40/interlace/1/ignore-error/1'
---

<div>   
<div class="articleWidth-content" data-v-6f469055><div class="content wangEditor-txt minHeight" data-v-6f469055><blockquote><p style="margin-left:0;">Assessing outcomes vs intention.</p><p style="margin-left:0;">评估结果与目的</p></blockquote><figure class="image ss-img-wrapper"><img src="https://cdn.sspai.com/2021/11/06/article/7c68c55aa274fd9e35a7f89b2db66a91?imageView2/2/w/1120/q/40/interlace/1/ignore-error/1" alt="image.png" data-original="https://cdn.sspai.com/2021/11/06/article/7c68c55aa274fd9e35a7f89b2db66a91" referrerpolicy="no-referrer"></figure><h2 style="margin-left:0;">你对频繁死机的电脑感到崩溃吗？</h2><p style="margin-left:0;">我们可能认为：相比于笨拙的人类，我们对功能失常的应用程序或设计糟糕的算法会更为苛刻。然而这只是一种模糊的猜测。</p><p style="margin-left:0;">事实上，用户对机器反而可能更具有同情心，他们似乎并不会对机器的错误感到生气或者苛责。根据César A. Hidalgo①（经济学家，前MIT研究员）的说法：这主要是因为用户审判机器是根据它们的行为结果，而不是行为目的。</p><p style="margin-left:0;">无论是面对有偏见的结果还是不公平的决定，用户似乎从来没有真正的在机器身上寻找道德原因，而只是把它们当成功能的载体（只关乎是否完成其工作）。</p><p style="margin-left:0;">这产生了四个新视角诠释人机交互的有趣行为科学结论。</p><h2 style="margin-left:0;">AI决策和人类判断</h2><figure class="image ss-img-wrapper"><img src="https://cdn.sspai.com/2021/11/06/article/ac75e5d5db91a41d38db5f413e119800?imageView2/2/w/1120/q/40/interlace/1/ignore-error/1" data-original="https://cdn.sspai.com/2021/11/06/article/ac75e5d5db91a41d38db5f413e119800" referrerpolicy="no-referrer"><figcaption>一幅由机器学习还原的奥巴马打码照片</figcaption></figure><p style="margin-left:0;">无论是辅助人类做决策，又或是雇佣新员工，识别人脸，我们都听说过算法偏见是如何影响人工智能的。算法收集的数据保留了系统性的判断偏见（judgmental biases），这有时候会反映在对某些个体的歧视性决策中。</p><p style="margin-left:0;">但是人类对机器制造不公平行为有什么感觉呢？相比于人类的歧视，他们会更反感机器吗？</p><p style="margin-left:0;">César Hidalgo试图通过社会实验来回答这些问题。他向参与者展示了几种歧视情况，并要求他们判断决策在人类或者机器负责下的公平性。这些场景里有： HR在候选人相同资质相同的情况下从不选择某种特定出身的候选人，和警察总是关押一些相同族群的无辜群众。</p><p style="margin-left:0;">在这些情况下，参与者都认为人类的行为更具有目的性，因此更需要责任感。因为参与者认为人类的行为带有自己的主观意愿，所以他们的行为比机器的错误判断更受到指责。然而，当问起他们应该由谁来取代这些歧视者，他们的答案更倾向于选择更正直的人。</p><h2 style="margin-left:0;">人类对自动化工作的模糊认知</h2><p style="margin-left:0;">当面对自动化工作（work automation）或者工作替代（job replacement）的时候，这种模糊性再次出现。Hidalgo通过类似的实验研究了人们对这种情况的反应。<br>他向参与者讲述了一些故事：比如公司员工要么被AI机器人取代，要么被效率更高，更有精力的外国年轻员工取代。Hidalgo询问参与者在不同情景和不同行业内对此会有什么感受。</p><p style="margin-left:0;">令人惊讶的是，参与者更情愿取代他们工作的是机器人，而不是外国人。虽然他们的偏好因情况而异：更接受司机被自动驾驶卡车取代，而不是老师被教学机器人取代，但他们通常同意他们更喜欢自动化而不是另一位工人。</p><p style="margin-left:0;">对这类情况的解释有几种。参与者可能觉得技术的发展是不可避免的，而被外国工人取代会激发他们的归属感。他们可能能更加切身的体会到后者（外国工人）的威胁性，因为这发生的更为频繁。人力替代感觉上也更不公平，因为同等资质的外国工人凭什么更有权利获得工作？</p><p style="margin-left:0;">这也解释了为什么和90-00年间欧美企业外迁劳动力（到发展中国家）相比，自动化引发的情绪抵抗似乎没有那么的极端。</p><h2 style="margin-left:0;">对机器事故的毫不宽容</h2><figure class="image ss-img-wrapper"><img src="https://cdn.sspai.com/2021/11/06/article/e241339c536eb0d23dae4b651867d866?imageView2/2/w/1120/q/40/interlace/1/ignore-error/1" data-original="https://cdn.sspai.com/2021/11/06/article/e241339c536eb0d23dae4b651867d866" referrerpolicy="no-referrer"><figcaption>自动驾驶汽车比人类驾驶员更负责任吗？</figcaption></figure><p style="margin-left:0;">随着自动驾驶离现实越来越近，我们可能好奇：相比于人类驾驶员，用户如何看待自动驾驶汽车需要承担的责任？</p><p style="margin-left:0;">为了回答这个问题，Hidalgo和他的团队把参与者置于各种各样的道路事故（包含了人类司机或自动驾驶汽车）之中。这些事故的严重程度可能取决于内在或外在因素，也包含了是伤害司机还是路人的抉择。</p><p style="margin-left:0;">这项实验首次提出了自动驾驶汽车担负了巨大的责任。参与者对涉及自动驾驶汽车的事故判断更为消极，认为它们会造成更严重的伤害。其中一个原因是因为他们更容易把自己带入人类驾驶员的视角（而不是机器的视角），他们更能同理人类司机做出的反应（毕竟在自己身上可能也会发生这样的情况），尤其这是由外部因素导致的事故（比如一棵树倒在了路上）。</p><p style="margin-left:0;">因此，参与者对造成事故的机器毫不宽容，希望它们更可靠安全。</p><h2 style="margin-left:0;">机器的好坏完全取决于结果</h2><p style="margin-left:0;">我们可以从这些研究中得出什么结论呢？</p><figure class="image ss-img-wrapper"><img src="https://cdn.sspai.com/2021/11/06/970a0f273f2175304c7e4387372367d4.png?imageView2/2/w/1120/q/40/interlace/1/ignore-error/1" data-original="https://cdn.sspai.com/2021/11/06/970a0f273f2175304c7e4387372367d4.png" referrerpolicy="no-referrer"><figcaption>César Hidalgo | How Humans Judge Machines | Talks at Google（前往youtube观看视频②）</figcaption></figure><p style="margin-left:0;">首先，当整体看待受伤害感和行为目的的关系时，人们认为人类行为比机器行为更带有目的。然而矛盾的是，参与者仍然会更容易原谅人类的行为，因为他们更容易将人为错误视为坏运气的结果，而机器错误则是需要纠正的错误。</p><p style="margin-left:0;">当我们在研究行为目的和不公正之间的关系时，我们发现了一个更符合人类判断的事实：对于那些带有当事人强烈主观意图的场景（比如侮辱和歧视），人们显然会对人类行为做出更为负面的评价。人们认为人类需要为自己的邪恶意图负责任，而机器则被默认为没有自己的意图和目的。另一方面，一些本身不带有目的的情况（比如交通事故），机器则承担的责备会更多。因为我们已经假定了机器的程序可以避免任何错误。</p><p style="margin-left:0;">最后通过评估不公正的感知和伤害范围之间的关系，我们发现：对涉及人造成的伤害越小，机器就越被视为罪魁祸首；相反，当伤害越大，人类当事人承担的评价就越为负面。</p><p style="margin-left:0;"><strong>总之，我们看到了两种截然不同的判断模式。</strong></p><p style="margin-left:0;">当涉及人类当事人时，观众就会通过当事人的意图来评判他们的行为：他们可以犯错，但如果居心不良，他们就需要对自己的行为负责。</p><p style="margin-left:0;">另一方面，机器的评估标准是行为的结果：如果它们无法避免破坏性的错误，无论发生的是什么，它们都会被批评。好的一面是，对于一些通常被定性为非常严重的情况（歧视或恶意羞辱），因为机器行为不带有目的，所以常常不被苛责。</p><p style="margin-left:0;">但这也意味着，作为设计师的我们，需要尽量减少数字服务和智能应用程序可能产生的间接伤害和歧视。因为可没有人会同情设计糟糕的算法③。</p><blockquote><p style="margin-left:0;">译者注③：</p><p style="margin-left:0;">设计师和算法的关系</p><p style="margin-left:0;">由于译者正好从事自动驾驶行业，所以就简单聊聊这个话题。可能有人会认为糟糕算法能是程序员的问题，这和设计师有什么关系？</p><p style="margin-left:0;">但事实上在以正向研发为主要流程的企业内，一个需求落地至少要经过产品经理，设计师，研发和测试四个阶段。产品和设计师需要根据客户/用户的诉求去定义和产品的功能和使用方式，然后研发才会在设计的框架内去实现这些功能。自然当设计师遗漏或者忽视一些安全问题时，研发是无法发现的，因为在他们的角度，功能的上下文是不明确的，他们只专注于功能本身的研发。</p><p style="margin-left:0;">举个例子，假设我们设计一个工业园区内使用的远程自动驾驶汽车遥控器，它的功能是给车辆派发运货订单，并且可以远程启动车辆。那么如果设计师对于使用场景足够了解，他会发现如果远程启动车辆，突然的启动可能会对周围在装卸货的工人带来安全隐患。所以需要在这个阶段提供预警或者二次鉴权的操作。而这可能是客户的原始需求中没有提及或者客户觉得麻烦的功能（客户只想点一次！），如果产品和设计没有深入研究，自然研发无法发现这样的问题。这就是我理解的为什么设计师需要对算法负责。</p><p style="margin-left:0;">关于设计师的责任这一块，有兴趣的朋友可以看一看前文《负责任的设计：设计师应该承担多少产品责任？》。我也和一些人交流过这个问题，刚毕业的设计师觉得，我们需要为用户负责！我们要站出来说话！在社会中摸爬滚打了多年的设计师多会认为这非常的理想。确实，从企业角度来讲，道德如果无关乎企业生存，是会被自动忽视的（毕竟罗翔老师也说过，法律只是规定了道德的底线），或者当成一种营销手段。只有一些和道德高度绑定的行业，比如说自动驾驶行业，设计师才会有必要，有能力去考虑一些道德问题。当然，我也不是说设计师不需要也没有能力去承担责任，而是说希望大家能够多去思考自己的正在做的产品，而不是说盲目的为了kpi或者okr去完成设计任务。可能你的一次轻微的尝试就会给产品带来一些正面的影响。</p></blockquote><h2 style="margin-left:0;">相关阅读</h2><p style="margin-left:0;">①《人类如何评判机器》How Humans Judge Machines</p><p style="margin-left:0;"><a href="https://book.douban.com/subject/35557225/" target="_blank">https://book.douban.com/subject/35557225/</a></p><p style="margin-left:0;">②How Humans Judge Machines | Talks at Google</p><p style="margin-left:0;"><a href="https://www.youtube.com/watch?v=H10-t5hnnw0" target="_blank">https://www.youtube.com/watch?v=H10-t5hnnw0</a></p><p style="margin-left:0;"> </p><p style="margin-left:0;">原文名称：Are users more forgiving of human mistakes than machine errors?</p><p style="margin-left:0;">原文作者：Jean-marc Buchert</p><p style="margin-left:0;">原文链接： <a href="https://uxdesign.cc/are-users-more-forgiving-of-human-mistakes-than-machine-errors-5ed2a87f5368" target="_blank">https://uxdesign.cc/are-users-more-forgiving-of-human-mistakes-than-machine-errors-5ed2a87f5368</a></p><p style="margin-left:0;">*已获得作者授权，若有翻译问题，欢迎一起交流~</p><p style="margin-left:0;">*欢迎关注公众号：<a href="https://mp.weixin.qq.com/s?__biz=MzkzODI5OTQ4NA==&mid=2247483704&idx=1&sn=fa5cb75fa67405caa802eaf107260c29&chksm=c283180bf5f4911dde9fcbf19e9d64114663427e683c1f22cb9d0173bd4cb6f4cc61f7f62937&token=912649156&lang=zh_CN#rd" target="_blank">第三设计研究所</a></p></div><!----></div><div style="border:1px solid transparent;" data-v-6f469055></div><div class="article-side sideTop" style="display:none;left:0;" data-v-7be936cf data-v-6f469055><div class="download-guide-container" data-v-14f9065e data-v-7be936cf><div class="btn-wrapper" data-v-14f9065e><!----><button class="btn btn-view" data-v-14f9065e><i class="iconfont iconfont-phone" data-v-14f9065e></i></button></div><a href="https://sspai.com/s/JYjP" target="_blank" data-v-14f9065e><!----></a></div><div class="item-wrapper" data-v-7be936cf><button class="btn btn-charge" data-v-7be936cf><i class="iconfont" data-v-7be936cf></i></button><span class="count" data-v-7be936cf>2</span></div><div class="item-wrapper" data-v-7be936cf><button class="btn-mini btn-comment" data-v-7be936cf><i class="iconfont iconfont-comment" data-v-7be936cf></i></button><span class="count" data-v-7be936cf>2</span></div><div class="item-wrapper" data-v-7be936cf><span data-v-7be936cf><div role="tooltip" id="el-popover-7709" aria-hidden="true" class="el-popover el-popper popper-share right ss-popper-dark-border" style="width:undefinedpx;display:none;"><!----><div class="article-side-share-btn"><a href="https://service.weibo.com/share/share.php?url=null?ref=weibo&title=%E3%80%90%E7%9B%B8%E6%AF%94%E4%BA%8E%E6%9C%BA%E5%99%A8%EF%BC%8C%E6%88%91%E4%BB%AC%E5%AF%B9%E4%BA%BA%E7%B1%BB%E7%9A%84%E9%94%99%E8%AF%AF%E6%9B%B4%E5%8A%A0%E5%AE%BD%E5%AE%B9%E5%90%97%EF%BC%9F%E3%80%91Assessingoutcomesvsintention.%E8%AF%84%E4%BC%B0%E7%BB%93%E6%9E%9C%E4%B8%8E%E7%9B%AE%E7%9A%84%E4%BD%A0%E5%AF%B9%E9%A2%91%E7%B9%81%E6%AD%BB%E6%9C%BA%E7%9A%84%E7%94%B5%E8%84%91%E6%84%9F%E5%88%B0%E5%B4%A9%E6%BA%83%E5%90%97%EF%BC%9F%E6%88%91%E4%BB%AC%E5%8F%AF%E8%83%BD%E8%AE%A4%E4%B8%BA%EF%BC%9A%E7%9B%B8%E6%AF%94%E4%BA%8E%E7%AC%A8%E6%8B%99%E7%9A%84%E4%BA%BA%E7%B1%BB%EF%BC%8C%E6%88%91%E4%BB%AC%E5%AF%B9%EF%BC%88%E6%9D%A5%E8%87%AA%20%40%E5%B0%91%E6%95%B0%E6%B4%BEsspai%EF%BC%89%E5%85%A8%E6%96%87%EF%BC%9A&pic=https%3A%2F%2Fcdn.sspai.com%2F2021%2F11%2F06%2Fb0a08813f049cf76b8de19fe16f329a9.png%3FimageMogr2%2Fauto-orient%2Fquality%2F95%2Fthumbnail%2F!1420x708r%2Fgravity%2FCenter%2Fcrop%2F1420x708%2Finterlace%2F1&appkey=3196502474#" target="_blank"><i class="iconfont iconfont-weibo-simple right-16"></i></a><span><div role="tooltip" id="el-popover-5601" aria-hidden="true" class="el-popover el-popper" style="width:undefinedpx;display:none;"><!----><div style="text-align:center;"><div id="qr-code"></div><small class="qr-small">扫码分享</small></div></div><span class="el-popover__reference-wrapper"><i class="iconfont iconfont-wechat-simple right-16"></i></span></span><a href="https://twitter.com/share?text=%E3%80%90%E7%9B%B8%E6%AF%94%E4%BA%8E%E6%9C%BA%E5%99%A8%EF%BC%8C%E6%88%91%E4%BB%AC%E5%AF%B9%E4%BA%BA%E7%B1%BB%E7%9A%84%E9%94%99%E8%AF%AF%E6%9B%B4%E5%8A%A0%E5%AE%BD%E5%AE%B9%E5%90%97%EF%BC%9F%E3%80%91Assessingoutcomesvsintention.%E8%AF%84%E4%BC%B0%E7%BB%93%E6%9E%9C%E4%B8%8E%E7%9B%AE%E7%9A%84%E4%BD%A0%E5%AF%B9%E9%A2%91%E7%B9%81%E6%AD%BB%E6%9C%BA%E7%9A%84%E7%94%B5%E8%84%91%E6%84%9F%E5%88%B0%E5%B4%A9%E6%BA%83%E5%90%97%EF%BC%9F%E6%88%91%E4%BB%AC%E5%8F%AF%E8%83%BD%E8%AE%A4%E4%B8%BA%EF%BC%9A%E7%9B%B8%E6%AF%94%E4%BA%8E%E7%AC%A8%E6%8B%99%E7%9A%84%E4%BA%BA%E7%B1%BB%EF%BC%8C%E6%88%91%E4%BB%AC%E5%AF%B9%EF%BC%88%E6%9D%A5%E8%87%AA%20%40%E5%B0%91%E6%95%B0%E6%B4%BEsspai%EF%BC%89%E5%85%A8%E6%96%87%EF%BC%9A&url=null" target="_blank" class="twitter"><i class="iconfont iconfont-twitter-simple right-16"></i></a></div></div><span class="el-popover__reference-wrapper"><button class="btn-mini btn-share" data-v-7be936cf><i class="iconfont iconfont-share" data-v-7be936cf></i></button></span></span></div><div class="item-wrapper" data-v-7be936cf><button class="btn-mini btn-collect" data-v-7be936cf><i class="iconfont iconfont-collect" data-v-7be936cf></i></button></div><!----></div><!---->  
</div>
            