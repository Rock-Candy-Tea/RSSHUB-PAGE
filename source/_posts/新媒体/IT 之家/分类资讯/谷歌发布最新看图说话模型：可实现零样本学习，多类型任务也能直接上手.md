
---
title: '谷歌发布最新看图说话模型：可实现零样本学习，多类型任务也能直接上手'
categories: 
 - 新媒体
 - IT 之家
 - 分类资讯
headimg: 'https://img.ithome.com/newsuploadfiles/2021/10/17b54316-626a-4b5b-be13-95c12a49ad72.jpg@s_2,w_820,h_277'
author: IT 之家
comments: false
date: Sun, 24 Oct 2021 07:53:54 GMT
thumbnail: 'https://img.ithome.com/newsuploadfiles/2021/10/17b54316-626a-4b5b-be13-95c12a49ad72.jpg@s_2,w_820,h_277'
---

<div>   
<p data-vmark="4f01">谷歌新推出了<span class="accentTextColor">弱监督看图说话模型 SimVLM</span>，能够轻松实现零样本学习（zero-shot）任务迁移。</p><p data-vmark="28f2">从文字描述图像到回答图片相关问题，模型无需微调也能样样精通。</p><p data-vmark="64da"><img src="https://img.ithome.com/newsuploadfiles/2021/10/17b54316-626a-4b5b-be13-95c12a49ad72.jpg@s_2,w_820,h_277" w="1002" h="338" title="谷歌发布最新看图说话模型：可实现零样本学习，多类型任务也能直接上手" srcset="https://img.ithome.com/newsuploadfiles/2021/10/17b54316-626a-4b5b-be13-95c12a49ad72.jpg 2x" width="1002" height="277" referrerpolicy="no-referrer"></p><p data-vmark="dc84">对于一般的视觉语言预训练（VLP）模型，训练数据集中要求包含大量精准标签。而模型的任务迁移，则需要针对特定任务重新进行数据集的标签标注。</p><p data-vmark="e574">总结下来，就是标注数据集不仅耗时耗力，还不能多任务通用。</p><p data-vmark="56f8">能不能开发出一种又简单又万能的 VLP 模型呢？</p><p data-vmark="f383">谷歌新开发的这款模型使用了弱监督学习进行模型训练，通过利用大量的弱对齐图像-文本对进行建模，简化了 VLP 的训练流程，大大降低了训练的复杂性。</p><p data-vmark="04f8">SimVLM 使用前缀语言建模的单一目标进行端到端训练，并<span class="accentTextColor">直接将原始图像作为输入</span>。这些设置允许模型对大规模的弱标记数据集进行利用，从而能够更好地实现零样本学习泛化效果。</p><h2 data-vmark="1949">SimVLM 模型是如何实现的？</h2><p data-vmark="3da8">SimVLM 模型的预训练过程采用了前缀语言建模 (PrefixLM) 的单一目标，接受序列的前缀作为输入，通过模型解码器来预测其延续的内容。</p><p data-vmark="9ed3">对于数据集中的图像-文本对，图像序列可视作其文本描述的前缀。</p><p data-vmark="c0fd">这种方法可以简化训练过程，最大限度地提高模型在适应不同任务设置方面的灵活性和通用性。</p><p data-vmark="a67b">模型的主干网络，则使用了在语言和视觉任务上均表现突出的 Transformer 架构。</p><p data-vmark="d4c2">对输入的原始图像数据提取上下文 patch，这里采用了 ResNet 卷积网络。</p><p data-vmark="22c1"><img src="https://img.ithome.com/newsuploadfiles/2021/10/37cdf031-1948-4ad9-b2ed-363fddb083f5.png" w="1080" h="552" title="谷歌发布最新看图说话模型：可实现零样本学习，多类型任务也能直接上手" width="1080" height="419" referrerpolicy="no-referrer"></p><p data-vmark="0645">如上图所示：<span class="accentTextColor">视觉模态中，图片被分割成多个 patch，然后压缩为一维序列。文本模态语句则被映射到了一个表征向量中</span>。</p><p data-vmark="be85">本模型使用了包含大约 1.8B 噪声的图像-文本对 ALIGN 训练集进行预训练，以此来实现更好的零样本学习泛化能力。</p><p data-vmark="3cf7">为了补偿训练集中的噪声影响，训练模型另外还使用了共 800G 的 Colossal Clean Crawled Corpus (C4) 数据集。</p><h2 data-vmark="19eb">SimVLM 模型基础性能如何？</h2><p data-vmark="e80e">模型的预训练完成后，需要在多模式任务中对模型进行微调，以测试性能。</p><p data-vmark="1c5b">这里用到的多模式任务有：VQA、NLVR2、SNLI-VE、COCO Caption、NoCaps 和 Multi30K En-De。</p><p data-vmark="4432"><img src="https://img.ithome.com/newsuploadfiles/2021/10/6f132c4e-9b9a-42d7-be40-1ef4fdee4390.jpg@s_2,w_820,h_266" w="1074" h="348" title="谷歌发布最新看图说话模型：可实现零样本学习，多类型任务也能直接上手" srcset="https://img.ithome.com/newsuploadfiles/2021/10/6f132c4e-9b9a-42d7-be40-1ef4fdee4390.jpg 2x" width="1074" height="266" referrerpolicy="no-referrer"></p><p data-vmark="54f9">▲ 性能指标：BLEU-4 (<a href="https://www.ithome.com/cdn-cgi/l/email-protection" class="__cf_email__" data-cfemail="c68486f2">[email protected]</a>)、METEOR (M)、CIDEr (C)、SPICE (S)</p><p data-vmark="a88f">将 SimVLM 模型与现有的功能完善的模型进行比较，测试结果如上表所示，参与评估的 SimVLM 模型还包括了三种不同规模：8600 万参数、3.07 亿参数和 6.32 亿参数。</p><p data-vmark="1621">跨模式任务的测试结果中，SimVLM 模型的性能表现最好（数据越大越好），除了 CoCo Caption 的 <a href="https://www.ithome.com/cdn-cgi/l/email-protection" class="__cf_email__" data-cfemail="36747602">[email protected]</a> 指标，在其他任务上都取得了新的 SOTA 结果，充分证明了该模型的先进性。</p><h2 data-vmark="d489">SimVLM 模型零样本泛化</h2><p data-vmark="94c1">SimVLM 模型在跨模式任务测试中可以取得不错的性能表现，那么它能否顺利执行零样本跨模态转移呢？</p><p data-vmark="9200">预训练的 SimVLM 模型仅对文本数据进行微调或完全不进行微调，通过图像字幕、多语言字幕、开放式 VQA 和视觉文本生成等任务，对模型进行测试。</p><p data-vmark="c807">测试结果如下图所示：</p><p data-vmark="3976"><img src="https://img.ithome.com/newsuploadfiles/2021/10/fbefe859-531d-4558-989a-04f19c44d9f8.png" w="637" h="238" title="谷歌发布最新看图说话模型：可实现零样本学习，多类型任务也能直接上手" width="637" height="238" referrerpolicy="no-referrer"></p><p data-vmark="ee59"><span class="accentTextColor">给定图像和文本提示，预训练模型无需微调即可预测图像的内容</span>。</p><p data-vmark="7e9a"><img src="https://img.ithome.com/newsuploadfiles/2021/10/aa53beac-b222-4950-9b98-ca7d11cdade3.png" w="635" h="238" title="谷歌发布最新看图说话模型：可实现零样本学习，多类型任务也能直接上手" width="635" height="238" referrerpolicy="no-referrer"></p><p data-vmark="fa22">除此之外，未进行过微调的模型在德语字幕生成、数据集外的答案生成、基于图像内容的文字描述、开放式视觉问题回答等应用上均有不错的表现。</p><p data-vmark="879b">为了量化 SimVLM 的零样本学习性能，这里采用了预训练的固化模型在 COCO Caption 和 NoCaps 上进行解码，然后与监督标准基线（Sup.）进行比较。</p><p data-vmark="f7a7"><img src="https://img.ithome.com/newsuploadfiles/2021/10/74acb108-62a3-45b9-a8a5-7a9adbaa4fce.png" w="1080" h="419" title="谷歌发布最新看图说话模型：可实现零样本学习，多类型任务也能直接上手" width="1080" height="318" referrerpolicy="no-referrer"></p><p data-vmark="3cb2">从结果对比上来看，即使没有监督微调，SimVLM 也可以达到有监督的训练质量水平。</p><h2 data-vmark="5b6e">作者介绍</h2><p data-vmark="800b">本研究的第一作者是谷歌学生研究员王子瑞，现就读于卡内基梅隆大学，曾以第一作者身份在 ICLR、EMNLP、CVPR 等顶会上发表了多篇论文。</p><p data-vmark="09d7"><img src="https://img.ithome.com/newsuploadfiles/2021/10/8b23a805-b78f-4b7c-a2bf-63170a5f65e3.jpg@s_2,w_820,h_1054" w="943" h="1212" title="谷歌发布最新看图说话模型：可实现零样本学习，多类型任务也能直接上手" srcset="https://img.ithome.com/newsuploadfiles/2021/10/8b23a805-b78f-4b7c-a2bf-63170a5f65e3.jpg 2x" width="943" height="1054" referrerpolicy="no-referrer"></p><p data-vmark="4b55">截止到 2020 年 12 月 20 日，他在 SuperGLUE 数据集上获得了第一个超过人类得分的 SOTA 性能（分数超过 90），目前则被百度团队赶超，位居第二。</p><p data-vmark="2557"><img src="https://img.ithome.com/newsuploadfiles/2021/10/dcbeb4a0-4251-44a1-b1a5-547a412771ba.jpg@s_2,w_820,h_251" w="1080" h="331" title="谷歌发布最新看图说话模型：可实现零样本学习，多类型任务也能直接上手" srcset="https://img.ithome.com/newsuploadfiles/2021/10/dcbeb4a0-4251-44a1-b1a5-547a412771ba.jpg 2x" width="1080" height="251" referrerpolicy="no-referrer"></p><p data-vmark="4542">这一次开发的 SimVLM 也在 6 个视觉语言基准测试中达到了单模型 SOTA 性能，并实现了基于文本引导的零样本学习泛化能力。</p><h2 data-vmark="8547">参考链接：</h2><p data-vmark="684d">https://arxiv.org/abs/2108.10904</p><p data-vmark="98ca">https://ai.googleblog.com/2021/10/simvlm-simple-visual-language-model-pre.html</p><p data-vmark="0f6a">http://www.cs.cmu.edu/~ziruiw/</p>
          
</div>
            