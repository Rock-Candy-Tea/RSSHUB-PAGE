
---
title: '微软和英伟达推出迄今为止训练最大最强的语言模型 MT-NLG'
categories: 
 - 新媒体
 - IT 之家
 - 分类资讯
headimg: 'https://img.ithome.com/newsuploadfiles/2021/10/e2f025e0-f1ee-4eec-8ba9-df3c4c5597f6.png'
author: IT 之家
comments: false
date: Tue, 12 Oct 2021 00:17:06 GMT
thumbnail: 'https://img.ithome.com/newsuploadfiles/2021/10/e2f025e0-f1ee-4eec-8ba9-df3c4c5597f6.png'
---

<div>   
<p data-vmark="5900"><a class="s_tag" href="https://www.ithome.com/" target="_blank">IT之家</a> 10 月 12 日消息 语言模型（Language Model）简单来说就是一串词序列的概率分布，主要作用是为一个长度为 m 的文本确定一个概率分布 P，表示这段文本存在的可能性。</p><p data-vmark="9997">大家之前可能或多或少听说过 GPT-3，OpenAI 最新的语言模型，堪称地表最强语言模型，也被认为是革命性的人工智能模型。除此之外还有 BERT、Switch Transformer 等重量级产品，而且业内其他企业也在努力推出自家的模型。</p><p data-vmark="8f1b">微软和英伟达今天宣布了由 DeepSpeed 和 Megatron 驱动的 Megatron-Turing 自然语言生成模型（MT-NLG），这是迄今为止训练的最大和最强大的解码语言模型。</p><p data-vmark="8f2b"><img src="https://img.ithome.com/newsuploadfiles/2021/10/e2f025e0-f1ee-4eec-8ba9-df3c4c5597f6.png" w="1067" h="600" title="微软和英伟达推出迄今为止训练最大最强的语言模型 MT-NLG" width="1067" height="461" referrerpolicy="no-referrer"></p><p data-vmark="a104">IT之家了解到，作为 Turing NLG 17B 和 Megatron-LM 的继任者，这个模型包括 5300 亿个参数，而且 MT-NLG 的参数数量是同类现有最大模型 GPT-3 的 3 倍，并在一系列广泛的自然语言任务中展示了无与伦比的准确性，例如：</p><ul class=" list-paddingleft-2"><li><p data-vmark="bbb9">完成预测</p></li><li><p data-vmark="dd28">阅读理解</p></li><li><p data-vmark="ced9">常识推理</p></li><li><p data-vmark="b9ec">自然语言推理</p></li><li><p data-vmark="e019">词义消歧</p></li></ul><p data-vmark="da87">105 层、基于转换器的 MT-NLG 在零、单和少样本设置中改进了先前最先进的模型，并为两个模型规模的大规模语言模型设定了新标准和质量。</p><p data-vmark="0c66">据悉，模型训练是在基于 NVIDIA DGX SuperPOD 的 Selene 超级计算机上以混合精度完成的，该超级计算机由 560 个 DGX A100 服务器提供支持，这些服务器以完整的胖树（FatTree）配置与 HDR InfiniBand 联网。每个 DGX A100 有 8 个 NVIDIA A100 80GB Tensor Core GPU，通过 NVLink 和 NVSwitch 相互完全连接。微软 Azure NDv4 云超级计算机使用了类似的参考架构。</p><p data-vmark="9dce"><img src="https://img.ithome.com/newsuploadfiles/2021/10/40a125ea-c890-444a-a473-c8270a027a9e.jpg" w="300" h="168" title="微软和英伟达推出迄今为止训练最大最强的语言模型 MT-NLG" width="300" height="168" referrerpolicy="no-referrer"></p><p data-vmark="07f7" style="text-align: justify;">更多内容可查看微软和英伟达官方说明：</p><p data-vmark="856d" style="text-align: start;"><a href="https://www.microsoft.com/en-us/research/blog/using-deepspeed-and-megatron-to-train-megatron-turing-nlg-530b-the-worlds-largest-and-most-powerful-generative-language-model/" target="_blank" title="微软">微软</a></p><p data-vmark="395a" style="text-align: start;"><a href="https://developer.nvidia.com/blog/using-deepspeed-and-megatron-to-train-megatron-turing-nlg-530b-the-worlds-largest-and-most-powerful-generative-language-model/" target="_blank" title="英伟达">英伟达</a></p>
          
</div>
            