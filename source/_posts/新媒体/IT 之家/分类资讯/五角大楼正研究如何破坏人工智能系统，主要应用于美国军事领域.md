
---
title: '五角大楼正研究如何破坏人工智能系统，主要应用于美国军事领域'
categories: 
 - 新媒体
 - IT 之家
 - 分类资讯
headimg: 'https://img.ithome.com/newsuploadfiles/2021/7/ab701a3f-5cca-4172-8ab4-97a4292df3ee.png'
author: IT 之家
comments: false
date: Tue, 20 Jul 2021 01:11:31 GMT
thumbnail: 'https://img.ithome.com/newsuploadfiles/2021/7/ab701a3f-5cca-4172-8ab4-97a4292df3ee.png'
---

<div>   
<p>五角大楼认为，人工智能是一种<span class="accentTextColor">战胜和支配未来对手的方式</span>。人工智能的脆弱本质意味着，如果没有足够的重视，这项技术可能会为敌人提供一种新的攻击方式。</p><p><img src="https://img.ithome.com/newsuploadfiles/2021/7/ab701a3f-5cca-4172-8ab4-97a4292df3ee.png" w="1080" h="501" title="五角大楼正研究如何破坏人工智能系统，主要应用于美国军事领域" width="1080" height="380" referrerpolicy="no-referrer"></p><p>由五角大楼创建的联合人工智能中心 (Joint Artificial Intelligence Center)，旨在帮助美国军方利用人工智能。该中心最近成立了一个部门，负责<span class="accentTextColor">收集、审查和向国防部各部门分发开源和工业机器学习模型</span>。这种努力一部分指向了将人工智能用于军事目的的一个关键挑战。</p><p>一个被称为“测试和评估小组”(Test and Evaluation Group) 的机器学习“<span class="accentTextColor">红队</span>”将调查预先训练过的模型的弱点，另一个网络安全团队则检查人工智能代码和数据中隐藏的漏洞。</p><p>机器学习是现代人工智能背后的技术，它代表了一种根本不同的、通常更强大的编写计算机代码的方式。机器学习通过从数据中学习生成自己的规则，而不是编写让机器遵循的规则。问题是，这种学习过程，以及训练数据中的人工制品或错误，可能会导致 AI 模型以奇怪或不可预测的方式行为。</p><p>JAIC 战略和政策主管格雷戈里・艾伦 (Gregory Allen) 表示:“对于某些应用来说，机器学习软件比传统软件要好上万亿倍。”但是，他补充说，机器学习“也<span class="accentTextColor">以不同于传统软件的方式进行突破</span>。”</p><p>例如，训练用来识别卫星图像中特定车辆的机器学习算法，也可能学会将车辆与周围某一颜色的风景联系起来。对手可以通过改变车辆周围的场景来欺骗 AI。在获取训练数据时，对手也可以植入图像，比如特定的符号，这会使算法混乱。</p><p>艾伦说，五角大楼对其使用的软件的可靠性和安全性有严格的规定，这种方法可以扩展到人工智能和机器学习，并指出 JAIC 正在更新国防部的软件标准，包括机器学习方面的问题。</p><p>人工智能正在改变一些企业的运营方式，因为它可能是一种高效而强大的方式来实现任务和流程的自动化。</p><p>例如，一家公司可以使用一个人工智能算法，查看数千或数百万以前的销售情况，并设计自己的模型，预测谁将购买什么产品。</p><p>美国和其他国家的军队也看到了类似的优势，正急于利用人工智能来<span class="accentTextColor">改善后勤、情报收集、任务规划和武器技术</span>。</p><p>中国不断增长的技术能力在五角大楼内激起了采用人工智能的紧迫感。艾伦表示，国防部正在“以负责任的方式，优先考虑安全性和可靠性”。</p><p>研究人员正在开发更有创造性的方法来<span class="accentTextColor">破解、颠覆或破坏人工智能系统</span>。</p><p>2020 年 10 月，以色列的研究人员展示了经过精心调整的图像会如何混淆让特斯拉解读前方道路的人工智能算法。这类“对抗性攻击”涉及对机器学习算法的输入进行调整，以找到导致重大错误的小变化。</p><p>加州大学伯克利分校教授 Dawn Song 在特斯拉的传感器和其他人工智能系统上进行了类似的实验。他表示，在欺诈检测等领域，对机器学习算法的攻击已经成为一个问题。一些公司提供工具来测试用于金融领域的人工智能系统。“自然会有攻击者想要逃避系统，”她说。“我认为我们将看到更多这类问题。”</p><p>一个简单的机器学习攻击的例子，涉及到 2016 年首次亮相的微软聊天机器人 Tay，其使用了一种算法，通过检查之前的对话来学习如何回应新的询问，reddit 用户很快意识到他们可以利用这一点，让 Tay 发布仇恨信息。</p><p>马里兰大学研究机器学习算法脆弱性的副教授 Tom Goldstein 表示，攻击人工智能系统的方法有很多种，包括修改输入算法的数据，使其以特定方式行动。机器学习模型与传统软件的不同之处在于，获得一个模型可以让对手设计一种无法抵御的攻击，比如误导性输入。</p><p>戈尔茨坦说:“我们真的不知道如何解决人工智能的所有弱点，我们不知道如何让系统完全抵御对抗性攻击。”</p><p>在军事方面，如果有一个资源丰富、技术先进的对手，防范各种新的进攻路线非常重要。</p><p>乔治敦大学安全与新兴技术中心的最近一份报告警告称，人工智能中的“数据中毒”可能对国家安全构成严重威胁。这将涉及渗透用于训练人工智能模型的过程，可能是通过让一个代理志愿者给输入算法的图像贴上标签，或者在网络上植入图像，然后提取图像并输入人工智能模型。</p>
          
</div>
            