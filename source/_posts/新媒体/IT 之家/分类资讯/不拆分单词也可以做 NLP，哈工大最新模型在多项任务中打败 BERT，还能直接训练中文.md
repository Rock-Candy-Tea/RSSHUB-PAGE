
---
title: '不拆分单词也可以做 NLP，哈工大最新模型在多项任务中打败 BERT，还能直接训练中文'
categories: 
 - 新媒体
 - IT 之家
 - 分类资讯
headimg: 'https://img.ithome.com/newsuploadfiles/2022/3/8727675e-56b2-4721-b459-7dd9aa5a7365.jpg'
author: IT 之家
comments: false
date: Sat, 05 Mar 2022 07:13:16 GMT
thumbnail: 'https://img.ithome.com/newsuploadfiles/2022/3/8727675e-56b2-4721-b459-7dd9aa5a7365.jpg'
---

<div>   
<p data-vmark="0ef1">众所周知，BERT 在预训练时会对某些单词进行拆分 （术语叫做“WordPiece”）。比如把“loved”、“loving”和“loves”拆分成“lov”、“ed”、“ing”和”es”。</p><p data-vmark="2fdf">目的是缩减词表、加快训练速度，但这样一来，在某些时候反而会阻碍模型的理解能力。比如把”lossless”分成”loss”和”less”的时候。</p><p data-vmark="bd0b">现在，来自哈工大和腾讯 AI Lab 的研究人员，尝试利用不做单词拆分的词汇表开发了一个 BERT 风格的预训练模型 ——WordBERT。结果，这个 WordBERT 在完形填空测试和机器阅读理解方面的成绩相比 BERT 有了很大提高。</p><p data-vmark="028d">在其他 NLP 任务，比如词性标注 (POS-Tagging)、组块分析 (Chunking) 和命名实体识别 (NER) 中，WordBERT 的表现也都优于 BERT。由于不用分词，这个 WordBERT 还可以直接进行中文训练。更值得一提的是，它在性能提升的同时，推理速度并没有变慢。</p><p data-vmark="030a" style="text-align: center;"><img src="https://img.ithome.com/newsuploadfiles/2022/3/8727675e-56b2-4721-b459-7dd9aa5a7365.jpg" w="292" h="350" title="不拆分单词也可以做 NLP，哈工大最新模型在多项任务中打败 BERT，还能直接训练中文" width="292" height="350" referrerpolicy="no-referrer"></p><p data-vmark="4650">可谓一举多得。</p><h3 data-vmark="68c0">NO WordPieces</h3><p data-vmark="0916">与 BERT 类似，WordBERT 包含两个组件：词向量（word embedding）和 Transformer 层。和以前的模型一样，WordBERT 采用多层双向 Transformer 来学习语境表示（contextualized representation）。</p><p data-vmark="3591">word embedding 则是用来获得单词向量表示的参数矩阵，与把单词分成 WordPiece 的 BERT 相比，WordBERT 的词汇由完整的单词组成。他们用自然语言处理软件包 Spacy 处理数据，生成了两个词汇表，一个规模为 500K，一个为 1M。词汇表中还被单独添加了 5 个特殊单词：[PAD]、[UNK]、 [CLS]、[SEP] 和 [MASK]。</p><p data-vmark="0471">通过不同的词汇表规模、初始化配置和不同语言，最后研究人员一共训练出四个版本的 WordBERT：WordBERT-500K、WordBERT-1M、WordBERT-Glove 和 WordBERT-ZH。</p><p data-vmark="712d" style="text-align: center;"><img src="https://img.ithome.com/newsuploadfiles/2022/3/e70dbfc2-2a06-4fcd-9769-a5831db5fecf.png" w="1080" h="238" title="不拆分单词也可以做 NLP，哈工大最新模型在多项任务中打败 BERT，还能直接训练中文" width="1080" height="181" referrerpolicy="no-referrer"></p><p data-vmark="9fbe">它们的配置如上，嵌入参数都是随机初始化的，嵌入维数和基准 BERT 保持一致。其中 WordBERT-Glove 用的词汇表是现成的 Glove vocabulary，里面包含约 190 万个未编码的单词，该模型由相应的单词向量（word vectors）在 WordBERT 之上初始化而来。WordBERT-ZH 则是用中文词汇训练出来的 WordBERT，它也保持了 768 的词嵌入维数。</p><h3 data-vmark="202c">性能与速度兼具</h3><p data-vmark="a34d">在测试环节中，完形填空的测试数据集来自 CLOTH，它由中学教师设计，通常用来对中国初高中学生进行入学考试。其中既有只需在当前句子中进行推理的简单题，也有需要在全文范围内进行推理的难题。结果如下：</p><p data-vmark="fd67" style="text-align: center;"><img src="https://img.ithome.com/newsuploadfiles/2022/3/a36fd080-ff0e-4dac-9788-2a34dd61c385.png" w="592" h="334" title="不拆分单词也可以做 NLP，哈工大最新模型在多项任务中打败 BERT，还能直接训练中文" width="592" height="334" referrerpolicy="no-referrer"></p><p data-vmark="53e5" style="text-align: center;">△ M 代表初中，H 代表高中</p><p data-vmark="275a">WordBERT-1M 获得了最佳成绩，并接近人类水平。它在高中题比 BERT 高了 3.18 分，初中题高了 2.59 分，这说明 WordBERT 在复杂任务中具有更高的理解和推理能力。在词性标注、组块分析和命名实体识别（NER）等分类任务中，WordBERT 的成绩如下：</p><p data-vmark="be46" style="text-align: center;"><img src="https://img.ithome.com/newsuploadfiles/2022/3/cf363a5e-8ecb-4088-8538-331a83163963.png" w="1026" h="280" title="不拆分单词也可以做 NLP，哈工大最新模型在多项任务中打败 BERT，还能直接训练中文" width="1026" height="224" referrerpolicy="no-referrer"></p><p data-vmark="83e9">相比来看，它在 NER 任务上的优势更明显一些（后两列）。</p><p data-vmark="f5d2">研究人员推测，这可能是 WordBERT 在学习低频词的表征方面有优势，因为命名实体（named entities）往往就是一些不常见的稀有词。对于“中文版”WordBERT-ZH，研究人员在 CLUE benchmark 上的各种任务中测试其性能。除了 BERT，对比模型还包括 WoBERT 和 MarkBERT，这也是两个基于 BERT 预训练的中文模型。</p><p data-vmark="3201" style="text-align: center;"><img src="https://img.ithome.com/newsuploadfiles/2022/3/226ec338-aa7f-41e7-9b66-260b1c8cd52e.png" w="872" h="286" title="不拆分单词也可以做 NLP，哈工大最新模型在多项任务中打败 BERT，还能直接训练中文" width="872" height="269" referrerpolicy="no-referrer"></p><p data-vmark="434b">结果，WordBERT-ZH 在四项任务中都打败了所有其他对比模型，在全部五项任务上的表现都优于基线 BERT，并在 TNEWS（分类）、OCNLI（推理）和 CSL（关键字识别）任务上取得了 3 分以上的差距。这说明，基于词的模型对中文也是非常有效的。</p><p data-vmark="3c6c">最后，实验还发现：性能不差的 WordBERT，在不同任务上的推理速度也并未“落于下风”。</p><p data-vmark="164e" style="text-align: center;"><img src="https://img.ithome.com/newsuploadfiles/2022/3/9645e3f3-b04c-44bd-948a-c3552b887d85.png" w="528" h="400" title="不拆分单词也可以做 NLP，哈工大最新模型在多项任务中打败 BERT，还能直接训练中文" width="528" height="400" referrerpolicy="no-referrer"></p><h3 data-vmark="cd0e">关于作者</h3><p data-vmark="7dcd">一作为哈工大计算机专业在读博士生冯掌印，研究方向为 NLP、文本生成。</p><p data-vmark="db99" style="text-align: center;"><img src="https://img.ithome.com/newsuploadfiles/2022/3/f4f3732d-7ce0-4b8b-9523-1c3e142c284c.png" w="1080" h="1085" title="不拆分单词也可以做 NLP，哈工大最新模型在多项任务中打败 BERT，还能直接训练中文" width="1080" height="824" referrerpolicy="no-referrer"></p><p data-vmark="3675">他曾在微软亚研院自然语言计算组、哈工大和科大讯飞联合实验室实习，在 NLP 领域的顶会 ENNLP 发表过一篇一作论文。</p><p data-vmark="92cc">通讯作者为史树明，来自腾讯 AI Lab。</p><p data-vmark="14c3" style="text-align: center;"><img src="https://img.ithome.com/newsuploadfiles/2022/3/7a0ce917-7922-47cd-9029-fa36ef2fee82.png" w="250" h="256" title="不拆分单词也可以做 NLP，哈工大最新模型在多项任务中打败 BERT，还能直接训练中文" width="250" height="256" referrerpolicy="no-referrer"></p><p data-vmark="3cf8">论文地址：</p><p data-vmark="afd5"><span class="link-text-start-with-http">https://arxiv.org/abs/2202.12142</span></p>
          
</div>
            