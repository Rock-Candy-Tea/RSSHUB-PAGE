
---
title: '上海交大发布「人类行为理解引擎」：深度学习 + 符号推理，AI 逐帧理解大片中每个动作'
categories: 
 - 新媒体
 - IT 之家
 - 分类资讯
headimg: 'https://img.ithome.com/newsuploadfiles/2022/3/73661d35-8846-4ec2-9b39-c20ca39c6506.png'
author: IT 之家
comments: false
date: Wed, 09 Mar 2022 11:53:53 GMT
thumbnail: 'https://img.ithome.com/newsuploadfiles/2022/3/73661d35-8846-4ec2-9b39-c20ca39c6506.png'
---

<div>   
<p data-vmark="eec7">看图看片，对现在的 AI 来说早已不是什么难事。不过让 AI 分析视频中的人类动作时，传统基于目标检测的方法会碰到一个挑战：静态物体的模式与行为动作的模式有很大不同，现有系统效果很不理想。</p><p data-vmark="a009">现在，来自上海交大的卢策吾团队基于这一思路，将整个任务分为了两个阶段：先将像素映射到一个“基元活动”组成的过度空间，然后再用可解释的逻辑规则对检测到的基元做推断。</p><p data-vmark="9f1a" style="text-align: center;"><img src="https://img.ithome.com/newsuploadfiles/2022/3/73661d35-8846-4ec2-9b39-c20ca39c6506.png" w="1080" h="240" title="上海交大发布「人类行为理解引擎」：深度学习 + 符号推理，AI 逐帧理解大片中每个动作" width="1080" height="182" referrerpolicy="no-referrer"></p><p data-vmark="da34">△ 左：传统方法，右：新方法</p><p data-vmark="4118">新方法让 AI 真正看懂剧里的卷福手在举杯 (hold)，右边的人在伸手掏东西 (reach for)：</p><p data-vmark="13b8" style="text-align: center;"><img src="https://img.ithome.com/newsuploadfiles/2022/3/e770176b-d85a-4eb8-b09b-4d16302cfea8.gif" w="846" h="470" title="上海交大发布「人类行为理解引擎」：深度学习 + 符号推理，AI 逐帧理解大片中每个动作" width="846" height="456" referrerpolicy="no-referrer"></p><p data-vmark="9a9a">对于游戏中的多人场景也能准确分辨每一个角色的当前动作：</p><p data-vmark="f49b" style="text-align: center;"><img src="https://img.ithome.com/newsuploadfiles/2022/3/8bb141a0-4cfd-42ad-8503-f0516b0f7a06.gif" w="840" h="460" title="上海交大发布「人类行为理解引擎」：深度学习 + 符号推理，AI 逐帧理解大片中每个动作" width="840" height="449" referrerpolicy="no-referrer"></p><p data-vmark="8757">甚至连速度飞快的自行车运动员都能完美跟随：</p><p data-vmark="af5e" style="text-align: center;"><img src="https://img.ithome.com/newsuploadfiles/2022/3/3b6ce591-3105-4cde-a938-856d3eda0eb3.gif" w="846" h="470" title="上海交大发布「人类行为理解引擎」：深度学习 + 符号推理，AI 逐帧理解大片中每个动作" width="846" height="456" referrerpolicy="no-referrer"></p><p data-vmark="0f7b">能够像这样真正理解视频的 AI，就能在医疗健康护理、指引、警戒等机器人领域应用。这篇论文的一作为上海交大博士李永露，曾在 CVPR 2020 连中三篇论文。目前相关代码已开源。</p><h3 data-vmark="8858">知识驱动的行为理解</h3><p data-vmark="0e5d">要让 AI 学习人类，首先要看看人类是怎么识别活动的。比如说，要分辨走路和跑步，我们肯定会优先关注腿部的运动状态。再比如，要分辨一个人是否是在“喝水”，那么他的手是否在握杯，随后头又是否接触杯子，这些动作就成为了一个判断标准。这些原子性的，或者说共通的动作就可以被看作是一种“基元”（Primitive）。</p><p data-vmark="bee0" style="text-align: center;"><img src="https://img.ithome.com/newsuploadfiles/2022/3/20064901-6f24-4fb7-b63b-eed288573373.png" w="1008" h="348" title="上海交大发布「人类行为理解引擎」：深度学习 + 符号推理，AI 逐帧理解大片中每个动作" width="1008" height="283" referrerpolicy="no-referrer"></p><p data-vmark="8d4e">我们正是将一个个的基元“组合”推理出整体的动作，这就是就是人类的活动感知。那么 AI 是否也能基于发现这种基元的能力，将其进行组合，并编程为某个具有组合概括性的语义呢？因此，卢策吾团队便提出了一种知识驱动的人类行为知识引擎，HAKE（Human Activity Knowledge Engine）。</p><p data-vmark="8cda">这是一个两阶段的系统：</p><ul class=" list-paddingleft-2"><li><p data-vmark="a26e">将像素映射到由原子活动基元跨越的中间空间</p></li><li><p data-vmark="0c6a">用一个推理引擎将检测到的基元编程为具有明确逻辑规则的语义，并在推理过程中更新规则。</p></li></ul><p data-vmark="f6d0" style="text-align: center;"><img src="https://img.ithome.com/newsuploadfiles/2022/3/1f118b95-ce65-4f4b-8e58-a611214dad3a.png" w="1080" h="403" title="上海交大发布「人类行为理解引擎」：深度学习 + 符号推理，AI 逐帧理解大片中每个动作" width="1080" height="306" referrerpolicy="no-referrer"></p><p data-vmark="9923">整体来说，上述两个阶段也可以分为两个任务。首先是建立一个包括了丰富的活动-基元标签的知识库，作为推理的“燃料”。在于 702 位参与者合作之后，HAKE 目前已有 35.7 万的图像 / 帧，67.3 万的人像，22 万的物体基元，以及 2640 万的 PaSta 基元。</p><p data-vmark="0cbe" style="text-align: center;"><img src="https://img.ithome.com/newsuploadfiles/2022/3/5a1ca2b9-8f7b-4afd-bf9e-806605236c64.png" w="1074" h="484" title="上海交大发布「人类行为理解引擎」：深度学习 + 符号推理，AI 逐帧理解大片中每个动作" width="1074" height="370" referrerpolicy="no-referrer"></p><p data-vmark="459e">其次，是构建逻辑规则库和推理引擎。在检测到基元后，研究团队使用深度学习来提取视觉和语言表征，并以此来表示基元。然后，再用可解释的符号推理按照逻辑规则为基元编程，捕获因果的原始活动关系。</p><p data-vmark="441c" style="text-align: center;"><img src="https://img.ithome.com/newsuploadfiles/2022/3/0f2fde0a-089d-4b39-9acc-b17bec419f46.png" w="1080" h="321" title="上海交大发布「人类行为理解引擎」：深度学习 + 符号推理，AI 逐帧理解大片中每个动作" width="1080" height="244" referrerpolicy="no-referrer"></p><p data-vmark="ddd7">在实验中，研究者选取了建立在 HICO 基础上，包含 4.7 万张图片和 600 次互动的 HICO-DET，以及包含 430 个带有时空标签的视频的 AVA，这两个大规模的基准数据集。在两个数据集上进行实例级活动检测：即同时定位活动的人 / 物并对活动进行分类。</p><p data-vmark="3bf7">结果，HAKE，在 HICO-DET 上大大提升了以前的实例级方法，特别是在稀有集上，比 TIN 提高了 9.74mAP（全类平均精度），HAKE 的上限 GT-HAKE 也优于最先进的方法。在 AVA 上，HAKE 也提高了相当多的活动的检测性能，特别是 20 个稀有的活动。</p><p data-vmark="09d2" style="text-align: center;"><img src="https://img.ithome.com/newsuploadfiles/2022/3/3c2e0182-15c0-4fd0-b790-69107ac54b85.png" w="740" h="410" title="上海交大发布「人类行为理解引擎」：深度学习 + 符号推理，AI 逐帧理解大片中每个动作" width="740" height="410" referrerpolicy="no-referrer"></p><h3 data-vmark="2ae6">通讯作者曾为李飞飞团队成员</h3><p data-vmark="baec">论文的通讯作者是上海交通大学的卢策吾，也是计算机科学的教授。在加入上海交大之前，他在中国香港中文大学获得了博士学位，并曾在斯坦福大学担任研究员，在李飞飞团队工作。现在，他的主要研究领域为计算机视觉、深度学习、深度强化学习和机器人视觉。</p><p data-vmark="7c79" style="text-align: center;"><img src="https://img.ithome.com/newsuploadfiles/2022/3/1c926752-a090-42dd-bee0-92131797ad0f.png" w="265" h="265" title="上海交大发布「人类行为理解引擎」：深度学习 + 符号推理，AI 逐帧理解大片中每个动作" width="265" height="265" referrerpolicy="no-referrer"></p><p data-vmark="b363">一作李永露为上海交通大学的博士生，此前他曾在中国科学院自动化研究所工作。在 CVPR 2020 他连中三篇论文，也都是围绕知识驱动的行为理解（Human Activity Understanding）方面的工作。</p><p data-vmark="1761" style="text-align: center;"><img src="https://img.ithome.com/newsuploadfiles/2022/3/562e4169-f792-4696-a884-ebec22f1d715.png" w="480" h="557" title="上海交大发布「人类行为理解引擎」：深度学习 + 符号推理，AI 逐帧理解大片中每个动作" width="480" height="557" referrerpolicy="no-referrer"></p><p data-vmark="cd93">论文：</p><p data-vmark="b0f2"><span class="link-text-start-with-http">https://arxiv.org/abs/2202.06851v1</span></p><p data-vmark="50d0">开源链接：</p><p data-vmark="1fa5"><span class="link-text-start-with-http">https://github.com/DirtyHarryLYL/HAKE-Action-Torch/tree/Activity2Vec</span></p>
          
</div>
            