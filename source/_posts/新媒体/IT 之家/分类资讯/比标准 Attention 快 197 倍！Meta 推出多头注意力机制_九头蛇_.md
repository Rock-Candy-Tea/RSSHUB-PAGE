
---
title: '比标准 Attention 快 197 倍！Meta 推出多头注意力机制_九头蛇_'
categories: 
 - 新媒体
 - IT 之家
 - 分类资讯
headimg: 'https://img.ithome.com/newsuploadfiles/2022/9/9a7ef1c2-20df-402b-bd14-57c0fe4854f5.png'
author: IT 之家
comments: false
date: Tue, 20 Sep 2022 08:20:22 GMT
thumbnail: 'https://img.ithome.com/newsuploadfiles/2022/9/9a7ef1c2-20df-402b-bd14-57c0fe4854f5.png'
---

<div>   
<p data-vmark="64f2">尽管 Transformer 已经开始在诸多视觉任务上“大展身手”，但还有一个问题。</p><p data-vmark="5218">那就是在处理<strong>大图像</strong>上计算比较费劲。</p><p data-vmark="3349">比如面对一个 1080p 的图时，它会有<strong>超过 60% 的计算量</strong>都耗费在了创建和应用注意矩阵上。</p><p data-vmark="a596" style="text-align: center;"><img src="https://img.ithome.com/newsuploadfiles/2022/9/9a7ef1c2-20df-402b-bd14-57c0fe4854f5.png" w="240" h="240" title="比标准 Attention 快 197 倍！Meta 推出多头注意力机制“九头蛇”" width="240" height="240" referrerpolicy="no-referrer"></p><p data-vmark="dfc6">究其原因，主要是因为自注意力头的数量是 token 的平方，而 token 的数量又与图形大小呈二次方的关系。</p><p data-vmark="4ff6">那能怎么办呢？</p><p data-vmark="d9f3">好消息是 ——</p><p data-vmark="4b18">现在 Meta 捣鼓出了一种<strong>多头注意力</strong>操作方法，可以做到足足<strong>比标准注意力快 197 倍</strong>！</p><p data-vmark="ef47">而且在提高计算速度的同时，它也不会牺牲准确率，有时甚至还能将准确率提高 1-2 个点。</p><p data-vmark="a1da" style="text-align: center;"><img src="https://img.ithome.com/newsuploadfiles/2022/9/05de8819-e8f1-4196-8322-f64ec77c559f.png" w="1080" h="226" title="比标准 Attention 快 197 倍！Meta 推出多头注意力机制“九头蛇”" width="1080" height="172" referrerpolicy="no-referrer"></p><p data-vmark="0d33">具体怎么回事儿？</p><h2 data-vmark="c10d">思路来源一个“矛盾点”</h2><p data-vmark="146d">这个方法名叫 <strong>Hydra Attention</strong>，主要针对 Vision Transformer。</p><p data-vmark="bb4a">（“Hydra”有“九头蛇”之义，来自希腊神话。）</p><p data-vmark="3dad">Hydra Attention 的思路源于线性注意力中的一种<strong>有点矛盾</strong>的点：</p><p data-vmark="5a05">使用标准的多头自注意力，再向模型中添加更多头可以保持计算量不变。</p><p data-vmark="275d">而在线性注意力中改变操作顺序后，增加更多的头实际上还会降低层的计算成本。</p><p data-vmark="545b">于是，作者通过将模型中的<strong>注意力头数量设置成特征</strong>（feature）<strong>数</strong>，创建出一个 token 和 feature 的计算都是线性的注意力模块，从而把上述特性发挥到极致。</p><p data-vmark="de6d">具体来说：</p><p data-vmark="6853" style="text-align: center;"><img src="https://img.ithome.com/newsuploadfiles/2022/9/99844df3-e4e4-4c9c-8568-3c0744945131.png" w="948" h="418" title="比标准 Attention 快 197 倍！Meta 推出多头注意力机制“九头蛇”" width="948" height="362" referrerpolicy="no-referrer"></p><p data-vmark="fbc2">当标准自注意力头是 token 数的平方（O (T2D)）时，通过使用可分解核（decomposable kernel），我们重新安排操作顺序，让注意力头的数量变为特征 D 的平方。</p><p data-vmark="cb51">然后再使用 Hydra Trick，最大化注意力头 H 的数量，让 H=D，最终就可以化为一个在空间和时间上的 O（TD）简化操作了。</p><p data-vmark="f558">其中，Hydra Trick 的依据见下图：</p><p data-vmark="4074" style="text-align: center;"><img src="https://img.ithome.com/newsuploadfiles/2022/9/52e83a85-5150-40db-955f-9336c3e4a71b.png" w="1080" h="430" title="比标准 Attention 快 197 倍！Meta 推出多头注意力机制“九头蛇”" width="1080" height="326" referrerpolicy="no-referrer"></p><p data-vmark="2989">作者在 ImageNet-1k 上训练了具有不同头数的 DeiT-B 模型，包括使用标准自注意力（蓝线，基于 softmax）和使用多头线性注意（红线，基于余弦相似性）的。</p><p data-vmark="6d38">前者在 H>96、后者在 H<3 时出现内存不足的情况。</p><p data-vmark="69ce">当他们往模型中添加更多的头时，Softmax 注意力模型的准确性似乎会崩溃，而多头线性注意力仍可以保持一致，因此就有了上述操作。</p><p data-vmark="595e">（需要注意的是，H 必须除以 D=768。）</p><h2 data-vmark="b0d4">速度快 197 倍，准确率还能更上层楼</h2><p data-vmark="268d">来看看 Hydra Attention 交出的成绩单。</p><p data-vmark="d036">可以看到，Hydra 注意力比标准注意力快 197 倍（T=197）。</p><p data-vmark="0ef6">随着图像大小的增加，它显著提高了模型的 FLOP 数，在创建和应用注意力矩阵所占的计算量上也<strong>始终只有 0.02%</strong>。</p><p data-vmark="6edb" style="text-align: center;"><img src="https://img.ithome.com/newsuploadfiles/2022/9/f097702c-b20f-4b90-b774-fb4472d00dfa.png" w="964" h="258" title="比标准 Attention 快 197 倍！Meta 推出多头注意力机制“九头蛇”" width="964" height="219" referrerpolicy="no-referrer"></p><p data-vmark="1691">除此之外，作者发现，使用不同的内核，Hydra Attention 还能让模型的准确率提高大约两个百分点。</p><p data-vmark="45a2" style="text-align: center;"><img src="https://img.ithome.com/newsuploadfiles/2022/9/7bb4434a-3ff6-4eae-8cc9-361cb51299c6.png" w="936" h="254" title="比标准 Attention 快 197 倍！Meta 推出多头注意力机制“九头蛇”" width="936" height="223" referrerpolicy="no-referrer"></p><p data-vmark="5ddb">或者用 Hydra Attention 替换特定的注意力层，也能将模型的精度在 ImageNet 上提高 1% 或者与基线维持不变。</p><p data-vmark="c237" style="text-align: center;"><img src="https://img.ithome.com/newsuploadfiles/2022/9/d975a4fa-9152-4e18-9882-efe225607b61.png" w="928" h="352" title="比标准 Attention 快 197 倍！Meta 推出多头注意力机制“九头蛇”" width="928" height="311" referrerpolicy="no-referrer"></p><p data-vmark="7b39" style="text-align: center;"><img src="https://img.ithome.com/newsuploadfiles/2022/9/dbabb0d9-7895-425a-998e-57d18d976649.png" w="950" h="292" title="比标准 Attention 快 197 倍！Meta 推出多头注意力机制“九头蛇”" width="950" height="252" referrerpolicy="no-referrer"></p><p data-vmark="e00a">当然，最多可替换 8 层。</p><p data-vmark="34b6">另外，作者表示，这个方法应该可以扩展到 NLP 领域，不过他们还没试。</p><p data-vmark="059c" style="text-align: center;"><img src="https://img.ithome.com/newsuploadfiles/2022/9/183c40bc-3d62-46f0-9d8f-42b13fa0b003.png" w="1080" h="493" title="比标准 Attention 快 197 倍！Meta 推出多头注意力机制“九头蛇”" width="1080" height="374" referrerpolicy="no-referrer"></p><h2 data-vmark="bf3d">作者介绍</h2><p data-vmark="8349">这篇成果已入选 <strong>ECCV 2022 Workshop</strong>。</p><p data-vmark="f5be">作者一共 5 位，分别来自 Meta AI 和佐治亚理工学院。</p><p data-vmark="6f85" style="text-align: center;"><img src="https://img.ithome.com/newsuploadfiles/2022/9/3828f196-6f1d-40cd-9900-5be7de245241.png" w="1056" h="432" title="比标准 Attention 快 197 倍！Meta 推出多头注意力机制“九头蛇”" width="1056" height="335" referrerpolicy="no-referrer"></p><p data-vmark="3e0c">其中华人 3 名，分别是：</p><p data-vmark="d623">Cheng-Yang Fu，本硕毕业于清华大学，博士毕业于美国北卡罗来纳大学教堂山分校，现在是 Meta 计算机视觉方面的研究科学家。</p><p data-vmark="0aa7">Xiaoliang Dai，本科毕业于北大，博士毕业于普林斯顿大学，同就职于 Meta。</p><p data-vmark="d5e8">Peizhao Zhang，本硕毕业于中山大学，博士于美国德克萨斯 A&M 大学，已在 Meta 工作五年。</p><p data-vmark="dedb">论文地址：</p><p data-vmark="b72b"><span class="link-text-start-with-http">https://arxiv.org/abs/2209.07484</span></p><p data-vmark="ee7d"><span class="referenceTitle">参考</span></p><ul class="custom_reference list-paddingleft-1"><li class="list-undefined list-reference-paddingleft"><p data-vmark="aea4"><span class="link-text-start-with-http">https://www.reddit.com/r/MachineLearning/comments/xgqwvu/r_hydra_attention_efficient_attention_with_many/</span></p></li></ul>
          
</div>
            