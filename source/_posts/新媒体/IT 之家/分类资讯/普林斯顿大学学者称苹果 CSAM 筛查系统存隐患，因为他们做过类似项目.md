
---
title: '普林斯顿大学学者称苹果 CSAM 筛查系统存隐患，因为他们做过类似项目'
categories: 
 - 新媒体
 - IT 之家
 - 分类资讯
headimg: 'https://img.ithome.com/newsuploadfiles/2021/8/1c7fde53-58b1-41ce-acfa-76a31f59dc10.jpg'
author: IT 之家
comments: false
date: Mon, 23 Aug 2021 00:41:24 GMT
thumbnail: 'https://img.ithome.com/newsuploadfiles/2021/8/1c7fde53-58b1-41ce-acfa-76a31f59dc10.jpg'
---

<div>   
<p>8 月 23 日上午消息，近期普林斯顿大学的两位学者表示，他们认为苹果公司儿童性虐待照片（简称 CSAM）筛查系统具有一定危险性，因为他们曾构建过一个类似的系统。</p><p><img src="https://img.ithome.com/newsuploadfiles/2021/8/1c7fde53-58b1-41ce-acfa-76a31f59dc10.jpg" w="641" h="386" title="普林斯顿大学学者称苹果 CSAM 筛查系统存隐患，因为他们做过类似项目" width="641" height="386" referrerpolicy="no-referrer"></p><p>此前，<span class="accentTextColor">苹果公司出于保护儿童免受色情内容侵袭，宣布推出三项保护政策</span>。包括：机器筛查 Messages 信息，当孩子收到色情图片时，图片会变得模糊，并且 Messages 应用程序会显示一条警告；儿童性虐待内容（简称 CSAM）检测，该功能让苹果检测存储在 iCloud 云端照片中的这类图像特征信息，如果确定有此类图像，苹果会向执法部门报告；Siri 和搜索功能改进，人们可询问 Siri 如何报告儿童性虐待内容或性剥削，Siri 将告诉用户在哪可以汇报给有关部门。</p><p>普林斯顿大学这两位学者说，他们曾经的一个项目工作原理与 Apple 的方法完全相同，但他们很快发现了明显的问题。</p><p>助理教授 Jonathan Mayer 和研究生研究员 Anunay Kulshrestha 在《华盛顿邮报》的报道中提到，他们与苹果有着同样的善意目标：</p><p>“我们的研究项目始于两年前，作为一个实验系统，用于识别端到端加密在线服务中的 CSAM。作为安全研究人员，我们知道端到端加密的意义，它可以保护数据免受第三方访问。但我们也对 CSAM 在加密平台上激增感到震惊。而且我们担心，<span class="accentTextColor">在线服务不愿意在没有额外工具来对抗 CSAM 的情况下加密</span>。</p><p>我们试图探索一种可能的中间立场，即在线服务可以识别有害内容，同时保留端到端加密。这个概念很简单：如果有人分享了与已知有害内容数据库相匹配的内容，该服务就会收到警报。如果一个人分享了无关的内容，该系统就不会筛查到。人们无法阅读数据库或了解内容是否匹配，因为这些信息可以揭示执法方法并帮助犯罪分子逃避检测。”</p><p>在停止项目之前，他们已经做出一个工作原型：</p><p>经过多次错误的启动，我们构建了一个工作原型。但是我们遇到了一个明显的问题。</p><p>我们的系统可以很容易地重新用于监视和审查。筛查设计不限于特定类别的内容。例如，<span class="accentTextColor">外国政府可以强制要求提供服务，将那些分享不受欢迎的政治言论的人找出来。</span></p><p>两人表示，他们对苹果公司决定推出这项服务感到困惑，因为目前未能妥善解决他们提出的风险：和我们一样，Apple 的动机是保护儿童。它的系统在技术上比我们的更高效、更有能力。但是我们很发现，对于我们提出的难题，Apple 几乎没有答案。</p><p>这也是苹果这些动作引发争议的原因：虽然他们表示不会允许这种滥用，但政府可以依法强制公司这样做。</p>
          
</div>
            