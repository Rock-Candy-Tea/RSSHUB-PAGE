
---
title: '字节跳动最新文本生成图像 AI，训练集里居然没有一张带文字描述的图片？！'
categories: 
 - 新媒体
 - IT 之家
 - 分类资讯
headimg: 'https://img.ithome.com/newsuploadfiles/2022/3/a5f2538f-b006-41d3-8621-9f10c88ce3d4.jpg'
author: IT 之家
comments: false
date: Wed, 23 Mar 2022 13:36:35 GMT
thumbnail: 'https://img.ithome.com/newsuploadfiles/2022/3/a5f2538f-b006-41d3-8621-9f10c88ce3d4.jpg'
---

<div>   
<p data-vmark="1847">一个文本-图像对数据都不用，也能让 AI 学会看文作图？来自字节的最新 text2image 模型，就做到了。实验数据显示，它的效果比 VQGAN-CLIP 要真实，尤其是泛化能力还比不少用大量文本-图像数据对训练出来的模型要好很多。</p><p style="text-align: center;" data-vmark="8108"><img src="https://img.ithome.com/newsuploadfiles/2022/3/a5f2538f-b006-41d3-8621-9f10c88ce3d4.jpg" w="776" h="319" title="字节跳动最新文本生成图像 AI，训练集里居然没有一张带文字描述的图片？！" width="776" height="319" referrerpolicy="no-referrer"></p><p data-vmark="0a0f">嗯？不给文字注释 AI 怎么知道每一张图片代表什么？这个模型到底咋训练出来的？</p><p style="text-align: center;" data-vmark="e022"><img src="https://img.ithome.com/newsuploadfiles/2022/3/a01a2dbb-d2b6-47c6-85ea-be461a64a3d1.png" w="400" h="381" title="字节跳动最新文本生成图像 AI，训练集里居然没有一张带文字描述的图片？！" width="400" height="381" referrerpolicy="no-referrer"></p><h3 data-vmark="a445">不用文字训练也能根据文本生成图像</h3><p data-vmark="686f">首先，之所以选择这样一种方式，作者表示，是因为收集大量带文字的图像数据集的成本太高了。而一旦摆脱对文本-图像对数据的需求，我们就可以直接用大型无文本图像数据集 （比如 ImageNet）来训练强大且通用的 text2image 生成器。字节实现的这个模型叫做 CLIP-GEN，它具体是怎么操作的？</p><p data-vmark="2006">一共分三大步。</p><p data-vmark="f9da">首先，对于一幅没有文本标签的图像，使用 CLIP 的图像编码器，在语言-视觉（language-vision）联合嵌入空间（embedding space）中提取图像的 embedding。</p><p data-vmark="11b7">接着，将图像转换为 VQGAN 码本空间（codebook space）中的一系列离散标记（token）。也就是将图像以与自然语言相同的方式进行表示，方便后续使用 Transformer 进行处理。其中，充当 image tokenizer 角色的 VQGAN 模型，可以使用手里的无标记图像数据集进行训练。</p><p data-vmark="4161">最后，再训练一个自回归 Transformer，用它来将图像标记从 Transformer 的语言-视觉统一表示中映射出对应图像。经过这样的训练后，面对一串文本描述，Transformer 就可以根据从 CLIP 的文本编码器中提取的文本嵌入（text embedding）生成对应的图像标记（image tokens）了。</p><p style="text-align: center;" data-vmark="7815"><img src="https://img.ithome.com/newsuploadfiles/2022/3/8b9bc211-f151-40af-a91e-2421f18666d6.png" w="1080" h="651" title="字节跳动最新文本生成图像 AI，训练集里居然没有一张带文字描述的图片？！" width="1080" height="494" referrerpolicy="no-referrer"></p><p data-vmark="7ccb">那这样全程没有文本数据参与训练的文本-图像生成器，效果到底行不行？</p><h3 data-vmark="257f">性能与清华 CogView 相当</h3><p data-vmark="75d8">作者分别在 ImageNe 和 MSCOCO 数据集上对 CLIP-GEN 进行训练和评估。首先，用 MS-COCO 验证集中的六个文本描述生成样本。CLIP-GEN 和其他通过大量文本-图像对训练的 text2image 生成模型的效果对比如下：</p><p style="text-align: center;" data-vmark="ade9"><img src="https://img.ithome.com/newsuploadfiles/2022/3/dbd46b48-979c-4048-a335-966fb6d7ff58.png" w="1080" h="726" title="字节跳动最新文本生成图像 AI，训练集里居然没有一张带文字描述的图片？！" width="1080" height="551" referrerpolicy="no-referrer"></p><p data-vmark="9892">其中，VQGAN-CLIP 的结果比较不真实，并且伴随严重的形状扭曲。来自清华的 CogView 号称比 DALL-E 更优秀，在这里的实验中，它确实可以生成良好的图像结构，但在纹理细节上差点儿事儿。DF-GAN 可以生成具有丰富细节的合理图像，但也容易产生局部伪影。</p><p data-vmark="e09f">作者认为，与这些对比模型相比，CLIP-GEN 的图像细节更丰富，质量更高一些，比如它就很好地诠释了第二组文字中要求的“水中倒影”（不过不太能理解“三只毛绒熊“中的数字概念）。</p><p data-vmark="6c8b">定量实验结果基本证明了这一结论：</p><p data-vmark="03da">CLIP-GEN 拿到了最高的 FID-0、FID-1 分数；CapS 得分（衡量输入文本和生成图像之间的语义相似性）除了比 CogView 低 4%，比其他模型都高很多。</p><p style="text-align: center;" data-vmark="c63e"><img src="https://img.ithome.com/newsuploadfiles/2022/3/9a2e9759-725f-49a3-9657-f2f22e2f3a76.png" w="652" h="226" title="字节跳动最新文本生成图像 AI，训练集里居然没有一张带文字描述的图片？！" width="652" height="226" referrerpolicy="no-referrer"></p><p data-vmark="f36f">此外，作者还发现，CLIP-GEN 的泛化能力似乎也不错。在下面这组非常规的文字描述中，比如生成“一只会飞的企鹅”，“叼雪茄的狗”、“有脸和头发的柠檬”……CLIP-GEN 基本都可以实现，别的模型却不太能理解。</p><p style="text-align: center;" data-vmark="fa1a"><img src="https://img.ithome.com/newsuploadfiles/2022/3/9986fea3-039c-476f-bfb1-fc5f2bdac31b.png" w="1080" h="726" title="字节跳动最新文本生成图像 AI，训练集里居然没有一张带文字描述的图片？！" width="1080" height="551" referrerpolicy="no-referrer"></p><h3 data-vmark="4f51">作者介绍</h3><p data-vmark="502c">本模型的五位作者全部来自字节。</p><p style="text-align: center;" data-vmark="f5f2"><img src="https://img.ithome.com/newsuploadfiles/2022/3/884d3b64-8b11-4cd7-896d-294e664b7263.png" w="1080" h="200" title="字节跳动最新文本生成图像 AI，训练集里居然没有一张带文字描述的图片？！" width="1080" height="152" referrerpolicy="no-referrer"></p><p data-vmark="a296">一作 Wang Zihao 本科毕业于北京理工大学，博士毕业于 UC 伯克利，曾在谷歌担任 3 年软件开发工程师，现就职于 TikTok。</p><p data-vmark="48d0">通讯作者名叫易子立，本科毕业于南京大学，博士毕业于加拿大纽芬兰纪念大学，目前在字节担任人工智能专家（主要研究多模态、超分辨率、人脸特效），在此之前，他曾在华为工作。</p><p data-vmark="9077">论文地址：</p><p data-vmark="a425"><span class="link-text-start-with-http">https://arxiv.org/abs/2203.00386</span></p>
          
</div>
            