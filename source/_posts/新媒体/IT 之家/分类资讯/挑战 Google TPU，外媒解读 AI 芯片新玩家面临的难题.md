
---
title: '挑战 Google TPU，外媒解读 AI 芯片新玩家面临的难题'
categories: 
 - 新媒体
 - IT 之家
 - 分类资讯
headimg: 'https://img.ithome.com/newsuploadfiles/2021/8/0ea196d0-9f2a-471a-9b71-c693aae5ea9c.jpg'
author: IT 之家
comments: false
date: Sun, 08 Aug 2021 09:24:10 GMT
thumbnail: 'https://img.ithome.com/newsuploadfiles/2021/8/0ea196d0-9f2a-471a-9b71-c693aae5ea9c.jpg'
---

<div>   
<dir class="ithomerem"><p>距离 Google 第一代 TPU 助力 AlphaGo 打败李世石已有 5 年，Google TPU 已经更新到第四代。与此同时，数据中心领域的 AI 芯片大热，在大量投资下涌现了不少新玩家，尽管未来是光明的，但新玩家们不得不面对 Google 等大企业雄厚的财力和已经形成的市场格局。挑战 Google TPU，AI 芯片新玩家还会面临哪些难题？围绕这一话题，外媒作者 BRIAN BAILEY 进行了全面而深入的解读，雷锋网对本文进行了不改变原意的编译。</p></dir><p>近些年，大量资金涌入到数据中心领域新型 AI 处理器的研发中。</p><p>但在投资热潮的背后，也要注意到问题所在。毕竟，该领域需要处理的问题是特定的，结果不可预测；且该领域的竞争者们财力雄厚（因为它们往往是巨头），能够提供用户黏性非常强的产品。</p><p>对于新型 AI 芯片设计公司而言，最大的问题在于：来自终端的数据不足。</p><p><img src="https://img.ithome.com/newsuploadfiles/2021/8/0ea196d0-9f2a-471a-9b71-c693aae5ea9c.jpg" w="740" h="360" alt="挑战 Google TPU，AI 芯片新玩家面临哪些难题？" title="挑战 Google TPU，外媒解读 AI 芯片新玩家面临的难题" width="740" height="360" referrerpolicy="no-referrer"></p><h2>需要多少个数据中心，才能实现盈利？</h2><p>通常，芯片设计公司在设计一个新的 AI 处理器时，首先会弄清楚一个基本问题 —— 如何定义产品的灵活性？是专为单一任务而设计？还是支持更多的工作负载？</p><p>这两个问题之间存在一系列解决方案，但与过去的许多解决方案相比，为 AI 处理器找到合适的解决方案更加困难，对数据中心工作负载而言尤其如此。</p><p>之所以更加困难，是因为有许多因素需要平衡。“既需要在一定的成本和时间内设计和制造芯片，又要考虑成本和回报问题。”Synopsys 人工智能产品和研发总监 Stelios Diamantidis 解释道。这些限制因素缩小了 AI 处理器的潜在市场。</p><p>“设计和制造定制芯片，什么时候才能赚钱？”Synopsys 验证组工程副总裁 Susheel Tadikonda 说。</p><p>“如果我们要为数据中心提供定制芯片，那么需要多少个数据中心才能实现盈利？也许可以高价出售芯片，但光是这样远远不够。如果是为消费电子设备设计和制造芯片，那么这一领域存在十亿台设备体量，这也是 AISC 芯片能够赚取更多利润的市场之一，当然设备体量越大越好。”</p><p>不过就算最终弄清楚多少个数据中心才能实现盈利，也无法确定设计方案。</p><p>“芯片定制化程度越来越高，以至于能够为非常特定的算法创建芯片，提供更高的能效和性能，”西门子 EDA 战略和业务发展高级经理 Anoop Saha 说：“但这会牺牲一部分市场，也会缩短芯片的寿命。如果两年后出现了一个新算法，那为旧算法定制的芯片的价值还会如初吗？很多事情都会互相牵制。“</p><p>“一些边缘算法确实已经趋于稳定。这是因为业界经过多年研究，找到一些多场景适用的最佳算法，例如我们已经看到的神经卷积网络算法 CNN（convolutional neural network），还有对于唤醒词检测、手写识别等特定应用找到的最佳算法。”Anoop Saha 补充道。</p><h2>芯片自定义的优势</h2><p>要对芯片进行自定义，核心是理解面向何种工作负载 —— 芯片自定义的确为许多玩家带来优势。</p><p>Xilinx 人工智能和软件产品营销总监 Nick Ni 表示：“大多数大型企业已经组建了自己的芯片部门，并为其数据中心一些高工作负载打造芯片。例如，如果 Google 将‘推荐’型神经网络视为其数据中心中最高的工作负载之一，那么它就很有必要为此创建专用芯片。如果排名第二的工作负载是语音识别，排名第三的是 YouTube 的视频转码，那么为其打造专用芯片都是有意义的。“</p><p>“其实这里的机会很多，但 Google 只是一个孤例。几年前，谷歌发表了一篇广受好评的论文，文章陈述了一个事实 —— 数据中心的工作负载类型非常丰富，但没有一种工作负载占比超过 10%，这意味着还有大量占比微小的工作负载需要优化。”</p><p>“大多数定制都是面向推理的，当这些定制芯片转向训练时，就需要浮点支持，”Synopsys 的 Diamantidis 表示。</p><p>“但是如果需要的是一个 100% 应用于推理的解决方案，那么它的定点（ fixed point）位数可能是八位甚至更低的精度。如果模型是固定的，那么在推理基础设备之上进行定制是否有意义？例如，针对语音、视频以及其他重量级应用程序的定制解决方案。大企业们（Hyperscaler）实际上正在投资应用于推理的芯片解决方案，这些推理适用于它们自身在 AI 领域的定制化高级模型和解决方案 —— 但如果是需要处理多种应用，那么就需要更多的灵活性和可定制性。”</p><p>当然，对 Google 而言，这已经是一个良性循环。“TPU 旨在满足 Google 数据中心内的特定工作负载，”Synopsys 的 Tadikonda 说。“Google 最开始打造 TPU，是因为意识到处理如此庞大复杂的数据和计算，需要建立起大量的数据中心。"</p><p>“第一代 TPU 体积大且非常耗电，但它已经通过不断地学习而得到了改进，这正是这些 TPU 的工作，这就是 Google 。”</p><p>并非每家公司都能够使用 Google 的反馈循环，不过其他公司也确有其他选择。“我们发现，其中一个关键点是尽可能早得关注和重视选择正确的架构，”西门子的 Saha 说。“所谓正确，并不是指某一个人认为正确，也不是基于过去的经验，凭直觉做出的决定，因为现在还有太多的未知数。业界正在做的，是在设计周期早期，依靠数据驱动做出决定，这样我们能够在发现某些东西不起作用时迅速做出改变。”</p><p>这些决定是宏观的，也可以是微观的。“比如说，你的存储元素与计算元素差距有多大？”Saha 问道：“再比如，多久执行一次内存读取，这是一个重要的问题，因为读取和写入将直接影响整体的能效。业界正在寻找新的架构，没有人知道什么样的架构才真正起作用。不过可以确定的是，要有一定的可塑性，且在决定架构之前，能够确保有足够的市场数据来支撑。”</p><h2>硬件和算法迭代快</h2><p>影响架构选择的另一个因素是硬件和算法的发展速度。这决定了数据中心所有者从他们购买的硬件中赚钱的时间，也决定了他们愿意支付的价格，同时限定了芯片开发的总成本。</p><p>那么，数据中心芯片（即硬件）的使用寿命是多久？</p><p>“通常情况下，芯片或电路板的寿命为三到四年，”Xilinx 的 Ni 说。“一些较为激进的数据中心可能会在这一时间段内升级，还有一些则会持续使用更长时间。在人工智能领域，如果我们关注 Google TPU 的发布新闻，就能发现在过去六年左右的时间里，Google 发行了四个版本的 TPU，也就是说，几乎每隔一两年 Google 就会更换一次内部硬件，针对 AI 等快速变化的工作负载进行优化。”</p><p>换个角度来看，AI 芯片公司可能每 18 个月就有一次进入数据中心的机会。“要搅动这个市场并不容易，”Saha 说。“有两个重要因素 —— 更换现有数据中心芯片的频率，以及添加新东西的频率。我看见几乎所有的数据中心都在尝试更新的东西，几乎每个构建数据中心芯片的公司都在同一些终端客户合作。“</p><p>“市场多久更换一次正在工作的芯片？只要芯片在工作，公司就会尽量延长芯片的使用寿命。一旦芯片进入数据中心，持续的时间会很长且难以更换。这就是为什么我们可以看到大型数据中心芯片领域有大量投资。”</p><p>“一部分投资者认为这是赢家通吃的市场，最终会有一至三个获胜者获得最大的市场份额。一旦市场被这些公司占领，这些公司的地位就很难被取代。”</p><h2>设计面向 18 个月后的芯片</h2><p>如果从今天开始设计芯片，那么这颗芯片必须满足 18 个月后需要满足的条件。</p><p>“当我们决定对芯片进行模块化时，我们还必须针对特定精度进行优化，”Xilinx 的 Ni 说。“例如，当我们选择在 8 位数精度上做文章时，我们不得不立下赌约，当这款产品成为主流时，8 位仍然是主流。</p><p>“我们还要确保制造出的产品可以处理混合精度网络，其中一半是 8 位，四分之一是 4 位，另外四分之一是 1 位。为此，我们在 AI 引擎中执行 8 位，其运行基本性能非常快，然后可以在 FPGA 架构中实现 4 位和 1 位 MAC 单元。”</p><p>设计时间和算法进化的时间要保持一致。“在 18 个月内，应用程序很可能会变得相当不同，”Tadikonda  警告说。“我认为今天的数据科学家不会向任何人保证他们将在未来 18 个月内运行与今天相同的模型。”</p><p>还有其他一系列决策也需要作出。</p><p>“量化可能是许多能效指标中的最大因素，”Saha 说。“量化将对推理产生更大的影响，推理分散在数据中心和边缘之间，但在‘学习’端也需要一些量化。当我们量化成较低的位数时，就意味着我们正在权衡能效而不是准确性。训练可能需要浮点数，不过有一些新型浮点数出现。谷歌在设计下一代 TPU 时，他们创造了 Bfloat16，这是用于训练的“大脑浮点数”。它与 IEEE 浮点数非常不同，它在精度上具有浮点数的优势，但也具有显着的能效优势。”</p><p>不过这将让经济因素陷入困境。</p><p>“对于如此规模的 ASIC，需要在快速变化节奏里付出巨大的努力，只有少数公司能够保证其经济性，”Tadikonda 说。“因为有关这些数据的用例正在增加，所以算法正在发生变化。我们今天认为有效的算法明天不一定有效，想要跟上节奏并处于最前沿，就必须不断创新或重新研发 ASIC。谷歌占据优势，因为它拥有足够多的数据以至于能快速搅动局面，它从自己的 TPU 中学到了很多东西，知道为了保证程序运行地更好需要作出哪些改变。”</p><p>“如果我是第三方芯片开发商，我没有这些数据，就只能依靠我的客户来提供，因此周转周期会更长。谷歌的情况非常特殊。”</p><p>数据的缺乏也给验证带来压力。“浮点硬件的验证对满足这些芯片的性能和功耗要求至关重要，”OneSpin 市场营销主管 Rob Van Blommestein 说。</p><p>“长期以来，浮点硬件设计的验证一直被认为是一项重大挑战。FPU（floating-point unit）将浮点运算的数学复杂性与需要复杂控制路径的各种特殊情况相结合。我们需要一种正式的验证解决方案，以验证由硬件浮点单元 (FPU) 计算出的算术运算结果是否与 IEEE 754 标准规范准确匹配。”</p><h2>结论</h2><p>人们常说，数据是新的石油，这一比喻在人工智能领域得以明显体现。</p><p>对于芯片架构师来说，这个比喻再恰切不过。他们需要访问数据来改进构建更好的产品，这也是数据中心处理器用户黏性高的原因；架构师们一旦拥有数据中心处理器，就有机会获得需要的数据。</p><p>另一个唯一可行的办法是加快设计速度来提升效率，推动产品成本的回收 —— 但具有讽刺意味的是，在试图通过提升效率来解决问题的时候，AI 却成为了唯一的阻碍者。</p><p>毕竟，AI 领域的算法进化速度和变数实在是太大了。</p>
          
</div>
            