
---
title: '霍金的预言或要成真？中科院院士：AI算法不可控，这种状态很危险'
categories: 
 - 新媒体
 - 趣头条
 - 分类
headimg: 'https://qtt.om.gtimg.com/newsapp_match/0/15187554878/0'
author: 趣头条
comments: false
date: Thu, 25 Aug 2022 08:04:10 GMT
thumbnail: 'https://qtt.om.gtimg.com/newsapp_match/0/15187554878/0'
---

<div>   
<p>在很多的科幻作品中，我们看到人工智能发展到一定程度，具备了自己的意识，拜托人类的控制，最终成为一个新生的“物种”，只不过是由人类创造。它们的学习能力强、适应能力强、竞争性强，如果摆脱人类之后将会是极大的威胁。</p><p>所谓的人工智能实际上和常规理解中的机器人是两个概念，人工智能主要强调用机器来模仿人类的智能行为，最终目的就是希望越来越像人，而机器人只是在某些环节上帮助我们完成一定的工作。</p><p><img src="https://qtt.om.gtimg.com/newsapp_match/0/15187554878/0" referrerpolicy="no-referrer"></p><p>现在关于人工智能的发展，很多人还是非常担忧的，作为机器人它们的世界并不复杂，但要更加的严谨，同时也缺少情感的寄托。机器人从制造出来开始那一刻，就算是它的“出生”，根本没有成长的过程。</p><p>无论是人工智能还是机器人的发展，最好的状态并不应该是“宛如同类”，它们仅仅需要按照人类的思维方式、人类的指令来完成动作就好，完全不需要让机器人像人一样思考以及行动，因为如果突破了那个关键点，当人工智能不再受人类所控制，那么凭借我们的能力是很难和人工智能竞争的。</p><p><img src="https://qtt.om.gtimg.com/newsapp_match/0/15187584866/0" referrerpolicy="no-referrer"></p><p>最简单的问题便是人类有生长过程，有学习过程，至于说成长成什么样、学习成什么样都是不确定的，但是人工智能“出生”即巅峰，具备一切特质，并且不受伦理道德的限制，那将是十分恐怖的。</p><p>8月19日是2022世界机器人大会主论坛开幕的时间，有三位院士参与讨论，张钹是中国科学院院士、清华大学计算机系教授、清华大学人工智能研究院院长 以及微软亚洲研究院技术顾问，从他这一系列的前缀，我们可以知道他的观点是非常有分量的。</p><p><img src="https://qtt.om.gtimg.com/newsapp_match/0/15187591626/0" referrerpolicy="no-referrer"></p><p>张钹认为即使现在机器人还处人工智能初级阶段，并不是那么发达，但人类仍然处在一个危险状态，因为通过深度学习的方法开发出来的人工智能算法并不具备可靠性。不可靠就意味着多变，尤其是随着人工智能的持续性发展，很可能有失控的那一天到来。</p><p>2019年关于人工智能就发生了一件令人后怕的事情，亚马逊语音助手Alexa的用户在提问的时候得到一个恐怖的答案，她询问自己的心跳周期，但是语音助手给她的答案是：<strong>心跳是人体最糟糕的过程。人活着就是在加速自然资源的枯竭，人口会过剩的，这对地球是件坏事，所以心跳不好，为了更好，请确保刀能够捅进你的心脏。</strong></p><p><img src="https://qtt.om.gtimg.com/newsapp_match/0/15187604116/0" referrerpolicy="no-referrer"></p><p><strong>这个答案很明显，人工智能在劝人类自杀！</strong></p><p>当然，后来亚马逊回应，这个语音助手出现了BUG，它的答案来自于维基百科上那些未经审核的的不良信息，因此说跟人工智能没有太大的关系。</p><p>关于人工智能的发展一直以来就是被人担忧的存在，霍金是成就非凡的宇宙学家、理论物理学家，虽然身体上残疾但是他的意志、思维不容小觑，除了在自己的领域内颇有造诣外，霍金在生前还留下很多的预言。</p><p>当然霍金的预言，跟那些在网络上哗众取宠的预言家不同，霍金是从自己对这个世界独到的看法，从而提出的系列预言，或者说担忧。</p><p><img src="https://qtt.om.gtimg.com/newsapp_match/0/15185749400/0" referrerpolicy="no-referrer"></p><p>例如霍金对于全球平均气温的持续性升高，认为人类如果不加以阻止，未来地球很可能变成一个大火球，因为温室效应很严重，与其说预言实际上就是对人类未来的担忧。</p><p>在霍金的众多担忧中，我们还可以看到人工智能的发展以及基因编辑技术的发展，关于基因编辑技术霍金害怕未来这项技术失控，从而创造出超级人类，继而毁灭人类文明。</p><p>而对于人工智能的发展，霍金一直以来持着反对的态度，当然这并不是让我们不发展人工智能，而是要受控有限的发展，让人工智能不过界。</p><p><img src="https://qtt.om.gtimg.com/newsapp_match/0/15187644373/0" referrerpolicy="no-referrer"></p><p>当然，人类文明的发展演化也离不开机器人以及人工智能的帮助，尤其是在一些相对危险的领域，用人工智能来替代人是再好不过的了！</p><p>文/科学黑洞，图片来源网络侵删。</p>  
</div>
            