
---
title: '1块显卡＋几行代码：大模型训练提速40%！'
categories: 
 - 新媒体
 - 快科技（原驱动之家）
 - 最新新闻
headimg: 'https://img1.mydrivers.com/img/20220713/S55718bfa-21b7-48cc-8847-7939335d1e30.png'
author: 快科技（原驱动之家）
comments: false
date: Wed, 13 Jul 2022 15:36:08 GMT
thumbnail: 'https://img1.mydrivers.com/img/20220713/S55718bfa-21b7-48cc-8847-7939335d1e30.png'
---

<div>   
<p>不得不说，为了让更多人能用上大模型，技术圈真是各出奇招！</p>
<p>模型不够开放？有人自己上手搞免费开源版。</p>
<p>比如最近风靡全网的DALL·E Mini，Meta开放的OPT-175B（Open Pretrained Transformer）。</p>
<p>都是通过复刻的方式，让原本不够open的大模型，变成人人可用。</p>
<p style="text-align: center"><a href="https://img1.mydrivers.com/img/20220713/55718bfa-21b7-48cc-8847-7939335d1e30.png" target="_blank"><img alt="1块显卡＋几行代码：大模型训练提速40%！" h="123" src="https://img1.mydrivers.com/img/20220713/S55718bfa-21b7-48cc-8847-7939335d1e30.png" style="border: black 1px solid" w="600" referrerpolicy="no-referrer"></a></p>
<p>还有人觉得模型太大，个人玩家很难承受起天价成本。</p>
<p>所以提出异构内存、并行计算等方法，让大模型训练加速又降本。</p>
<p><strong>比如开源项目Colossal-AI，前不久刚实现了让一块NVIDIA RTX 3090就能单挑180亿参数大模型。</strong></p>
<p style="text-align: center"><a href="https://img1.mydrivers.com/img/20220713/bbce5189-ef50-4ec5-ad03-79fc47054ce9.png" target="_blank"><img alt="1块显卡＋几行代码：大模型训练提速40%！" h="337" src="https://img1.mydrivers.com/img/20220713/Sbbce5189-ef50-4ec5-ad03-79fc47054ce9.png" style="border: black 1px solid" w="600" referrerpolicy="no-referrer"></a></p>
<p>而在这两天，他们又来了一波上新：</p>
<p><strong>无缝支持Hugging Face社区模型，只需添加几行代码，就能实现大模型的低成本训练和微调。</strong></p>
<p style="text-align: center"><a href="https://img1.mydrivers.com/img/20220713/c5abf01d-7f11-4492-a858-cc2fc6016816.png" target="_blank"><img alt="1块显卡＋几行代码：大模型训练提速40%！" h="246" src="https://img1.mydrivers.com/img/20220713/Sc5abf01d-7f11-4492-a858-cc2fc6016816.png" style="border: black 1px solid" w="600" referrerpolicy="no-referrer"></a></p>
<p>要知道，Hugging Face作为当下最流行的AI库之一，提供了超过5万个AI模型的实现，是许多AI玩家训练大模型的首选。</p>
<p>而Colossal-AI这波操作，是让公开模型的训练微调变得更加切实可行。</p>
<p>并且在训练效果上也有提升。</p>
<p><strong><span style="color:#ff0000;">单张GPU上，相比于微软的DeepSpeed，使用Colossal-AI的自动优化策略，最快能实现40%的加速。</span></strong></p>
<p>而PyTorch等传统深度学习框架，在单张GPU上已经无法运行如此大的模型。</p>
<p>对于使用8张GPU的并行训练，仅需在启动命令中添加-nprocs 8就能实现。</p>
<p style="text-align: center"><a href="https://img1.mydrivers.com/img/20220713/98e4373c-3d6b-4bf6-b5e0-238c7b4550d0.jpg" target="_blank"><img alt="1块显卡＋几行代码：大模型训练提速40%！" h="161" src="https://img1.mydrivers.com/img/20220713/S98e4373c-3d6b-4bf6-b5e0-238c7b4550d0.jpg" style="border: black 1px solid" w="600" referrerpolicy="no-referrer"></a></p>
<p>这波下来，可以说是把个人AI玩家需要考虑的成本、效率、实操问题，都拿捏住了~</p>
<p><strong>无需修改代码逻辑</strong></p>
<p>光说不练假把式。</p>
<p>下面就以OPT为例，详细展开看看Colossal-AI的新功能到底怎么用。</p>
<p>OPT，全称为Open Pretrained Transformer。</p>
<p>它由Meta AI发布，对标GPT-3，最大参数量可达1750亿。</p>
<p>最大特点就是，GPT-3没有公开模型权重，而OPT开源了所有代码及权重。</p>
<p>因此，每一位开发者都能在此基础上开发个性化的下游任务。</p>
<p>下面的举例，就是根据OPT提供的预训练权重，进行因果语言模型（Casual Language Modelling）的微调。</p>
<p>主要分为两个步骤：</p>
<p>1、添加配置文件</p>
<p>2、运行启动</p>
<p><strong>第一步，是根据想进行的任务添加配置文件。</strong></p>
<p>比如在一张GPU上，以异构训练为例，只需在配置文件里加上相关配置项，并不需要更改代码的训练逻辑。</p>
<p>比如，tensor_placement_policy决定了异构训练的策略，参数可以为CUDA、CPU及auto。</p>
<p>每个策略的优点不同、适应的情况也不一样。</p>
<p>CUDA：将全部模型参数都放置于GPU上，适合不offload时仍然能进行训练的传统场景。</p>
<p>CPU：将模型参数都放置在CPU内存中，仅在GPU显存中保留当前参与计算的权重，适合超大模型的训练。</p>
<p>auto：根据实时的内存信息，自动决定保留在GPU显存中的参数量，这样能最大化利用GPU显存，同时减少CPU-GPU之间的数据传输。</p>
<p>对于普通用户来说，使用auto策略是最便捷的。</p>
<p>这样可以由Colossal-AI自动化地实时动态选择最佳异构策略，最大化计算效率。</p>

<p><em>from colossalai.zero.shard_utils import TensorShardStrategy</em></p>
<p><em>zero = dict(model_config=dict(shard_strategy=TensorShardStrategy(),</em></p>
<p><em>                              tensor_placement_policy="auto"),</em></p>
<p><em>            optimizer_config=dict(gpu_margin_mem_ratio=0.8))</em></p>
<p><strong>第二步，是在配置文件准备好后，插入几行代码来启动新功能。</strong></p>
<p>首先，通过一行代码，使用配置文件来启动Colossal-AI。</p>
<p>Colossal-AI会自动初始化分布式环境，读取相关配置，然后将配置里的功能自动注入到模型及优化器等组件中。</p>
<p><em>colossalai.launch_from_torch(config='./configs/colossalai_zero.py')</em></p>
<p>然后，还是像往常一样定义数据集、模型、优化器、损失函数等。</p>
<p>比如直接使用原生PyTorch代码，在定义模型时，只需将模型放置于ZeroInitContext下初始化即可。</p>
<p>在这里，使用的是Hugging Face提供的OPTForCausalLM模型以及预训练权重，在Wikitext数据集上进行微调。</p>

<p><em>with ZeroInitContext(target_device=torch.cuda.current_device(), </em></p>
<p><em>                    shard_strategy=shard_strategy,</em></p>
<p><em>                    shard_param=True):</em></p>
<p><em>    model = OPTForCausalLM.from_pretrained(</em></p>
<p><em>                'facebook/opt-1.3b'</em></p>
<p><em>                config=config</em></p>
<p>接下来，只需要调用colossalai.initialize，便可将配置文件里定义的异构内存功能统一注入到训练引擎中，即可启动相应功能。</p>
<p><em>engine, train_dataloader, eval_dataloader, lr_scheduler = colossalai.initialize(model=model,</em></p>
<p><em>                                                                               optimizer=optimizer,</em></p>
<p><em>                                                                               criterion=criterion,</em></p>
<p><em>                                                                               train_dataloader=train_dataloader,</em></p>
<p><em>                                                                               test_dataloader=eval_dataloader,</em></p>
<p><em>                                                                               lr_scheduler=lr_scheduler)</em></p>
<p><strong>还是得靠GPU+CPU异构</strong></p>
<p>而能够让用户实现如上“傻瓜式”操作的关键，还是AI系统本身要足够聪明。</p>
<p>发挥核心作用的是Colossal-AI系统的高效异构内存管理子系统Gemini。</p>
<p>它就像是系统内的一个总管，在收集好计算所需的信息后，动态分配CPU、GPU的内存使用。</p>
<p>具体工作原理，就是在前面几个step进行预热，收集PyTorch动态计算图中的内存消耗信息。</p>
<p>在预热结束后，计算一个算子前，利用收集的内存使用记录，Gemini将预留出这个算子在计算设备上所需的峰值内存，并同时从GPU显存移动一些模型张量到CPU内存。</p>
<p style="text-align: center"><a href="https://img1.mydrivers.com/img/20220713/5c0159bc-6d93-4147-89ea-394b3cca633b.png" target="_blank"><img alt="1块显卡＋几行代码：大模型训练提速40%！" h="232" src="https://img1.mydrivers.com/img/20220713/S5c0159bc-6d93-4147-89ea-394b3cca633b.png" style="border: black 1px solid" w="600" referrerpolicy="no-referrer"></a></p>
<p>Gemini内置的内存管理器给每个张量都标记一个状态信息，包括HOLD、COMPUTE、FREE等。</p>
<p>然后，根据动态查询到的内存使用情况，不断动态转换张量状态、调整张量位置。</p>
<p>带来的直接好处，就是能在硬件非常有限的情况下，最大化模型容量和平衡训练速度。</p>
<p>要知道，业界主流方法ZeRO （Zero Reduency Optimizer），尽管也利用CPU+GPU异构内存的方法，但是由于是静态划分，还是会引起系统崩溃、不必要通信量等问题。</p>
<p>而且，使用动态异构CPU+GPU内存的办法，还能用加内存条的办法来扩充内存。</p>
<p>怎么也比买高端显卡划算多了。</p>
<p style="text-align: center"><img alt="1块显卡＋几行代码：大模型训练提速40%！" h="238" src="https://img1.mydrivers.com/img/20220713/f3718912-7ada-473f-911b-34b09bf36b55.png" style="border: black 1px solid" w="238" referrerpolicy="no-referrer"></p>
<p>目前，使用Colossal-AI的方法，RTX 2060 6GB普通游戏本能训练15亿参数模型；RTX 3090 24GB主机直接单挑180亿参数大模型；Tesla V100 32GB连240亿参数都能拿下。</p>
<p>除了最大化利用内存外，Colossal-AI还使用分布式并行的方法，让训练速度不断提升。</p>
<p>它提出同时使用数据并行、流水并行、2.5维张量并行等复杂并行策略。</p>
<p>方法虽复杂，但上手却还是非常“傻瓜操作”，只需简单声明，就能自动实现。</p>
<p>无需像其他系统和框架侵入代码，手动处理复杂的底层逻辑。</p>
<p><em>parallel = dict(</em></p>
<p><em>pipeline=2,</em></p>
<p><em>tensor=dict(mode='2.5d', depth = 1, size=4)</em></p>
<p><em>)</em></p>
<p><strong>Colossal-AI还能做什么？</strong></p>
<p>实际上，自开源以来，Colossal-AI已经多次在GitHub及Papers With Code热榜位列世界第一，在技术圈小有名气。</p>
<p>除了如上提到的用单张GPU训练大模型外，Colossal-AI在扩展至数十张甚至数百张GPU的大规模并行场景时，相比于英伟达Megatron-LM等现有系统，性能可以翻倍，使用资源可以降低至其十分之一之下。</p>
<p>换算一下，在预训练GPT-3等超大AI模型上，节省的费用可以达到数百万元。</p>
<p style="text-align: center"><a href="https://img1.mydrivers.com/img/20220713/33b091ab-fab8-4c9a-9172-d6b9d7ba95af.png" target="_blank"><img alt="1块显卡＋几行代码：大模型训练提速40%！" h="166" src="https://img1.mydrivers.com/img/20220713/S33b091ab-fab8-4c9a-9172-d6b9d7ba95af.png" style="border: black 1px solid" w="600" referrerpolicy="no-referrer"></a></p>
<p>据透露，Colossal-AI相关的解决方案已经被自动驾驶、云计算、零售、医药、芯片等行业的知名厂商用上了。</p>
<p>与此同时，他们也非常注重开源社区建设，提供中文教程、开放用户社群论坛，根据大家的需求反馈不断更新迭代。</p>
<p>比如我们发现，之前有粉丝留言询问，Colossal-AI能否直接加载Hugging Face上的一些模型？</p>
<p>好嘛，这次更新就来了。</p>
<p style="text-align: center"><a href="https://img1.mydrivers.com/img/20220713/261d8af7-0d79-43f9-bf42-8e96aaedc562.png" target="_blank"><img alt="1块显卡＋几行代码：大模型训练提速40%！" h="80" src="https://img1.mydrivers.com/img/20220713/S261d8af7-0d79-43f9-bf42-8e96aaedc562.png" style="border: black 1px solid" w="600" referrerpolicy="no-referrer"></a></p>
<p>所以，对于大模型训练，你觉得现在还有哪些难点亟需解决呢？</p>

           
           
           <p class="end"><img src="https://icons.mydrivers.com/news/end_article.png" referrerpolicy="no-referrer"></p>
<div style="overflow: hidden;font-size:14px;">
             
          <p class="url"><span style="color:#666">责任编辑：上方文Q</span><a href="javascript:;" class="jiucuo" id="leftjiucuo">文章纠错</a></p>
        </div>
        <div class="page_article" id="bnext">
 
</div>
<p class="bqian">话题标签：<a href="https://news.mydrivers.com/tag/xianka.htm">显卡</a><a href="https://news.mydrivers.com/tag/rengongzhineng.htm">人工智能</a><a href="https://news.mydrivers.com/tag/xunlian.htm">训练</a><a href="https://news.mydrivers.com/tag/daima.htm">代码</a>  </p>
        
</div>
            