
---
title: '麻省理工为高性能计算机开发新的编程语言'
categories: 
 - 新媒体
 - cnBeta
 - 最新
headimg: 'https://static.cnbetacdn.com/article/2022/0215/cd39a230a90dd15.png'
author: cnBeta
comments: false
date: Tue, 15 Feb 2022 11:37:20 GMT
thumbnail: 'https://static.cnbetacdn.com/article/2022/0215/cd39a230a90dd15.png'
---

<div>   
在上月于费城举办的编程语言原理大会上，麻省理工学院（MIT）计算机科学与人工智能实验室（CSAIL）二年级博士生 <strong>Amanda Liu 表示，使用他们专为高性能计算而设计的新编程语言，可以很好地兼顾速度与正确性。</strong>此前人们普遍认为，速度与可靠性存在不可避免的权衡。<br>
 <p><img src="https://static.cnbetacdn.com/article/2022/0215/cd39a230a90dd15.png" referrerpolicy="no-referrer"></p><p>据悉，Liu 与加州大学伯克利分校博士后 Gilbert Louis Bernstein、MIT 副教授 Adam Chlipala 和助理教授 Jonathan Ragan-Kelley 一道，描述了他们最近开发的“张量语言”（A Tensor Language）。</p><blockquote><p>ATL 语言旨在产生一个数字或张量，所谓张量就向向量和矩阵的泛化。</p><p>向量是一维对象（通常由单独的箭头表示），矩阵是相对脸熟的二维数字数组。</p><p>而张量是 n 维数组，例如可用 3×3×3 的数组形式、或更高 / 更低的维度。</p></blockquote><p style="text-align: center;"><iframe src="//tv.sohu.com/s/sohuplayer/iplay.html?bid=326223087&autoplay=false&disablePlaylist=true" width="640" height="480" frameborder="0"></iframe></p><p style="text-align: center;">a verified framework for optimizing tensor programs（<a href="https://tv.sohu.com/v/dXMvODIyMjQwNTMvMzI2MjIzMDg3LnNodG1s.html" target="_self">via</a>）</p><p>计算机算法或程序的全部意义，在于启动特定的计算。不过想要实现目的，可用诸多不同的方式来编写。正如该研究团队在即将发表的会议论文中所写的那样：</p><blockquote><p>各种不同的代码实现方式让人眼花缭乱，某些方案的速度要快得多。</p><p>但鉴于高性能计算的资源开销极其夸张，ATL 希望用更高效的方式来修改或重写程序。</p><p>普通开发者习惯从最容易着手的地方开始编程，但这显然没有考虑到最佳的运行效率，因而需要进一步调整优化。</p></blockquote><p><img src="https://static.cnbetacdn.com/article/2022/0215/d174b59e061b02e.jpg" alt="2.jpg" referrerpolicy="no-referrer"></p><p>假设图像由 100×100 的数字数组表示，每个数字对应一个像素，且希望获得这些数字的均值。</p><p>这项工作可通过两阶计算完成，首先确定每行的平均值，然后获取每列的平均值。</p><p>ATL 提供了一个相关的工具包 —— 计算机科学家称之为“框架”—— 能够展示如何将这两个步骤转换为更快的一步过程。</p><p><a href="https://static.cnbetacdn.com/article/2022/0215/effee9c2d5713e8.jpg" target="_blank"><img src="https://static.cnbetacdn.com/thumb/article/2022/0215/effee9c2d5713e8.jpg" alt="3.jpg" referrerpolicy="no-referrer"></a></p><p>Liu 补充道：我们可借助所谓的“证明助手”（proof assistant），来确保这种优化的正确性。</p><p>有鉴于此，团队在现有的 Coq 语言的基础上构建了新语言。而其中包含的证明助手，具有以数学严谨的方式证明其断言的内在能力。</p><p>不过在 MIT 团队看来，Coq 有另一个值得称道的内在特性 —— 用它编写或适配的程序，是无法在无限循环中无止境地运行的。</p><p><a href="https://static.cnbetacdn.com/article/2022/0215/afa5b01e1c2ddce.jpg" target="_blank"><img src="https://static.cnbetacdn.com/thumb/article/2022/0215/afa5b01e1c2ddce.jpg" alt="4.jpg" referrerpolicy="no-referrer"></a></p><p>举个例子，用 Java 编写的程序，可能会发生这种状况。我们运行一个程序来得到一个单一的答案 —— 一个数字、或一个张量。</p><p>一个永不终止的程序，对我们说来毫无用处，但终止（terminate）是我们可使用 Coq 免费获得的一项特性。</p><p>只得一提的是，ATL 项目结合了 Ragan-Kelley 和 Chlipala 两项研究的成果，前者长期持续关注着高性能计算背景下的算法优化。</p><p>与此同时，Chlipala 更关注算法优化的形式化（例如基于数学的验证），但 ATL 是两者都首次合作 —— Bernstein 和 Liu 与去年携手，并产出了 ATL 这个成果。</p><p><a href="https://static.cnbetacdn.com/article/2022/0215/03e6b491b2a066c.jpg" target="_blank"><img src="https://static.cnbetacdn.com/thumb/article/2022/0215/03e6b491b2a066c.jpg" alt="5.jpg" referrerpolicy="no-referrer"></a></p><p>据悉，ATL 是首个、也是迄今唯一一个具有正式验证优化的张量语言。目前 ATL 仍处于原型阶段，但研究团队已在许多小程序上展开了测试，可知其具有相当光明的前景。</p><p>展望未来，他们的主要目标之一是提升 ATL 的可扩展性，以便它能够用于我们在现实世界中看到的更大型的程序。</p><p>此前这些程序的优化工作，通常需要人工来完成。除了总有临时需要解决的问题、还总涉及反复实验，因而难免发生大量的错误。</p><p>好消息是，借助 ATL，我们有望遵循一种更具原则的方法来重写这些程序 —— 且这么做更加容易，也更能保证程序的正确性。</p>   
</div>
            