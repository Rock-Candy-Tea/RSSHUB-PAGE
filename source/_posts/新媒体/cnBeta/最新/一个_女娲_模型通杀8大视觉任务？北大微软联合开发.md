
---
title: '一个_女娲_模型通杀8大视觉任务？北大微软联合开发'
categories: 
 - 新媒体
 - cnBeta
 - 最新
headimg: 'https://x0.ifengimg.com/ucms/2021_48/D5EECC903D77C8679CFE60BFF3AB0B58399778ED_size3028_w1079_h942.gif'
author: cnBeta
comments: false
date: Sat, 27 Nov 2021 09:19:14 GMT
thumbnail: 'https://x0.ifengimg.com/ucms/2021_48/D5EECC903D77C8679CFE60BFF3AB0B58399778ED_size3028_w1079_h942.gif'
---

<div>   
有这样一个模型。它可以做到<strong>一句话生成视频</strong>：不仅<strong>零样本</strong>就能搞定，性能还直达<strong>SOTA</strong>。它的名字，叫<strong>“NüWA”</strong>（女娲）。<strong>“女娲女娲，神通广大”</strong>，正如其名，一句话生成视频只是这个模型的<strong>技能之一</strong>。除此之外，一句话生成图片，草图生成图像、视频，图像补全，视频预测，图像编辑、视频编辑——<br>
 <p><img src="https://x0.ifengimg.com/ucms/2021_48/D5EECC903D77C8679CFE60BFF3AB0B58399778ED_size3028_w1079_h942.gif" alt="图片" referrerpolicy="no-referrer"><br></p><p><img src="https://x0.ifengimg.com/ucms/2021_48/3D3575687CD389A31C7366625C6EE5AE99E8D0A6_size42_w1080_h377.jpg" alt="图片" referrerpolicy="no-referrer"><br></p><p>一共<strong>八种</strong>视觉任务，它其实<strong>全部都能搞定</strong>。</p><p>完全是一位不折不扣的“全能型选手”。</p><p>它，就是由<strong>微软亚研院和北大</strong>联合打造的一个<strong>多模态预训练模型</strong>，在首届微软峰会上亮相。</p><p>目前，在Twitter上已“小有热度”。</p><p><img src="https://x0.ifengimg.com/ucms/2021_48/48F18EAECC79C1F4A144266E8F0780F11173DF23_size91_w772_h1188.jpg" alt="图片" referrerpolicy="no-referrer"><br></p><p>八项全能“女娲”，单拎出来也不差</p><p>所以这个全能型选手究竟表现如何？</p><p>直接与SOTA模型对比，来看看“她”在各项任务上的表现。</p><p>在<strong>文本生成图像</strong>中，不得不说，即使“女娲”的FID-0得分不及XMC-GAN，但在实际效果中，“女娲”生成的图肉眼可见的更好，<strong>清晰又逼真</strong>。</p><p><img src="https://x0.ifengimg.com/ucms/2021_48/CA9B372249778A6132000FC5A15ECB3EFF4491A6_size123_w862_h940.jpg" alt="图片" referrerpolicy="no-referrer"><br></p><p><img src="https://x0.ifengimg.com/ucms/2021_48/ACEAC5D2A95DFD2839DAF339D002B8E6C094C577_size44_w814_h414.jpg" alt="图片" referrerpolicy="no-referrer"><br></p><p><strong>文本到视频</strong>中，“女娲”每一项指标都获得了第一名，从逐帧图片来看，差距很明显。</p><p><img src="https://x0.ifengimg.com/ucms/2021_48/94D4890A4506CB607FBFE3E672C029FA42E9EDDA_size29_w794_h306.jpg" alt="图片" referrerpolicy="no-referrer"><br></p><p><img src="https://x0.ifengimg.com/ucms/2021_48/E12CC5C6CB223FAA29DC633517734DCD82813B53_size74_w1080_h543.jpg" alt="图片" referrerpolicy="no-referrer"><br></p><p>在<strong>视频预测</strong>中，所有模型使用64x64的分辨率，Cond.代表供预测的帧数。</p><p>尽管<strong>只有1帧</strong>，“女娲”也将FVD得分<strong>从94±2</strong><strong>降到86.9</strong>。</p><p><img src="https://x0.ifengimg.com/ucms/2021_48/78F33BE99ABABC876D27313CF4A0960DA66C2EFF_size38_w826_h628.jpg" alt="图片" referrerpolicy="no-referrer"><br></p><p><a target="_blank" href="https://static.cnbetacdn.com/article/2021/1127/6d11f2ca4f1bbb0.gif"><img data-original="https://static.cnbetacdn.com/article/2021/1127/6d11f2ca4f1bbb0.gif" src="https://static.cnbetacdn.com/thumb/article/2021/1127/6d11f2ca4f1bbb0.gif" referrerpolicy="no-referrer"></a><br></p><p><strong>草图转图像</strong>时，与SOTA模型相比，“女娲”生成的卡车都更逼真。</p><p><img src="https://x0.ifengimg.com/ucms/2021_48/84B3F97CFA0D863B343F028DAC7450061A4432FF_size59_w652_h532.jpg" alt="图片" referrerpolicy="no-referrer"><br></p><p>而在零样本的<strong>图像补全</strong>任务中，“女娲”拥有<strong>更丰富的“想象力”</strong>。</p><p><img src="https://x0.ifengimg.com/ucms/2021_48/61D13E56A945599601927368605E53583B1645F7_size49_w640_h532.jpg" alt="图片" referrerpolicy="no-referrer"><br></p><p>在零样本的<strong>图像编辑</strong>任务中，“女娲”明显比SOTA模型的<strong>“P图”能力更强</strong>。</p><p>并且，它的另一个优势是推理速度，<strong>几乎50秒</strong>就可以生成一个图像；而Paint By Word在推理过程中需要额外的训练，大约需要300秒才能收敛。</p><p><img src="https://x0.ifengimg.com/ucms/2021_48/4CA364854E2437A4BBC8E9CA568D044EE00FBEB9_size55_w642_h554.jpg" alt="图片" referrerpolicy="no-referrer"><br></p><p>而<strong>草图生成视频</strong>以及文本引导的<strong>视频编辑</strong>任务，是本次研究首次提出，目前还没有可比对象。</p><p>直接上效果：</p><p><a target="_blank" href="https://static.cnbetacdn.com/article/2021/1127/bc60738e650a2f0.gif"><img data-original="https://static.cnbetacdn.com/article/2021/1127/bc60738e650a2f0.gif" src="https://static.cnbetacdn.com/thumb/article/2021/1127/bc60738e650a2f0.gif" referrerpolicy="no-referrer"></a><br></p><p>看，像上面这些仅用色块勾勒轮廓的视频草图，经“女娲”之手就能生成相应视频。</p><p>而输入一段潜水视频，“女娲”也能在文本指导下让潜水员浮出水面、继续下潜，甚至“游”到天上。</p><p><a target="_blank" href="https://static.cnbetacdn.com/article/2021/1127/0a3991dbb12e67a.gif"><img data-original="https://static.cnbetacdn.com/article/2021/1127/0a3991dbb12e67a.gif" src="https://static.cnbetacdn.com/thumb/article/2021/1127/0a3991dbb12e67a.gif" referrerpolicy="no-referrer"></a><br></p><p>可以说，“女娲”不仅技能多，哪个单项拿出来也完全不赖。</p><p>如何实现？</p><p>这样一个无论操作对象是图像还是视频，无论是合成新的、还是在已有素材上改造都能做到做好的“女娲”，是如何被打造出来的呢？</p><p>其实不难，<strong>把文字、图像、视频分别看做一维、二维、三维数据</strong>，分别对应3个以它们为输入的编码器。</p><p><strong>另外预训练好一个处理图像与视频数据的3D解码器。</strong></p><p>两者配合就获得了以上各种能力。</p><p>其中，对于图像补全、视频预测、图像视频编辑任务，输入的部分图像或视频直接馈送给解码器。</p><p><img src="https://x0.ifengimg.com/ucms/2021_48/F4C986BDEFE1CD1FB7B5292E5C74EAA2103C6105_size40_w1080_h432.jpg" alt="图片" referrerpolicy="no-referrer"><br></p><p>而编码解码器都是基于一个<strong>3D Nearby的自注意力机制</strong>（3DNA）建立的，该机制可以同时考虑空间和时间轴的上局部特性，定义如下：</p><p><img src="https://x0.ifengimg.com/ucms/2021_48/5E585CEF829BD73EB75CBDA154B53DCA9C220FB2_size2_w408_h72.jpg" alt="图片" referrerpolicy="no-referrer"><br></p><p>W表示可学习的权重，X和C分别代表文本、图像、视频数据的3D表示：</p><p><img src="https://x0.ifengimg.com/ucms/2021_48/22106D8C386EEED034FEEDA79812BC61F45CE62E_size3_w518_h46.jpg" alt="图片" referrerpolicy="no-referrer"><br></p><p>其中，h和w表示空间轴上的token数，s表时间轴上的token数（文本默认为1），d表示每个token的维数。</p><p>如果C=X，3DNA表示对目标X的自注意；如果C≠X，3DNA表示对在条件C下目标X的交叉注意。</p><p>该机制不仅可以<strong>降低模型的计算复杂度</strong>，还能<strong>提高生成结果的质量</strong>。</p><p>此外，模型还使用<strong>VQ-GAN</strong>替代VQ-VAE进行视觉tokenization，这也让生成效果好上加好。</p><p><strong>团队介绍：</strong></p><p>一作Chenfei Wu，北京邮电大学博士毕业，现工作于微软亚研院。</p><p>共同一作Jian Liang， 来自北京大学。</p><p>其余作者包括微软亚研院的高级研究员Lei Ji，首席研究员Fan Yang，合作首席科学家Daxin Jiang，以及北大副教授方跃坚。</p><p>通讯作者为微软亚研院的高级研究员&研究经理段楠。</p>   
</div>
            