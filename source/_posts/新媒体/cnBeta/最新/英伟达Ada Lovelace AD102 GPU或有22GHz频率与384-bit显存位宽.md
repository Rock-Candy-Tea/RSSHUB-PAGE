
---
title: '英伟达Ada Lovelace AD102 GPU或有2.2GHz频率与384-bit显存位宽'
categories: 
 - 新媒体
 - cnBeta
 - 最新
headimg: 'https://static.cnbetacdn.com/article/2021/0921/4dbf895b1fa5cbe.jpg'
author: cnBeta
comments: false
date: Tue, 21 Sep 2021 03:40:31 GMT
thumbnail: 'https://static.cnbetacdn.com/article/2021/0921/4dbf895b1fa5cbe.jpg'
---

<div>   
@Greymon55 最新爆料称：<strong>英伟达下一代 Ada Lovelace AD102 GPU 的时钟速率有望达到 2.2 GHz、搭配 384-bit 位宽的 GDDR6X 显存，采用台积电 5nm 工艺制造、且具有超过 80 TFLOPs 的算力。</strong>对于工作站客户与游戏玩家来说，作为 Ampere GA102 GPU 的继任者，RTX 40 系列旗舰显卡有望带来更显著的性能改进。<br>
<p><img src="https://static.cnbetacdn.com/article/2021/0921/4dbf895b1fa5cbe.jpg" alt="1.jpg" referrerpolicy="no-referrer"></p><p style="text-align: center;">（图 via <a href="https://wccftech.com/nvidia-ada-lovelace-ad102-gpu-clock-as-high-as-2-2-ghz-384-bit-bus-gddr6x-over-80-tflops/" target="_self">WCCFTech</a>）</p><p>在有关 Ada Lovelace GPU 的大量传闻中，AD102 SKU 显然是大家最为关注的那一个。据说它会使用台积电 5nm（N5）工艺，辅以单片式设计。</p><p>除了 @Greymon55 新披露的 ≥2.2 GHz 主频，Kopite 也曾披露过 AD102 GPU 的一些参数，比如可能具有 18432 个 CUDA 核心 —— 几乎较 Ampere 翻倍。</p><p>但其实从 RTX 20 系的 Turing 架构转向 RTX 30 系的 Ampere 架构之时，英伟达就已经让 GPU 规格跃升了一次。</p><p><img src="https://static.cnbetacdn.com/article/2021/0921/289deb144422174.png" alt="2.png" referrerpolicy="no-referrer"></p><p>基于 2.2 GHz 的 GPU 时钟速率，Ada Lovelace AD102 GPU 有望提供 81 TFLOPs 的 FP32 算力。作为参考，RTX 3090 只有 36 TFLOPs 。</p><p>尽管 125% 的性能提升看似很惊人，但我们也要清楚 Ampere GA102（RTX 3090）已经较 Turing TU102（RTX 2080 Ti）的 13 TFLOPs 大跃进了一次（150% 以上）。</p><p>不过从实际游戏体验来看，RTX 3090 平均也仅较 RTX 2080 Ti 领先 50~60% 左右。</p><p><img src="https://static.cnbetacdn.com/article/2021/0921/c25362669a2ddcb.png" alt="3.png" referrerpolicy="no-referrer"></p><p>显存方面，爆料人暗示 RTX 40 系列旗舰 SKU 仍会保留类似 RTX 3090 的 384-bit @ GDDR6X 规格。虽然没有迁移到更新一代的 GDDR7 标准，但至少 GDDR6X 显存的频率可达 20 Gbps 以上。</p><p>如果按期上市，搭载 Ada Lovelace GPU 的下一代 GeForce RTX 40 系列显卡，将于 <a data-link="1" href="https://c.duomai.com/track.php?site_id=242986&euid=&t=https://amd-cpu.jd.com/" target="_blank">AMD</a> 基于 RNDA 3 GPU 的 Radeon RX 7000 系列展开直接的竞争。</p><p>遗憾的是，目前尚不清楚英伟达将于何时采纳多芯片（MCM）GPU 设计方案。</p>   
</div>
            