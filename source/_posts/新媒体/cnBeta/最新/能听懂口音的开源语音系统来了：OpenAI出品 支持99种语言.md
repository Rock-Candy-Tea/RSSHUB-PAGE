
---
title: '能听懂口音的开源语音系统来了：OpenAI出品 支持99种语言'
categories: 
 - 新媒体
 - cnBeta
 - 最新
headimg: 'https://static.cnbetacdn.com/article/2022/0924/3e35c83ddce7e19.jpg'
author: cnBeta
comments: false
date: Sat, 24 Sep 2022 11:18:22 GMT
thumbnail: 'https://static.cnbetacdn.com/article/2022/0924/3e35c83ddce7e19.jpg'
---

<div>   
<strong>逼近人类水平</strong>的语音识别系统来了？没错，OpenAI新开源了一个名为<strong>「Whisper」</strong>的新语音识别系统，据称在英文语音识别方面拥有接近人类水平的鲁棒性和准确性！不仅如此，对于<strong>不同口音</strong>、<strong>专业术语</strong>的识别效果也是杠杠的！一经发布就在推特上收获<strong>4800+点赞</strong>，<strong>1000+转发</strong>。<br>
 <p><img src="https://static.cnbetacdn.com/article/2022/0924/3e35c83ddce7e19.jpg" referrerpolicy="no-referrer"><br></p><p>网友们纷纷对它意料之外的强大功能表示惊讶。</p><p>不仅是英文，有人用法国诗人波德莱尔的《恶之花》进行了语音测试，得到的文本<strong>几乎与原文一致</strong>。</p><p><img src="https://static.cnbetacdn.com/article/2022/0924/2dcfbc37518a3fd.jpg" referrerpolicy="no-referrer"><br></p><p>OpenAI联合创始人&首席科学家Ilya Sutskever就表示：</p><blockquote id="171215CO">终于有一个靠谱的语音识别系统能听懂我的口音了。</blockquote><p><img src="https://static.cnbetacdn.com/article/2022/0924/f5fbbb4f2a659ff.jpg" referrerpolicy="no-referrer"><br></p><p>前任特斯拉人工智能总监Andrej Karpathy甚至转发评论：OpenAI正处于最好的状态中。</p><p><img src="https://static.cnbetacdn.com/article/2022/0924/ed6a2c521817e9f.jpg" referrerpolicy="no-referrer"><br></p><p>话不多说，让我们看看这个被“好评如潮”的语音系统究竟是怎么回事。</p><p>逼近人类水平的语音识别系统</p><p>首先，Whisper最大特点是它使用的<strong>超大规模训练集</strong>：</p><p>它使用从网络上收集的<strong>68万小时</strong>的多语言、多任务监督数据进行训练。</p><p>这导致数据集的内容非常多元化，涵盖了许多不同环境、不同录音设备下、不同语言的音频。</p><p>具体而言，65%(438218小时)是英语音频和匹配的英语文本，大约18%(125739小时)是非英语音频和英语文本，而最后17%(117113小时)则是非英语音频和相应的文本。</p><p>其中，非英语部分共包含<strong>98种</strong>不同语言。</p><p><img src="https://static.cnbetacdn.com/article/2022/0924/cc0da60acc0f3d4.jpg" referrerpolicy="no-referrer"><br></p><p>不过，虽然音频质量的多样性可以帮助提高训练模型的鲁棒性，但转录文本质量的多样性并不是同样有益的。</p><p>初步检查显示，原始数据集中有大量不合格的、现有自动语音识别(ASR)系统生成的转录文本。</p><p>而以往的研究表明，在人工和机器混合生成的数据集上进行训练，会显著损害翻译系统的性能。</p><p>为了解决这个问题，研究团队开发了几种自动过滤方法来识别和删除低质量的数据源。</p><p>但值得一提的是，没有说话内容的片段会被留下，作为语音活动检测的训练数据。</p><p>其次，Whisper体系结构是一种简单的端到端方法，具体来说就是Transformer的编码器-解码器格式。</p><p><img src="https://static.cnbetacdn.com/article/2022/0924/7d11c761d72726a.jpg" referrerpolicy="no-referrer"><br></p><p>输入音频被分成30秒的片段，再转换成log-Mel谱图，然后传入编码器。</p><p>解码器被训练来预测相应的文本标题，并混合特殊标记，指示单一模型执行诸如语言识别、多语言语音转录和英语语音翻译等任务。</p><p><img src="https://static.cnbetacdn.com/article/2022/0924/1aaaeb4c07912af.jpg" referrerpolicy="no-referrer"><br></p><p>除此之外，研究人员还为Whisper设置了<strong>5种不同的型号</strong>，以下是各模型大致的内存需求和相对速度，使用者可以自行选择。</p><p>但需要注意的是，只有“large”型号支持多语言，前4个模型都只支持英语。</p><p><img src="https://static.cnbetacdn.com/article/2022/0924/ede003488c3c2f4.jpg" referrerpolicy="no-referrer"><br></p><p>不过不需要担心，与其他模型相比，英文语音识别正是Whisper的核心竞争力。</p><p>实验结果证明，Whisper在Librispeech test-clean测试的错误率达到2.7%。</p><p>虽然这一数值与Wav2vec 2.0一样，但在零样本性能上，Whisper明显更稳健，<strong>平均误差减少了55%</strong>。</p><p><img src="https://static.cnbetacdn.com/article/2022/0924/8c4fc3866834618.jpg" referrerpolicy="no-referrer"><br></p><p>甚至零样本Whisper模型还<strong>缩小了与人类鲁棒性之间的差距</strong>。</p><p>可以看出，与人类Alec相比，LibriSpeech模型的错误率大约是人类的两倍，而Whisper模型的鲁棒性边界则包括Alec95%的置信区间。</p><p><img src="https://static.cnbetacdn.com/article/2022/0924/2e6efe60f81784a.jpg" referrerpolicy="no-referrer"><br></p><p>研究团队</p><p>Whisper的研究团队来自OpenAI，共同一作有两位：Alec Radford、Jong Wook Kim。</p><p><img src="https://static.cnbetacdn.com/article/2022/0924/f0f7abfd0d198b6.jpg" referrerpolicy="no-referrer"><br></p><p>Alec Radford，OpenAI的机器学习研究员，也是indico.io的联合创始人。</p><p><img src="https://static.cnbetacdn.com/article/2022/0924/25df3e8f0120ea5.jpg" referrerpolicy="no-referrer"><br></p><p>Jong Wook Kim，在纽约大学获得了音乐技术专业的博士学位，研究方向包括多模态深度学习和音乐理解，目前是OpenAI的研究人员。</p><p><img src="https://static.cnbetacdn.com/article/2022/0924/81c34cec0d891eb.jpg" referrerpolicy="no-referrer"><br></p><p>值得一提的是，研究团队指出，虽然目前Whisper还没有实时功能，但它的运行速度和内存大小表明，在这一基础上搭建实时语音识别和翻译功能是可行的。</p><p>他们希望Whisper的高精度和易用性，将允许开发人员将语音接口添加到更广泛的应用程序中。</p><p>论文和GitHub链接附在文末，感兴趣的小伙伴们可以自取～</p><p>论文链接：</p><p>https://cdn.openai.com/papers/whisper.pdf</p><p>GitHub链接：</p><p>https://github.com/openai/whisper#approach</p><p>参考链接：</p><p>[1]https://colab.research.google.com/github/openai/whisper/blob/master/notebooks/LibriSpeech.ipynb</p><p>[2]https://techcrunch.com/2022/09/21/openai-open-sources-whisper-a-multilingual-speech-recognition-system/?guccounter</p><p>[3]https://news.ycombinator.com/item?id=32927360</p><p>[4]https://twitter.com/alecrad</p><p>[5]https://jongwook.kim/</p>   
</div>
            