
---
title: '苹果公司隐私主管再谈CSAM系统 从设计之初就想过防止被干预和滥用'
categories: 
 - 新媒体
 - cnBeta
 - 最新
headimg: 'https://static.cnbetacdn.com/thumb/article/2021/0807/4b67d42ad1477f2.jpg'
author: cnBeta
comments: false
date: Tue, 10 Aug 2021 23:56:39 GMT
thumbnail: 'https://static.cnbetacdn.com/thumb/article/2021/0807/4b67d42ad1477f2.jpg'
---

<div>   
<strong>苹果公司的负责隐私的主管Erik
Neuenschwander近期详细介绍了该公司CSAM（儿童性虐待图片）扫描系统中内置的一些细节，以减少外界对此功能的疑虑。</strong>上周苹果宣布将推出一系列的防止儿童色情犯罪功能，其中最有争议的，他们将会扫描用户的云端相册，并将其数据与相关数据库中的关键部分对比，如果符合并且经过人工确认后，会向相关部门报告。<br>
 <p><a href="https://static.cnbetacdn.com/thumb/article/2021/0807/4b67d42ad1477f2.jpg" target="_blank"><img src="https://static.cnbetacdn.com/thumb/article/2021/0807/4b67d42ad1477f2.jpg" referrerpolicy="no-referrer"></a></p><p>CSAM检测系统也因此引发争议。作为回应，苹果公司提供了许多如何在不危及用户隐私的情况下扫描CSAM的详细信息。</p><p>在接受外媒TechCrunch采访时，苹果公司隐私负责人Erik Neuenschwander表示，该系统从一开始想到了防止政府过度干预和滥用。</p><p>一方面，该系统仅适用于美国，第四修正案的保护措施已经防止非法搜查和扣押。另外，他适配iOS 15系统，如果没有更新iOS，也无法进行；第三，它不能针对个人用户提供特定的更新，只是将指纹数据（也就是部分关键特征）与政府或相关部门提供的数据库比对。不在国家失踪和受虐儿童中心提供的数据库中的图像也不会被标记。</p><p>苹果公司也有人工审核流程。如果iCloud帐户被标记为疑似收集非法CSAM材料，苹果公司团队将审查该数据，以确保机器是正确判断的。</p><p>“所以这个假设（指的是滥用此功能）需要跳过很多条件，包括让苹果公司改变其内部流程以引用，或者更改CSAM数据库，我们不相信人们能够在此基础上在美国提出这个要求”</p><p>此外，Neuenschwander补充说，该系统仅在用户启用了iCloud照片时才有效。苹果隐私负责人表示，如果用户不喜欢该系统，“他们可以选择不使用iCloud照片，这样系统的任何部分都无法运行”。</p><p>在上周，苹果公司也曾明确表示，它将拒绝任何政府组织试图修改或将系统用于CSAM以外的用途：“Apple将拒绝任何这样的要求，我们以前曾面临类似要求，这些会降低用户的隐私，因此坚决拒绝了。我们将来还会继续拒绝他们。这项技术仅限于探测存储在iCloud中的相关图片，我们不会同意任何政府扩展它用途的请求。”</p>   
</div>
            