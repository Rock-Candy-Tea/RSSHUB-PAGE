
---
title: 'Cerebras打破在单个设备上训练大规模AI模型的纪录'
categories: 
 - 新媒体
 - cnBeta
 - 最新
headimg: 'https://static.cnbetacdn.com/article/2022/0623/1d2112ff6ed5ca6.png'
author: cnBeta
comments: false
date: Thu, 23 Jun 2022 11:29:29 GMT
thumbnail: 'https://static.cnbetacdn.com/article/2022/0623/1d2112ff6ed5ca6.png'
---

<div>   
作为世上最大加速器芯片 CS-2 Wafer Scale Engine 背后的公司，<strong>Cerebras 刚又宣布了另一个里程碑 —— 在单个设备上完成了目前最大的自然语言处理（NLP）人工智能模型的训练。</strong>此前也有人尝试在智能手表这样的可穿戴设备设备上训练 AI 模型，但 Cerebras 这次又将参数提升到了 20 亿的量级。<br>
 <p><img src="https://static.cnbetacdn.com/article/2022/0623/1d2112ff6ed5ca6.png" alt="0-1.png" referrerpolicy="no-referrer"></p><p style="text-align: center;">Wafer Scale Engine-2 晶圆级芯片资料图（来自：Cerebras）</p><p>本次演示使用了 OpenAI 的 120 亿参数 DALL-E，且所有工作负载无需扩展到横跨多个加速器的平台上去完成，从而极大地降低了对基础设施和软件的复杂性要求。</p><p><img src="https://static.cnbetacdn.com/article/2022/0623/b166c2f173b1d47.jpg" alt="0-2.jpg" referrerpolicy="no-referrer"></p><p>不过需要指出的是，单个 CS-2 系统本身就已经可以媲美超算 —— 7nm 单晶圆（通常可容纳数百枚主流芯片），拥有惊人的 2.6 万亿个晶体管、85 万个内核、40GB 集成缓存，且封装功耗高达 15kW 。</p><p><a href="https://static.cnbetacdn.com/article/2022/0623/446f5de5ca4fd08.jpg" target="_blank"><img src="https://static.cnbetacdn.com/thumb/article/2022/0623/446f5de5ca4fd08.jpg" alt="1.jpg" referrerpolicy="no-referrer"></a></p><p>Cerebras 尝试在单个芯片上保留多达 200 亿个参数的 NLP 模型，以显著降低数以千计的 GPU 训练成本、扩展所需的相关硬件需求，并且消除了在它们之间划分模型的技术难度。</p><p><a href="https://static.cnbetacdn.com/article/2022/0623/3c71cfddbdc68f8.jpg" target="_blank"><img src="https://static.cnbetacdn.com/thumb/article/2022/0623/3c71cfddbdc68f8.jpg" alt="2.jpg" referrerpolicy="no-referrer"></a></p><p>Cerebras 指出，这也是常规 NLP 工作负载的痛点之一，有时动辄需要耗费数月时间才能完成。</p><p><a href="https://static.cnbetacdn.com/article/2022/0623/76915de40f12015.jpg" target="_blank"><img src="https://static.cnbetacdn.com/thumb/article/2022/0623/76915de40f12015.jpg" alt="3.jpg" referrerpolicy="no-referrer"></a></p><p>由于高度定制，每个正在处理的神经网络、GPU 规格、以及将它们联系到一起的网络，都是独一无二的 —— 这些元素必须在初次训练前就搞定，且无法做到跨系统移植。</p><p><a href="https://static.cnbetacdn.com/article/2022/0623/c615abb0fca297f.jpg" target="_blank"><img src="https://static.cnbetacdn.com/thumb/article/2022/0623/c615abb0fca297f.jpg" alt="4.jpg" referrerpolicy="no-referrer"></a></p><p>至于 OpenAI 的 GPT-3 自然预览处理模型，它有时已经能够编写出让你误以为是真人所撰写的整篇文章，且具有 1750 亿个惊人的参数。</p><p><a href="https://static.cnbetacdn.com/article/2022/0623/cc2828bc1390a15.jpg" target="_blank"><img src="https://static.cnbetacdn.com/thumb/article/2022/0623/cc2828bc1390a15.jpg" alt="5.jpg" referrerpolicy="no-referrer"></a></p><p>不过 DeepMind 在 2021 年底推出的 Gopher，已将这个数字大幅提升到了 2800 亿，且 Google Brain 甚至宣布训练了一个超万亿参数的 Switch Transformer 模型。</p><p><a href="https://static.cnbetacdn.com/article/2022/0623/05cf3637ed82068.jpg" target="_blank"><img src="https://static.cnbetacdn.com/thumb/article/2022/0623/05cf3637ed82068.jpg" alt="17.jpg" referrerpolicy="no-referrer"></a></p><p>Cerebras 首席执行官兼联合创始人 Andrew Feldman 表示：更大的 NLP 模型，意味着它的准确度也更高。</p><p><a href="https://static.cnbetacdn.com/article/2022/0623/e5297d8015ffa82.jpg" target="_blank"><img src="https://static.cnbetacdn.com/thumb/article/2022/0623/e5297d8015ffa82.jpg" alt="18.jpg" referrerpolicy="no-referrer"></a></p><p>但通常只有极少数公司拥有如此庞大的必要资源和专业知识，来分解这些大型模型、并将之分散到数百、或数千个 GPU 上去艰苦运算。</p><p><a href="https://static.cnbetacdn.com/article/2022/0623/567c01c8131ab6f.jpg" target="_blank"><img src="https://static.cnbetacdn.com/thumb/article/2022/0623/567c01c8131ab6f.jpg" alt="19.jpg" referrerpolicy="no-referrer"></a></p><p>正因如此，我们只看到过极少数公司能够训练大型 NLP 模型 —— 这对行业内的其他人来说过于昂贵、耗时、且难以使用。</p><p><a href="https://static.cnbetacdn.com/article/2022/0623/d9edc0a1f023539.jpg" target="_blank"><img src="https://static.cnbetacdn.com/thumb/article/2022/0623/d9edc0a1f023539.jpg" alt="20.jpg" referrerpolicy="no-referrer"></a></p><p>今天，Cerebras 很自豪地宣布普及了 GPT-3XL 1.3B、GPT-J 6B、GPT-3 13B 和 GPT-NeoX 20B，让整个 AI 生态系统都能够在几分钟内建立大型模型、并在单个 CS-2 平台上展开训练。</p><p><a href="https://static.cnbetacdn.com/article/2022/0623/8c4412bb7db471f.jpg" target="_blank"><img src="https://static.cnbetacdn.com/thumb/article/2022/0623/8c4412bb7db471f.jpg" alt="21.jpg" referrerpolicy="no-referrer"></a></p><p>不过与 CPU 领域类似，主频只是衡量性能的其中一项指标。比如 Chinchilla 就尝试通过使用更少的参数（700 亿个），得出了较 GPT-3 和 Gopher 更好的结果。</p>   
</div>
            