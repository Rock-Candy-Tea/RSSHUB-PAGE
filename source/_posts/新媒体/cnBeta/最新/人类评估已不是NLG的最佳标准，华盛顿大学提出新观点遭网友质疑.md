
---
title: '人类评估已不是NLG的最佳标准，华盛顿大学提出新观点遭网友质疑'
categories: 
 - 新媒体
 - cnBeta
 - 最新
headimg: 'https://static.cnbetacdn.com/article/2021/0717/107f2ffd3853a5e.jpg'
author: cnBeta
comments: false
date: Sat, 17 Jul 2021 07:55:26 GMT
thumbnail: 'https://static.cnbetacdn.com/article/2021/0717/107f2ffd3853a5e.jpg'
---

<div>   
AI生成的文本好不好，最权威的评估者竟然不是人类自己？<strong>最近，华盛顿大学和艾伦人工智能研究院的学者们在研究中发现：未经过训练的人类评估文本时，往往过分关注生成文本像不像人话，而忽略了生成文本更重要的问题，即它的内容是否正确、合乎逻辑。</strong><br>
 <p style="text-align: left;">明敏 发自 凹非寺</p><p style="text-align: left;">研究人员就给出了一个例子：</p><p style="text-align:center"><img src="https://static.cnbetacdn.com/article/2021/0717/107f2ffd3853a5e.jpg" referrerpolicy="no-referrer"></p><p style="text-align: left;">他们分别让未经训练的人类和机器来评价一段GPT-3生成的文字。</p><p style="text-align: left;">这段文字翻译过来为：</p><blockquote style="text-align: left;">从前，有一个海盗。他是那种宁愿把时间花在驱赶在船周围游泳的鲨鱼上，也不愿驶向外国港口寻找战利品的海盗。他是个好海盗，高尚的海盗，诚实的海盗。他是个宁愿和妻儿呆在家里也不愿出海的海盗。</blockquote><p style="text-align: left;">人类评估员认为这段文字除了有些啰嗦外，没什么大毛病。</p><blockquote style="text-align: left;">这可能就是一个海盗想回家陪老婆孩子吧，AI可能没理解，但是这也没什么稀奇的。</blockquote><p style="text-align: left;">机器评估也认为这段文字很啰嗦，不过它对文段的内容提出了质疑：</p><blockquote style="text-align: left;">海盗会有老婆孩子？还不和他一起在船上生活？</blockquote><p style="text-align: left;">对比两种判断，人类评估更看重这段话像不像人话，在检验过它的确非常流畅后，就会默认这段文本没什么大问题了。</p><p style="text-align: left;">而机器的判断角度则更加多维，会考虑到文字传达的意思是否正确。</p><p style="text-align: left;">很难分辨出GPT-3生成的文本</p><p style="text-align: left;">为了验证自己的观点，研究人员让未经训练的评估人员来区分人类写的文本和AI生成的文本。</p><p style="text-align: left;">他们选择了故事、新闻、菜谱三种不同的文体进行测试。</p><p style="text-align:center"><img src="https://static.cnbetacdn.com/article/2021/0717/14dc62038b5b0ee.jpg" referrerpolicy="no-referrer"></p><p style="text-align: left;">具体测试中，受试人员不仅要判断给出的文本是否人类创作的，还要填写相应的理由。</p><p style="text-align: left;">结果显示，在区分人类和GPT-2创作的文本时，被测试群体的正确率为<strong>57.9%</strong>。</p><p style="text-align: left;">但是在区分GPT-3生成的文本上，正确率就下降到了<strong>49.9%</strong>。</p><p style="text-align: left;">而二选一问题的随机概率就有50%……</p><p style="text-align:center"><img src="https://static.cnbetacdn.com/article/2021/0717/b309fde4da4ea98.jpg" referrerpolicy="no-referrer"></p><p style="text-align: left;">显然，普通人已经很难识别出当下最先进的NLG模型所生成的文本。</p><p style="text-align: left;">为了更进一步了解受试人员是如何做出判断的，研究人员对150个回答进行了分析。</p><p style="text-align: left;">结果发现，受试人员在做出判断后，更加倾向于从文本的<strong>格式、风格、语法</strong>角度上给出理由。</p><p style="text-align: left;">150个回答中，基于文本形式的判断几乎是基于内容判断的<strong>2倍</strong>。</p><p style="text-align: left;">但是，GPT-3在文本流畅度方面的表现其实已经非常出色，这或许也是为什么人类很难分辨GPT-3生成文本。</p><p style="text-align: left;">而且研究人员发现，受试人员给出判断的理由都不尽相同，这也表明人类评估文本没有一个明确的标准。</p><p style="text-align: left;">既然NLG模型训练后可以变强，那培训一下评估人员呢？</p><p style="text-align: left;">研究人员决定对一些受试人员进行了培训，提高他们评估文字的能力和速度。</p><p style="text-align: left;">他们准备了3种不同的培训：</p><p style="text-align: left;">第一种是给出明确的判断标准，让受试人员学习后来判断；</p><p style="text-align: left;">第二种是通过大量的实例训练，也就是题海战术；</p><p style="text-align: left;">第三种是通过不断对比来完成训练。</p><p style="text-align:center"><img src="https://static.cnbetacdn.com/article/2021/0717/baefe40dccc8e16.jpg" referrerpolicy="no-referrer"></p><p style="text-align: left;">然而结果表明，这好像并<strong>没有什么用</strong>。</p><p style="text-align: left;">三种培训后的判断正确率分别为52%、55%、53%，相较于未受训时的表现，没有显著提高。</p><p style="text-align: left;">不过从受试人员的回答中可以看到，更多人现在会多维度判断文本了，还是有进步的。</p><p style="text-align: left;">基于这样的实验结果，研究人员认为在评估最先进的NLG模型方面，人类可能真的不太靠谱了。</p><p style="text-align: left;">这实验不太靠谱</p><p style="text-align: left;">对于这样的结论，网友们提出了一些不同的看法：</p><blockquote style="text-align: left;">判断文本质量其实是一件非常艰巨的任务，需要专家来进行评估。</blockquote><blockquote style="text-align: left;">或许是这项研究中的受试人员不太行？</blockquote><p style="text-align:center"><img src="https://static.cnbetacdn.com/article/2021/0717/90e4a5d4481516c.jpg" referrerpolicy="no-referrer"></p><p style="text-align: left;">有人就指出了问题所在：他们用的Amazon Mechanical Turk的评估员。</p><blockquote style="text-align: left;">是受试人员不太行。</blockquote><p style="text-align:center"><img src="https://static.cnbetacdn.com/article/2021/0717/fdc89a499c8c70c.jpg" referrerpolicy="no-referrer"></p><p style="text-align: left;"><strong>AMTurk</strong>作为一个众包平台，近年来实在是饱受诟病。</p><p style="text-align: left;">此前BBC报道称，由于招募到的志愿者所在的地区存在一些观念偏见，导致最后研究出的算法也存在偏见。</p><p style="text-align: left;">而且招募到的人员水平也常常参差不齐。</p><p style="text-align: left;">不过有人也表示：这些人可能也是最适合的，因为他们最接近普通大众水平，专家认为好的文字，普通人未必也这么认为。</p><blockquote style="text-align: left;">这要取决于生成文本的目标人群是谁。</blockquote><blockquote style="text-align: left;">实验中的志愿者对乔伊斯（后现代文学作家）的欣赏程度肯定和英文系教授不同。</blockquote><blockquote style="text-align: left;">尽管顶级文学评论家将其描述为“20世纪实验文学的伟大纪念碑之一”和“英语中最美丽的散文诗之一”，但对于大多数普通读者而言，它非常晦涩难懂。</blockquote><p style="text-align: left;">此外，也有人就对这项研究提出了改进建议：</p><blockquote style="text-align: left;">我认为他们可以用更简单的NLG算法(基于规则，n-gram, rnn)进行更精细的分析，并对“非专家”评估者进行排名，而不是将他们作为一个群体来处理。</blockquote><p style="text-align: left;">而关于NLG模型生成文本的评估问题，Google曾给出过一个方案。</p><p style="text-align: left;">2020年，它们提出了一个可量化评估NLG模型性能的指标——<strong>BLEURT</strong>。</p><p style="text-align: left;">这是一个基于BERT的学习评价指标，在学习了几千个人类评估案例后，它可以对不同模型生成的文本进行打分。</p><p style="text-align: left;">其最大的优势就是，评估速度<strong>更快</strong>。</p><p style="text-align: left;">Google研究人员认为这个指标有助于NLG模型的研究和开发，而且可以为开发人员提供更加多维的评判标准。</p><p style="text-align: left;">论文地址：</p><p style="text-align: left;">https://arxiv.org/pdf/2107.00061.pdf</p>   
</div>
            