
---
title: 'IEEE Spectrum调查：AI 的 6 种最坏情况'
categories: 
 - 新媒体
 - cnBeta
 - 最新
headimg: 'https://static.cnbetacdn.com/article/2022/0125/b22af9a2502b200.png'
author: cnBeta
comments: false
date: Tue, 25 Jan 2022 10:50:57 GMT
thumbnail: 'https://static.cnbetacdn.com/article/2022/0125/b22af9a2502b200.png'
---

<div>   
对于人类社会，人工智能（AI）带来的最大威胁是什么？好莱坞科幻电影的“想象”提供了答案：它逐渐进化，获得人类思考能力，然后变成霸主，奴役或毁灭人类。也有些观点认为：在AI不知不觉杀死所有人之前，会存在许多危险情形。<br>
 <p>2022年1月份，IEEE Spectrum访谈了多位技术专家，列举出了当前6种AI危险事例，这些危险比科幻电影的描述“风平浪静”，却同样具有威胁性，如果放任自由会有意想不到的后果。它们分别是：虚拟定义现实、AI军备竞赛、隐私透明、斯金纳箱现象、AI偏见以及过度担忧AI影响。</p><section template="t"><section template="t"></section></section><section><section><section><strong>1</strong></section></section><section><section></section></section></section><section><strong>当虚拟定义现实……</strong></section><p>当人类无法区分数字世界中的真实与虚假时，会发生什么？</p><p>设想这样一个场景：AI已经拥有完美生成的能力，利用先进的机器学习工具生成的图像、视频、音频和文本已经“以假乱真”。但如果决策者陷入假信息旋涡，并做出决断，不可避免会导致危机。当上升到国家高度，甚至会发动战争。乔治敦大学的研究员安德鲁•罗恩（Andrew Lohn）认为：AI已经能够产生大量以假乱真的信息。而且AI的特点是“随着大量信息的生成，系统会不断与真实信息进行对比，并且升级生成能力”。</p><p>AI信息生成技术也被称为“<strong>DeepFake</strong>”，其带来的恶作剧已经造成了某些影响。例如去年5月份，一些欧洲高级议员收到了一些“俄罗斯反对派人物”的视频会议邀请，还煞有其事地讨论了克里米亚问题之类的政治事务，结果发现在这些所谓的“俄罗斯反对派人物”都是别人用Deepfake换脸假冒的。这些受骗者包括拉脱维亚议会外交事务委员会主席Rihards Kols，以及来自爱沙尼亚和立陶宛的议员……</p><section template="t"></section><section><section><section><strong>2</strong></section></section><section><section></section></section></section><section><strong>一场危险的逐底竞赛</strong></section><p>当谈到AI和国家安全时，开发速度既是重点也是问题所在。由于AI系统能为用户带来速度优势，所以最先开发军事应用的国家将获得战略优势。但是，一味追求速度可能会牺牲哪些设计原则呢？</p><p>首先，是<strong>“质量”问题</strong>。例如黑客会利用系统中微小的缺陷。乔治敦大学的海伦·托纳（Helen Toner）表明：“从一个无伤大雅单点故障开始，然后所有通信失灵，人们恐慌，经济活动陷入停滞；随后持续的信息缺乏，再加上其他错误计算，可能导致局势失控。”</p><p>另一方面，瑞典的斯德哥尔摩国际和平研究所高级研究员文森特•布拉南警告可能发生重大灾难：“大国为了赢得先发制人的优势而‘偷工减料’，如果一个国家将开发速度置于安全、测试或人为监督之上，那么这将是一场危险的竞逐。”例如，为了获得速度优势，国家安全领导人可能会倾向于授权指挥和控制决策，取消黑盒机器学习模型的人为监督。想象一下，如果自动发射导弹防御系统处于无人监督的环境下会发生什么？</p><section template="t"><section template="t"></section></section><section><section><section><strong>3</strong></section></section><section><section></section></section></section><section><strong>隐私和自由意志的终结</strong></section><p>使用数字技术的过程中产生了大量的电子数据，例如发送电子邮件，阅读文本，下载，购买，发帖等等。当允许公司和政府访问这些数据时，也意味着赋予工具监视和控制我们的权限。</p><p>随着面部识别、生物识别、基因组数据分析等技术兴起。安德鲁•罗恩担心：“我们有时候并没有意识到大数据跟踪和监视技术的不断发展，会使我们进入了未知的危险领域。”数据一旦被收集和分析，其作用就会远远超出跟踪和监视的功能，例如AI的预测性控制功能。今天，AI系统可以预测我们将购买哪些产品，我们将观看哪些娱乐节目，以及我们将点击哪些链接。当这些平台比我们自己更了解我们时，我们可能不会注意到这种微小的变化，但它剥夺了我们的自由意志并使我们受到外部力量的控制。</p><p><img src="https://static.cnbetacdn.com/article/2022/0125/b22af9a2502b200.png" referrerpolicy="no-referrer"><br></p><section template="t"><section template="t"></section></section><section><section><section><strong>4</strong></section></section><section><section></section></section></section><section><strong>人类的斯金纳箱实验</strong></section><p>曾经在20世纪70年代，有位叫做Walter Mischel的研究专家，在美国斯坦福大学附属幼儿园基地内进行了著名的“棉花糖”实验，又称——<strong>“</strong><strong>延迟满足”实验。</strong></p><p>而这个实验的观察数据，以及后期对这些孩子的追踪观察说明：</p><p>那些延迟满足能力强的孩子，自我控制能力也就越强，可以在没有外界监督的情况下，自主性的控制调节自身行为，在某一个任务完成程度上，要更胜一筹。</p><p>当前，具备延迟满足能力孩子也会屈服于AI算法给出的诱惑。</p><p>进一步，社交媒体用户已经成为实验室中的老鼠，生活在斯金纳盒子里。这些用户沉迷于<a data-link="1" href="https://c.duomai.com/track.php?site_id=242986&euid=&t=https://shouji.jd.com/" target="_blank">手机</a>，被迫牺牲更多宝贵的时间和注意力在数字平台上。</p><p>海伦·托纳认为：“算法经过优化，可使用户尽可能长时间地‘留’在平台上。”著名作家马尔科姆·默多克解释道：“通过以喜欢，评论和关注的形式提供奖励，算法会缩短我们大脑的工作方式，让我们不自觉地去参与下一个。”</p><p>为了最大化广告利润，公司把我们的注意力从工作、家人、朋友，责任甚至爱好上转移。更糟糕的是，如果推送内容质量下降，用户会痛苦和暴躁。海伦·托纳警告：“我们在平台上花费的时间越多，花在追求积极、高效和充实生活上的时间就越少。”</p><section template="t"><section template="t"></section></section><section><section><section><strong>5</strong></section></section><section><section></section></section></section><section><strong>人工智能设计的“暴政”</strong></section><p>把更多的日常生活交给人工智能机器是有问题的。即使出于最好的意图，AI系统的设计，包括训练数据和数学模型，也反映了编程人员的“狭隘”经验和兴趣。</p><p>当前，许多AI系统没有考虑到不同人的不同经历和特征，AI模型的训练往往基于有偏见的观点和数据，无法充足考虑每个人的独特需求来解决问题，因此此类系统在人类社会中缺乏一致性。甚至在AI大规模应用之前，对日常生活中常见物品的设计往往也是迎合了特定类型的人。例如，研究表明，汽车、包括手机在内的手持工具，甚至办公室环境中的温度设置都是为适合中等身材的男性而设置的，这使得包括女性在内的各种身材和体型的人处于劣势，有时甚至会对他们生活造成危害。</p><p>当不属于有偏见规范的个人被忽视、边缘化和排斥时，AI就会变成卡夫卡式的守门人：拒绝提供客户服务、工作、医疗等服务，这些设计决策的目的约束人们，而不是将他们从日常事务中解放出来。此外，这些选择还可以将一些最恶劣的偏见转化为种族主义和性别歧视，造成严重缺陷和有偏见的判决结果。</p><section template="t"><section template="t"></section></section><section><section><section><strong>6</strong></section></section><section><section></section></section></section><section><strong>对人工智能的恐惧剥夺了人类的利益</strong></section><p>构建机器智能的过程最终以数学为中心，正如默多克所言：“如果我们不注意的话，线性代数会做非常疯狂而强大的事情。”但是，如果人们变得极度害怕AI，并且督促政府通过剥夺“AI便利”方式对其进行监管，那会怎样呢？</p><p>毕竟AI已经帮助人类实现了重大科学进展，例如DeepMind的AlphaFold模型通过氨基酸序列精确预测蛋白质折叠结构方面取得了重大突破，使科学家能够识别98.5%的人类蛋白质的结构，这一里程碑将为生命科学的快速发展提供坚实的基础。考虑到这些AI好处，政府为防范“AI作恶”而采取的下意识监管行动也可能适得其反，并产生它们自己意想不到的负面后果，在这些后果中，我们对这项巨大技术的威力感到如此恐惧，以至于我们拒绝利用它为全世界带来实际好处。</p><p><strong>参考链接</strong></p><p>Via <a href="https://spectrum.ieee.org/ai-worst-case-scenarios" _src="https://spectrum.ieee.org/ai-worst-case-scenarios" target="_blank">https://spectrum.ieee.org/ai-worst-case-scenarios</a><br></p>   
</div>
            