
---
title: '英特尔下一个时代的_CPU_10倍以上性能提升 1000倍能耗降低'
categories: 
 - 新媒体
 - cnBeta
 - 最新
headimg: 'https://static.cnbetacdn.com/thumb/article/2021/1014/7cd117e68e3e804.png'
author: cnBeta
comments: false
date: Thu, 14 Oct 2021 11:07:51 GMT
thumbnail: 'https://static.cnbetacdn.com/thumb/article/2021/1014/7cd117e68e3e804.png'
---

<div>   
芯片行业有一种说法：“算力可以换一切”。也就是当芯片算力足够大的时候，许多难题都能迎刃而解，这也解释了摩尔定律如此受关注的原因。但随着AI、大数据的发展，传统的芯片越来越难以满足新兴应用的需求，业界需要全新类别的芯片。<br>
 <p>量子计算、神经拟态计算（也称类脑计算）是两个重要的突破方向。相比量子计算，神经拟态计算的关注度略低一些，但神经拟态计算已经被英特尔证明在一些应用中，可以带来至少10倍的性能提升，同时实现1000倍的能耗降低。</p><p><strong>英特尔神经拟态计算实验室总监Mike Davies在近日宣布英特尔第二代神经拟态芯片Loihi2后与雷锋网(公众号：雷锋网)的交流中说：“英特尔可能不是第一家将神经形态技术商业化的公司，因为英特尔的规模，我们正在寻找巨大的机会。所以，我们必须以十亿美元的水平看待所有的商业机会。我们并不急于商业化，但当我们商业化的时候，希望这是一项通用技术，可以像CPU、GPU在多种应用中为广大客户提供巨大价值。”</strong></p><p>从某种意义上来说，无论是英特尔正在探索的量子计算还是神经拟态芯片，都有可能成为地位可以比肩现有CPU的“下一个时代的CPU”。</p><p>Mike说至少要在3年后才可能看到英特尔正式推出神经拟态的商业化产品，但Loihi已经取得的成果值得我们期待神经拟态芯片的时代。</p><p><a target="_blank" href="https://static.cnbetacdn.com/article/2021/1014/7cd117e68e3e804.png"><img data-original="https://static.cnbetacdn.com/article/2021/1014/7cd117e68e3e804.png" src="https://static.cnbetacdn.com/thumb/article/2021/1014/7cd117e68e3e804.png" referrerpolicy="no-referrer"></a></p><p><strong>神经拟态芯片与传统芯片有何本质差别？</strong></p><p>谈论神经拟态芯片的优势之前，先解释一下神经拟态计算这种全新的计算方式。<strong>神经拟态计算受到生物学原理的启发，基于对现代神经科学的理解，从晶体管到架构设计，算法以及软件来模仿人脑的运算。</strong></p><p>神经拟态计算与传统芯片和传统处理方式有很多不同，比如，人脑中没有单独的存储器，没有动态随机存取存储器，没有哈希层级结构，没有共享存储器等等。“存储”和“处理器”错综复杂地深绕在人脑里，在人脑的结构中有“神经元”的存在。</p><p>系统编程层面也有很大的区别，传统的CPU或GPU结构以顺序操作或指令/程序来思考，在神经拟态芯片中，计算过程需要神经元单元的互动。神经拟态芯片处理答案的步骤也许不按照程序的执行步骤，更多的是根据动态的交换反应。</p><p><a target="_blank" href="https://static.cnbetacdn.com/article/2021/1014/d6e3068c820f01f.png"><img data-original="https://static.cnbetacdn.com/article/2021/1014/d6e3068c820f01f.png" src="https://static.cnbetacdn.com/thumb/article/2021/1014/d6e3068c820f01f.png" referrerpolicy="no-referrer"></a></p><p>当然也有相似之处，在电脑中，以数字化核心相互交流基于事件的信息，叫做脉冲，这点和人脑传递信息的方式相似。</p><p>相比传统计算与神经拟态计算原理的区别，更多人应该更加关心神经拟态计算表现在应用中的优势。<strong>Mike说：“有一系列问题人脑可以很好地处理，所以我们可以期待神经拟态计算对于真实数据的处理速度或反应时延的性能效率提升，尤其是对于真实数据样本的适应力或快速学习能力的提升。”</strong></p><p><strong>神经拟态芯片擅长处理哪些任务？</strong></p><p>2017年时，英特尔就对外公布了其第一代神经拟态芯片Loihi，随后便开始与业界共同探索神经拟态计算的可能性和可以发挥优势的场景，并建立了神经拟态研究社区（INRC），如今已经有将近150个成员机构。</p><p>INRC今年新增的成员包括福特（Ford）、佐治亚理工学院（Georgia Institute of Technology）、美国西南研究院（SwRI）、美国菲力尔公司（Teledyne FLIR）。</p><p><strong>“我们调查了最近所有的结果，研究了IEEE文献，在很多不同的领域都得出，Loihi的结果超过了使用最好的CPU和GPU算出的结果，其中很多出自感知领域。”Mike表示“特别是和新的事件相机传感器（新型视觉传感器）集成时，神经拟态芯片能够以非常低的功率水平来执行手势识别和其他视觉推理任务。”</strong></p><p><a target="_blank" href="https://static.cnbetacdn.com/article/2021/1014/1e98966920a6089.png"><img data-original="https://static.cnbetacdn.com/article/2021/1014/1e98966920a6089.png" src="https://static.cnbetacdn.com/thumb/article/2021/1014/1e98966920a6089.png" referrerpolicy="no-referrer"></a></p><p>比如气味传感器，与基于传统深度学习的方法相比较，神经拟态计算可以有效地学习多达三千倍的数据。</p><p>在机器人学习方面，基于神经拟态计算也展现出在机器手臂系统变化中的鲁棒性，在一些实时出现的偏差中，Loihi也可以识别到，然后可以回归预期轨道重新布局电路。</p><p>“过去的几个月中，神经拟态计算在量化优化领域取得了非常好的结果。”Mike兴奋的表示。</p><p>相比而言，英特尔与德国铁路公司的合作更能展现神经拟态计算未来的应用前景。<strong>Mike介绍，“使用Loihi解决铁路调度问题，速度比德国铁路公司运营的Dion's使用的先进商业云计算处理器快一个数量级以上，这是1000倍的低能耗。这表明高阶规划决策优化问题可以在以前根本不可能实现的形式因素中得到支持。”</strong></p><p>还有一个例子，一些早期的研究显示，热扩散方程（一个基本的物理行为属性）已经在Loihi中建模，桑迪亚国家实验室完成的这项研究极大地减少了科学计算存在功耗过大方面问题的可能性。</p><p>“我们对Loihi的结果非常满意。但与此同时，我们发现了硬件的一些限制。”Mike同时指出。</p><p><strong>迈向下一个CPU时代</strong></p><p><strong>更强大的硬件</strong></p><p>“我们有了一个编程性极强的神经元，可配置性极强的神经元模型，但它是一个固定功能类型的神经元。”<strong>Mike进一步表示，“自然界没有单一的神经元，实际上有1000种不同类型的神经元，它们在大脑中有许多不同类型的动态。我们想尝试支持的应用确实需要更多的灵活性，以使芯片中的神经元更加多样化。”</strong></p><p>雷锋网了解到，英特尔通过一个微码指令集来解决灵活性的问题，这个微码指令集定义了神经元模型，几乎可以编程任意的模型，涵盖了研究界试图探索的不同类型的方法。</p><p>“我们还扩展了脉冲的概念，这将提升结果的精确度，还可以缩小网络的大小，以支持特定的问题。”Mike介绍，“在功能上，我们在Loihi2上还加强了芯片的学习能力。”</p><p>这些研究层面和功能层面的进步需要更强大的硬件支撑。在电路层面，Loihi 2比Loihi 1快2到10倍，这取决于特定的瓶颈和你测量的特定参数。例如，基于模拟的结果显示，在前馈神经网络中，Loihi2比Loihi快10倍。</p><p>工作负荷层面，Loihi2的神经元的数量提升了8倍，同时将芯片的面积缩小了一半（芯片总内存大致相同），即基于核心大小为0.21 mm2的Loihi 2，最多支持 8192个神经元，对比核心尺寸为0.41 mm2的Loihi，最多支持1024个神经元。</p><p><a target="_blank" href="https://static.cnbetacdn.com/article/2021/1014/1faf3912210da0d.png"><img data-original="https://static.cnbetacdn.com/article/2021/1014/1faf3912210da0d.png" src="https://static.cnbetacdn.com/thumb/article/2021/1014/1faf3912210da0d.png" referrerpolicy="no-referrer"></a></p><p>Mike解释，“第一代Loihi做了固定分配，芯片中的每个核心都支持1024个神经元。但我们发现，在许多应用中，神经元的数量是一个有限的因素，芯片中的其它内存资源却没有得到充分利用。因此，<strong>Loihi 2的架构允许资源在有限的程度上进行交换，同时不影响架构的格式和效率，从而当应用工作负载受限于神经元数量时（通常会发生），能够提供更多的资源来扩展到更多的神经元。</strong>”</p><p>与此匹配，需要先进的半导体制造工艺。<strong>“神经拟态计算的架构相对于其他架构需要更大的资源密度，Intel 4制程能够提供更大的晶体管密度，我们可以在同样大小的芯片上放置更大的神经网络。”Mike还说，</strong></p><p>“与以往的制程技术相比，Intel 4制程节点采用的极紫外光刻（EUV）技术简化了布局设计规则，使Loihi 2的快速开发成为可能。”</p><p>采用预生产版本的Intel 4制程其实还有英特尔展示其先进制程领导力的作用。<strong>需要指出的是，神经拟态架构是一个非常同质的架构，这对于仍处于产量优化过程中的早期工艺来说有很大优势，因为它可以容忍大量的缺陷。</strong></p><p>“神经拟态架构比其他架构更能够从Intel4预生产过程中受益。”Mike表示。</p><p>不过，要解决更多实际问题还需要用Loihi2构建系统。为此，Loihi2的扩展能力也进行了提升，有了4倍速度的接口，还新增了两个接口，可以在3个维度上进行扩展。</p><p><a target="_blank" href="https://static.cnbetacdn.com/article/2021/1014/86ea816f4bff2ce.png"><img data-original="https://static.cnbetacdn.com/article/2021/1014/86ea816f4bff2ce.png" src="https://static.cnbetacdn.com/thumb/article/2021/1014/86ea816f4bff2ce.png" referrerpolicy="no-referrer"></a></p><p>同时，Loihi2对芯片间的连接进行了压缩，让许多工作负载的扩展提供了10倍以上的带宽，在减少拥堵和该架构扩展到更大网络的能力方面，综合提高了60倍以上。</p><p><a target="_blank" href="https://static.cnbetacdn.com/article/2021/1014/c0aeb9a20fd97be.png"><img data-original="https://static.cnbetacdn.com/article/2021/1014/c0aeb9a20fd97be.png" src="https://static.cnbetacdn.com/thumb/article/2021/1014/c0aeb9a20fd97be.png" referrerpolicy="no-referrer"></a></p><p><strong>总体而言，Loihi2的诸多改进，是为了减少支持相同程度能力所需的网络规模，从而获得更快的处理速度和更低的功耗。</strong></p><p><strong>软件是神经拟态芯片大规模商用的关键</strong></p><p><strong>“Loihi 2与第一代一样，属于通用的神经拟态架构。展望未来，我们希望能构建一种新的可编程处理器架构，类似CPU或GPU，不针对特定的应用，适合用于填充组合技术。”Mike展望。</strong></p><p>纵观成功的CPU和GPU，都有非常易于使用软件及软件生态。显然，神经拟态计算芯片想要成为像CPU一样的通用芯片，软件非常关键。</p><p><strong>Mike也说，“过去三年使用Loihi的过程，我们吸取到一个重要经验，软件对神经拟态领域进展的限制和硬件一样关键。”</strong></p><p><a target="_blank" href="https://static.cnbetacdn.com/article/2021/1014/c8082700932399b.png"><img data-original="https://static.cnbetacdn.com/article/2021/1014/c8082700932399b.png" src="https://static.cnbetacdn.com/thumb/article/2021/1014/c8082700932399b.png" referrerpolicy="no-referrer"></a></p><p>此前，想要使用神经拟态芯片，都需要从头开始开发软件，这就像每个人都在重新创造世界。借鉴深度学习领域成功的TensorFlow和PyTorch，加上在神经拟态领域的经验和需求分析，英特尔专为神经拟态计算打造了开源软件框架Lava。</p><p><a target="_blank" href="https://static.cnbetacdn.com/article/2021/1014/c66fded8b061077.png"><img data-original="https://static.cnbetacdn.com/article/2021/1014/c66fded8b061077.png" src="https://static.cnbetacdn.com/thumb/article/2021/1014/c66fded8b061077.png" referrerpolicy="no-referrer"></a></p><p><strong>Mike强调，“我们不会把Lava作为英特尔的一个成品发布给全世界使用，但这确实是与外界合作的起点。</strong>我们现在已经在GitHub上发布了这个软件框架，它借鉴了英特尔在这个领域观察到的东西，也借鉴了英特尔第一代软件开发获得的经验，也就是称之为NX软件开发工具包的NX SDK。”</p><p>开源框架Lava有一个重要特性，无论是将应用程序的成分映射到传统的CPU或GPU上，还是将其分解成神经过程然后运行在神经拟态芯片上都可以。</p><p><strong>“在使用Loihi 2研究芯片时，仍然需要通过英特尔相关法律程序的批准，这对很多想要参与这项研究的人来说是一个障碍。”Mike表示，“我们将为Lava提供一个开源许可证，这意味着开发人员可以自由进入并将这个框架移植到其他神经拟态芯片上。这是关键的一步，能够扩大开源社区，并将所有这些探索神经拟态研究的不同方向的努力和付出汇集在一起，至少在软件层面，可以更快速地实现商用落地。”</strong></p><p>雷锋网了解到，Lava使用的是Python语言，这在一定程度减轻了采用Lava的难度。</p><p><strong>“我想说的是，构建算法的方式，对于巨大的芯片来说是比学习编程语言而言更困难的障碍。”</strong>Mike对雷锋网表示，“可以尝试轻松过渡到使用神经形态芯片，但我们认为最大的挑战还是当前编程所需要使用的特定语言。在未来，我们或其他为Lava做出贡献的人可能会引入新的语言或特定领域的语言，因为很明显它可以提高生产力。”</p><p><a target="_blank" href="https://static.cnbetacdn.com/article/2021/1014/3687db90e5d1e01.png"><img data-original="https://static.cnbetacdn.com/article/2021/1014/3687db90e5d1e01.png" src="https://static.cnbetacdn.com/thumb/article/2021/1014/3687db90e5d1e01.png" referrerpolicy="no-referrer"></a></p><p><strong>写在最后</strong></p><p>对于英特尔而言，全面的产品组合是其保持当下以及可见的未来竞争力的关键。而对神经拟态计算和量子计算的探索，则关乎长远未来的技术领导力。正如Mike所说，神经拟态计算的大规模商用还有很长一段路要走，但英特尔一旦商用神经拟态计算芯片，瞄准的是十亿美元的市场。</p><p>也就是说，英特尔研究神经拟态计算和量子计算，瞄准的是下一个CPU/GPU级别的产品。</p><p>Loihi已经取得的成果，以及如今宣布的Loihi2，不仅能够释放出其在先进计算领域的进展，更能通过Intel4制程将其现在的竞争力和未来的竞争力很好地连接在一起，共同展示英特尔的未来。</p><p>但在先进计算的探索中，除了需要实力，也需要运气。</p>   
</div>
            