
---
title: '苹果公司从其儿童安全网页上删除了对有争议的CSAM扫描功能的提及'
categories: 
 - 新媒体
 - cnBeta
 - 最新
headimg: 'https://static.cnbetacdn.com/article/2021/1215/3a7471a06eddf88.webp'
author: cnBeta
comments: false
date: Wed, 15 Dec 2021 10:08:08 GMT
thumbnail: 'https://static.cnbetacdn.com/article/2021/1215/3a7471a06eddf88.webp'
---

<div>   
苹果公司已经悄悄地从其儿童安全网页上删除了所有提及CSAM的内容，这表明在对其方法提出重大批评后，其检测iPhone和iPad上儿童性虐待图像的有争议的计划可能悬而未决甚至可能中途夭折。<br>
 <p>苹果公司在8月宣布了一套计划中的新的儿童安全功能，包括扫描用户的iCloud照片库中的儿童性虐待材料（CSAM），在接收或发送色情照片时警告儿童及其父母的通信安全，以及扩大Siri和搜索中的CSAM指导。</p><p><img src="https://static.cnbetacdn.com/article/2021/1215/3a7471a06eddf88.webp" title alt="Child-Safety-Feature-yellow.webp" referrerpolicy="no-referrer"></p><p>这些功能公布后，受到广泛的个人和组织的批评，包括安全研究人员、隐私吹哨人爱德华·斯诺登、电子前沿基金会（EFF）、Facebook的前安全主管、政治家、政策团体、大学研究人员，甚至一些苹果员工都对此表示明确反对。</p><p>大多数批评都是针对苹果公司计划的设备上CSAM检测，研究人员抨击其依赖无效和危险的技术，近乎于实施大规模监视，并嘲笑其实际上在识别儿童性虐待图像方面没有效果。</p><p>苹果继续为"信息"功能推出通信安全特性，本周早些时候随着iOS 15.2的发布而上线，但在批评的洪流中，苹果决定推迟CSAM的推出，这显然是它没有想到的。</p><p>9月，该公司表示，"根据客户、宣传团体、研究人员和其他方面的反馈，我们决定在未来几个月花更多时间收集意见，并在发布这些极其重要的儿童安全功能之前进行改进。"</p><p>上述声明之前被添加到苹果公司的儿童安全页面，但它现在已经消失了，这就提出了一种可能性，即苹果公司可能已经把搁置，甚至完全放弃了这个计划。</p>   
</div>
            