
---
title: '谷歌用新AI超越自己：让Imagen能够指定生成对象 风格随意转换'
categories: 
 - 新媒体
 - cnBeta
 - 最新
headimg: 'https://static.cnbetacdn.com/thumb/article/2022/0827/daed01f7a49b3f6.jpeg'
author: cnBeta
comments: false
date: Sat, 27 Aug 2022 06:57:40 GMT
thumbnail: 'https://static.cnbetacdn.com/thumb/article/2022/0827/daed01f7a49b3f6.jpeg'
---

<div>   
给Imagen加上“指哪打哪”的能力，会变得有多强？只需上传3-5张 <strong>指定物体</strong>的照片，再用文字描述想要生成的背景、动作或表情，就能让指定物体“闪现”到你想要的场景中，动作表情也都栩栩如生。不止是动物，其他物体像墨镜、书包、花瓶，也都能做出几乎以假乱真的成品：<br>
 <p>属于是发朋友圈也不会被别人看出破绽的那种。（手动狗头）</p><p>这个神奇的文字-图像生成模型名叫DreamBooth，是Google的最新研究成果，基于Imagen的基础上进行了调整，一经发布就在Twitter上引发热议。</p><p><a target="_blank" href="https://static.cnbetacdn.com/article/2022/0827/daed01f7a49b3f6.jpeg"><img data-original="https://static.cnbetacdn.com/article/2022/0827/daed01f7a49b3f6.jpeg" src="https://static.cnbetacdn.com/thumb/article/2022/0827/daed01f7a49b3f6.jpeg" referrerpolicy="no-referrer"></a><br></p><p>有网友调侃：这简直是最先进的梗图生成器。</p><p><a target="_blank" href="https://static.cnbetacdn.com/article/2022/0827/a3bf59c69db8e3c.png"><img data-original="https://static.cnbetacdn.com/article/2022/0827/a3bf59c69db8e3c.png" src="https://static.cnbetacdn.com/thumb/article/2022/0827/a3bf59c69db8e3c.png" referrerpolicy="no-referrer"></a><br></p><p>目前相关研究论文已上传至arXiv。</p><p>几张照片就能“环游世界”</p><p>在介绍原理前，让我们先来看看DreamBooth的各种能力，包括换景、指定动作表情服饰、更迭风格等。</p><p>如果你是个“铲屎官”，有了这个模型的“ <strong>换景能力</strong>”，就能足不出户送自家狗子走出家门，凡尔赛宫里、富士山脚下……通通不在话下。</p><p><a target="_blank" href="https://static.cnbetacdn.com/article/2022/0827/52764e257f937a9.png"><img data-original="https://static.cnbetacdn.com/article/2022/0827/52764e257f937a9.png" src="https://static.cnbetacdn.com/thumb/article/2022/0827/52764e257f937a9.png" referrerpolicy="no-referrer"></a><br></p><p>△光照也比较自然</p><p>不仅如此，宠物的 <strong>动作和表情</strong>也都能随意指定，属实是把“一句话P图”的细节拿捏到位了。</p><p><a target="_blank" href="https://static.cnbetacdn.com/article/2022/0827/c123c9c241cf874.png"><img data-original="https://static.cnbetacdn.com/article/2022/0827/c123c9c241cf874.png" src="https://static.cnbetacdn.com/thumb/article/2022/0827/c123c9c241cf874.png" referrerpolicy="no-referrer"></a><br></p><p>除了上面的“基操”以外，DreamBooth甚至还能更换各种照片风格，也就是所谓的“加滤镜”。</p><p>例如，各种“世界名画”画风、各种视角的狗子，简直不要太艺术：</p><p><a target="_blank" href="https://static.cnbetacdn.com/article/2022/0827/df2bcf594abb1be.png"><img data-original="https://static.cnbetacdn.com/article/2022/0827/df2bcf594abb1be.png" src="https://static.cnbetacdn.com/thumb/article/2022/0827/df2bcf594abb1be.png" referrerpolicy="no-referrer"></a><br></p><p>至于给它们 <strong>加上装饰</strong>？各种cosplay的小道具，也是小菜一碟。</p><p><a target="_blank" href="https://static.cnbetacdn.com/article/2022/0827/e7386ff4b1daaa5.png"><img data-original="https://static.cnbetacdn.com/article/2022/0827/e7386ff4b1daaa5.png" src="https://static.cnbetacdn.com/thumb/article/2022/0827/e7386ff4b1daaa5.png" referrerpolicy="no-referrer"></a><br></p><p>除此之外，无论是更换颜色：</p><p>还是更魔幻一点，更换物种，这只AI也都能做到。</p><p>那么，如此有趣的效果背后的原理是什么呢？</p><p>给输入加个“特殊标识符”</p><p>研究人员做了个对比，相较于其他大规模文本-图像模型如DALL-E2、Imagen等，只有采用DreamBooth的方法，才能做到对输入图像的忠实还原。</p><p>如下图所示，输入3张右边表盘上画着黄色“3”的小闹表，其中DreamBooth生成的图像完美保留了钟表的所有细节，但DALL-E2和Imagen几次生成的钟都与原来的钟“有那么点差异”。</p><p><a target="_blank" href="https://static.cnbetacdn.com/article/2022/0827/e1fe8e29721d317.png"><img data-original="https://static.cnbetacdn.com/article/2022/0827/e1fe8e29721d317.png" src="https://static.cnbetacdn.com/thumb/article/2022/0827/e1fe8e29721d317.png" referrerpolicy="no-referrer"></a><br></p><p>△李逵和“李鬼”</p><p>而这也正是DreamBooth最大的特点—— <strong>个性化表达</strong>。</p><p>用户可以给定3-5张自己随意拍摄的某一物体的图片，就能得到不同背景下的该物体的新颖再现，同时又保留了其关键特征。</p><p>当然，作者也表示，这种方法并不局限于某个模型，如果DALL·E2经过一些调整，同样能实现这样的功能。</p><p>具体到方法上，DreamBooth采用了给物体加上“ <strong>特殊标识符</strong>”的方法。</p><p>也就是说，原本图像生成模型收到的指令只是一类物体，例如[cat]、[dog]等，但现在DreamBooth会在这类物体前加上一个特殊标识符，变成[V][物体类别]。</p><p>以下图为例，将用户上传的三张狗子照片和相应的类名（如“狗”）作为输入信息，得到一个经过微调的文本-图像扩散模型。</p><p>该扩散模型用“a [V] dog”来特指用户上传图片中的狗子，再把其带入文字描述中，生成特定的图像，其中[V]就是那个特殊标识符。</p><p><a target="_blank" href="https://static.cnbetacdn.com/article/2022/0827/05b50ce8f9feac8.png"><img data-original="https://static.cnbetacdn.com/article/2022/0827/05b50ce8f9feac8.png" src="https://static.cnbetacdn.com/thumb/article/2022/0827/05b50ce8f9feac8.png" referrerpolicy="no-referrer"></a><br></p><p>至于为什么不直接用[V]来指代整个[特定物体]？</p><p>作者表示，受限于输入照片的数量，模型无法很好地学习到照片中物体的整体特征，反而可能出现过拟合。</p><p>因此这里采用了微调的思路，整体上仍然基于AI已经学到的[物体类别]特征，再用[V]学到的特殊特征来修饰它。</p><p>以生成一只白色的狗为例，这里模型会通过[V]来学习狗的颜色（白色）、体型等个性化细节，加上模型在[狗]这个大的类别中学到的狗的共性，就能生成更多合理又不失个性的白狗的照片。</p><p>为了训练这个微调的文本-图像扩散模型，研究人员首先根据给定的文本描述生成低分辨率图像，这时生成的图像中狗子的形象是随机的。</p><p>然后再应用超分辨率的扩散模型进行替换，把随机图像换成用户上传的特定狗子。</p><p><a target="_blank" href="https://static.cnbetacdn.com/article/2022/0827/cd7d6b7729534b0.png"><img data-original="https://static.cnbetacdn.com/article/2022/0827/cd7d6b7729534b0.png" src="https://static.cnbetacdn.com/thumb/article/2022/0827/cd7d6b7729534b0.png" referrerpolicy="no-referrer"></a><br></p><p>研究团队</p><p>DreamBooth的研究团队来自Google，第一作者是Nataniel Ruiz。</p><p>Nataniel Ruiz是波士顿大学图像和视频计算组的四年级博士生，目前在Google实习。主要研究方向是生成模型、图像翻译、对抗性攻击、面部分析和模拟。</p>   
</div>
            