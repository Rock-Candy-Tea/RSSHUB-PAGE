
---
title: '公平衡量：MIT研究人员称机器学习的可解释性充满偏见'
categories: 
 - 新媒体
 - cnBeta
 - 最新
headimg: 'https://static.cnbetacdn.com/thumb/article/2022/0706/0a20726841d46ab.png'
author: cnBeta
comments: false
date: Wed, 06 Jul 2022 09:30:00 GMT
thumbnail: 'https://static.cnbetacdn.com/thumb/article/2022/0706/0a20726841d46ab.png'
---

<div>   
麻省理工学院（MIT）的科学家们，在近期的一篇新论文中提到 —— <strong>帮助用户确定机器学习模型的预测是否可信的方法，对弱势群体来说可能不太准确。</strong>由于解释方法可能存在长期偏见，弱势群体面临的结果或变得更加糟糕。<br>
 <p><a href="https://static.cnbetacdn.com/article/2022/0706/0a20726841d46ab.png" target="_blank"><img src="https://static.cnbetacdn.com/thumb/article/2022/0706/0a20726841d46ab.png" alt="1.png" referrerpolicy="no-referrer"></a></p><p style="text-align: center;">研究配图 - 1：非公正全局解释示例（<a href="https://arxiv.org/abs/2205.03295" target="_self">arXiv</a>）</p><p>使用机器学习算法的时候，有时会造成相当高的风险 —— 比如通过模型来预测哪些候选认更有可能通过法律考试，然后在让校方在招生时优先录取哪些学生。</p><p>即使理想很丰满，但现实往往有些骨感 —— 这些复杂模型动辄涉及数百万个参数，而 AI 研究人员几乎不可能完全了解其运作机理。</p><p><a href="https://static.cnbetacdn.com/article/2022/0706/fa5ecad5e619a7d.png" target="_blank"><img src="https://static.cnbetacdn.com/thumb/article/2022/0706/fa5ecad5e619a7d.png" alt="2.png" referrerpolicy="no-referrer"></a></p><p style="text-align: center;">研究配图 - 2：神经网络黑箱模型模拟</p><p>此外科学家有时也会通过创建预测的简单近似模型来简化解释，但这些易于理解的近似值，是否又能够公平承托所有人的信任呢？</p><p>假设某种解释方法让男性获得较女性更优的近似值、或让白人较有色人种更具优势，这种情况就会在两组对照时产生潜在的巨大差异。</p><p><a href="https://static.cnbetacdn.com/article/2022/0706/304e37275b5d8e5.png" target="_blank"><img src="https://static.cnbetacdn.com/thumb/article/2022/0706/304e37275b5d8e5.png" alt="3.png" referrerpolicy="no-referrer"></a></p><p style="text-align: center;">研究配图 - 3：有无健壮训练子组之间的保真度差距</p><p>实践中，这意味着如果女性申请人的近似质量较低，则解释与模型之间的预测可能存在不匹配，进而导致招生官员错误地回绝了更多女性候选认。</p><p>为了解这些公平差距到底有多普遍，MIT 研究人员尝试了多种技术来平衡竞争环境。但这么做只能适当缩小一些差距，而无法彻底根除。</p><p><a href="https://static.cnbetacdn.com/article/2022/0706/4b7286b76d80025.png" target="_blank"><img src="https://static.cnbetacdn.com/thumb/article/2022/0706/4b7286b76d80025.png" alt="4.png" referrerpolicy="no-referrer"></a></p><p style="text-align: center;">研究配图 - 4：即使底层黑箱足够公正，非零保真差距仍存在。</p><p>研究一作、MIT 计算机科学与人工实验室（CSAIL）健康机器学习小组研究生 Aparna Balagopalan 表示：</p><blockquote><p>在现实世界中，这意味着人们可能会错误地相信某些子群（而不是其它子群）的预测。</p><p>正因如此，解释模型的改进、以及将相关细节传达给最终用户，也显得同样重要。</p><p>只有了解到这些差距的存在，用户才会更加平和地接受并调节其对结果的预期。</p></blockquote><p><a href="https://static.cnbetacdn.com/article/2022/0706/cc9d5fd0a768226.png" target="_blank"><img src="https://static.cnbetacdn.com/thumb/article/2022/0706/cc9d5fd0a768226.png" alt="5.png" referrerpolicy="no-referrer"></a></p><p style="text-align: center;">研究配图 - 5：决策保真差的神经网络模拟</p><p>研究人员发现，所有数据集和解释模型都存在明显的保真度差距。若是群体的保真度通常要低得多，某些情况下可能高达 21% 。</p><p><a href="https://static.cnbetacdn.com/article/2022/0706/6abc23a8f28d21d.png" target="_blank"><img src="https://static.cnbetacdn.com/thumb/article/2022/0706/6abc23a8f28d21d.png" alt="6.png" referrerpolicy="no-referrer"></a></p><p style="text-align: center;">研究配图 - 6：黑箱与解释模型之间的 DP 差距与剩余误差</p><p>数据集在种族子组之间的保真度差距，近似值的平均错误率也高出了 7% 。假如有 10000 名申请者，那很大一部分可能被错误地拒绝。</p><p><a href="https://static.cnbetacdn.com/article/2022/0706/6cf3f4a7e1cf745.png" target="_blank"><img src="https://static.cnbetacdn.com/thumb/article/2022/0706/6cf3f4a7e1cf745.png" alt="7.png" referrerpolicy="no-referrer"></a></p><p style="text-align: center;">研究配图 - 7：更少特征的稀疏模型的局部跨子组保真度差距</p><p>Ghassemi 补充道，他们对这些普遍存在于所有评估数据集中的保真度差距感到震惊，但也很难过分强调如何对相关机器学习模型进行修饰。</p><p><a href="https://static.cnbetacdn.com/article/2022/0706/4f151d76fd16618.png" target="_blank"><img src="https://static.cnbetacdn.com/thumb/article/2022/0706/4f151d76fd16618.png" alt="8.png" referrerpolicy="no-referrer"></a></p><p style="text-align: center;">研究配图 - 8：稀疏模型往往有着更大的平均逼近误差</p><p>在确定了保真度的差距后，研究人员尝试训练了解释模型，以了解其识别数据集中可能容易出现低保真度的区域，然后对这些样本给予更高的关注度。</p><p><a href="https://static.cnbetacdn.com/article/2022/0706/be6f76515aec566.png" target="_blank"><img src="https://static.cnbetacdn.com/thumb/article/2022/0706/be6f76515aec566.png" alt="9.png" referrerpolicy="no-referrer"></a></p><p style="text-align: center;">研究配图 - 9：保真度差距与准确性，在一系列抽样方差中持续存在。</p><p>此外他们尝试了使用所有子组的相同数量样本的平衡数据集，这些强大的训练策略确实减少了一些保真度差距，但终究还是无法彻底消除。</p><p><a href="https://static.cnbetacdn.com/article/2022/0706/85aeb11dda44ff2.png" target="_blank"><img src="https://static.cnbetacdn.com/thumb/article/2022/0706/85aeb11dda44ff2.png" alt="10.png" referrerpolicy="no-referrer"></a></p><p style="text-align: center;">研究配图 - 10：有无健壮 LIME 和基于树的模型训练子组之间的差距</p><p>研究人员随后修改了解释模型，以探索为何会凸显保真度差距。分析表明，解释模型可能会间接地使用受保护、甚至隐藏的群体信息（比如性别或种族标签）。</p><p><a href="https://static.cnbetacdn.com/article/2022/0706/1583d2bbdbdd510.png" target="_blank"><img src="https://static.cnbetacdn.com/thumb/article/2022/0706/1583d2bbdbdd510.png" alt="11.png" referrerpolicy="no-referrer"></a></p><p style="text-align: center;">研究配图 - 11：解释保真度与组间决策准确性差距的正相关性</p><p>最后，MIT 研究人员希望能够在未来的工作中深入探索相关难题，并且计划进一步研究真实世界决策背景下的保真度差距的影响。</p>   
</div>
            